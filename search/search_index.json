{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my SRE journey website","text":"<p>Let me introduce.</p> <p>I am Satheesh Pandian from Bangalore, working in one of the premier banks in India as Senior Site Reliability Engineer.  I have been in IT industry since 2006 right after my post-graduation in Master of Technology.</p> <p>Though I started my career with Performance testing, eventually I landed upon performance engineering and optimization  for quite some time. Then, I took diversion and started learning new skills and became Sr.SRE. I started to capture whatever I learnt over the period of time in this website. I am sure this will help some of you  while working.</p> <p>Keep learning and NEVER QUIT. Learning is a continuous process until we die. </p>"},{"location":"ansible/intro/","title":"Introduction","text":"<p>Ansible provides open-source automation that reduces complexity and runs everywhere. Here are some use cases </p> <ol> <li>Eliminate repetition</li> <li>Manage and maintain system configuration</li> </ol> <p>Ansible uses playbook to automate the tasks. You declare the desire state of your system in the playbook  and ansible ensures that the system remains in the system.</p>"},{"location":"ansible/intro/#advantages","title":"Advantages","text":"<ol> <li>Agentless - No need to install any agent to automate the system</li> <li>Scalable - Easily scalable in any environment including cloud infrastructures.</li> </ol>"},{"location":"ansible/intro/#ansible-concepts","title":"Ansible Concepts","text":"<ul> <li> <p>Control Node</p> <ul> <li>This is the machine where you run Ansible commands in CLI tools. </li> <li>You can use any computer that meets the software requirements. </li> <li>You can run ansible as a container. Ansible uses the image in the container is called as execution environment.</li> <li>An execution environment image contains the following packages as standard:</li> <li><code>ansible-core</code>, <code>ansible-runner</code>, <code>python</code> and ansible content dependencies</li> </ul> </li> <li> <p>Managed Node</p> <ul> <li>These are the target device which you want to manage.</li> <li>We don't need to install in these machines.</li> </ul> </li> <li> <p>Plays (Basic unit of Ansible execution)</p> <ul> <li>This is nothing but an ordered list of tasks can be run repeatedly.</li> <li>Play contains variables, roles, tasks and defines how to iterate over them.</li> </ul> </li> <li> <p>Playbooks</p> <ul> <li>Playbook contains play. It is written in YAML. </li> <li><code>ansible-playbook</code> command operates on this file.</li> </ul> </li> <li> <p>Roles</p> <ul> <li>A reusable ansible content (tasks, variables, plugins etc.,) can be used in the play. </li> <li>To use any roles, it must be imported into the play.</li> </ul> </li> <li> <p>Tasks</p> <ul> <li>Action to be applied to the managed nodes.</li> </ul> </li> <li> <p>Handlers</p> <ul> <li>This is the task, but executed only when notified by the previous task which results in a changed status.</li> </ul> </li> <li> <p>Modules</p> <ul> <li>The code copies to each managed nodes and executes to accomplish the task by Ansible.</li> </ul> </li> <li> <p>Plugins</p> <ul> <li>Piece of code that expand Ansible's core capabilities. It can control how to you connect managed nodes, filter the data etc.,</li> </ul> </li> <li> <p>Collections</p> <ul> <li>It is a format of ansible content such as roles, plugins, playbooks etc., </li> <li>You can install collections using Ansible Galaxy.</li> </ul> </li> </ul>"},{"location":"aws/cw/cw-intro/","title":"Introduction to Cloudwatch (CW)","text":"<p>Cloudwatch is one of the services provided by AWS used to monitor the system and application health. It can monitor the AWS resources, AWS infrastructure and on-premises application as well. It can collect the logs, metrics from other AWS resources and helpful for taking decision based on the details.</p> <p>System monitoring is alone not helpful in any application. It requires a proactive mechanism to stop that event when it occurs. Alerting is one such mechanism can help, and it is part of cloudwatch service.</p>"},{"location":"aws/ecs/deploy-app-in-ecs/","title":"Deploy an app in ECS","text":"<p>Let us deploy a small python flask application in ECS.</p> <p>The application is just printing the content in the browser. We will build the docker image and push it to docker hub registry and run it in ECS.</p> <p><code>flask-app.py</code> <pre><code>from flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello Satheesh from flask app\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5001)\n</code></pre></p> <p><code>Dockerfile</code></p> <pre><code># Use the official Python image from the Docker Hub\nFROM --platform=linux/amd64 python:3.9-slim as build\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY ./flask-app.py .\n\n# Install Flask inside the container\nRUN pip install Flask\n\n# Expose the port the app runs on\nEXPOSE 5001\n\n# Run the application\nCMD [\"python\", \"flask-app.py\"]\n</code></pre> <p>Let us build the docker image using <code>docker build -t satheeshpandianj/hello-flask:v1 .</code> command.</p> <p></p> <p></p> <p>Let us push the docker image into docker hub registry using <code>docker push docker.io/satheeshpandianj/hello-flask:v1</code> command.</p> <p></p> <p>Check the docker hub registry if the docker image is available.</p> <p></p> <p>Now it is the time to deploy the application image in ECS.</p>"},{"location":"aws/ecs/deploy-app-in-ecs/#steps-to-deploy-an-application-in-ecs","title":"Steps to deploy an application in ECS","text":"<ol> <li>Navigate to ECS in AWS console</li> </ol> <ol> <li>Create a cluster. Click on the \"Create cluster\" button.</li> </ol> <ol> <li> <p>Fill the details in the section.  Under the infrastructure section, select \"AWS Fargate (serverless)\" option.  Keep as it is for the other options.</p> </li> <li> <p>Click on the \"Create\" button.</p> </li> </ol> <p></p> <p>Now the cluster is created successfully. </p> <ol> <li>Let us create the task definition. Click on \"Task definition\" on the left side menu.</li> </ol> <p></p> <ol> <li>Click on \"Create new task definition.\" Fill the form as per the below snapshots.</li> </ol> <p></p> <p></p> <p></p> <p>Then, click on the \"Create\" button. Now the task definition is created.</p> <p></p> <ol> <li>Run the task. Click on \"Deploy\" and select \"Run task\" </li> </ol> <p></p> <ol> <li>Select the cluster (we created this at step # 4).</li> </ol> <p></p> <p>Select the VPC, Subnet, security group based on your needs.</p> <p><code>Remember that security group inbound rule should contain the container port configuration.</code></p> <p></p> <ol> <li>Click on the \"Create\" button.</li> </ol> <p></p> <p></p> <ol> <li>Click on the task. Note down the public IP address. This IP address changes every single time.</li> </ol> <p></p> <ol> <li>Open the browser and enter the public IP address along with container port number and hit enter button</li> </ol> <p></p> <p>NOTE: </p> <p> In general, we need to create the service first and map the task under service. We need to run the service instead of the task directly. This is the right way to run the container. </p> <p><code>Remember that if we are running multiple tasks under a service, each task has unique IP address</code></p>"},{"location":"aws/ecs/deploy-app-in-ecs/#update-the-code-and-deploy-it-without-any-disturbance-of-end-user","title":"Update the code and deploy it without any disturbance of end user","text":"<p>Let us say that our application is already running in ECS containers, and we are updating the new feature in the  current code. We want to deploy the updated code in ECS without stopping the running containers. </p> <p><code>flask-app.py</code></p> <pre><code>from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello, I am working in AWS ECS with my updated code\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5001)\n</code></pre> <p><code>Dockerfile</code> - There is no change in the Dockerfile.</p> <pre><code># Use the official Python image from the Docker Hub\nFROM --platform=linux/amd64 python:3.9-slim as build\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY ./flask-app.py .\n\n# Install Flask inside the container\nRUN pip install Flask\n\n# Expose the port the app runs on\nEXPOSE 5001\n\n# Run the application\nCMD [\"python\", \"flask-app.py\"]\n</code></pre> <ol> <li>Build the new docker image with updated code</li> </ol> <p></p> <ol> <li>Push the docker image to the docker hub registry</li> </ol> <p></p> <ol> <li>Check if newly created image is available in docker hub registry.</li> </ol> <p></p> <ol> <li>Go to AWS console and open the cluster where ECS containers are running.</li> </ol> <p></p> <ol> <li>Select the service and click on the \"Update\" button</li> </ol> <p></p> <ol> <li>Check the \"Force deployment\" option. If there are any changes in the container setting, do update it. Else, keep it as it was.</li> </ol> <p></p> <ol> <li>Click on the \"Update\" button. This will create different tasks under the task tab. </li> </ol> <p></p> <p>After the new tasks are created, old tasks will be deleted automatically</p> <p></p> <ol> <li>Open the task and note down the public IP address.</li> </ol> <p></p> <ol> <li>Open the browser and enter the public IP along with port number.</li> </ol> <p></p> <ol> <li>Step # 8 and 9 will repeat for another task (Remember we create two different tasks)</li> </ol> <p></p> <p></p> <p><code>If you look at the public IP address for both the tasks, they are different.</code></p>"},{"location":"aws/ecs/ecs-intro/","title":"Elastic Container Services (ECS)","text":"<p>ECS is nothing but a container orchestrator managed by AWS. It will manage the container lifecycle such as creating the container, maintaining them and destroying them when needed. ECS helps to deploy an application and load balancing between the servers. It also helps to autoscale when the traffic is high. Rollout of the application is also easy (When we are updating our code and deploying the updated code without disturbing, the end user experience is straightforward via ECS). </p> <p>Let us assume that we build an application and created docker images for it. In our environment, assume that there are three servers and our application needs to run in all these three servers. In this case, if we run docker command to run our application, it will run only one server. If we want to run our application in all three servers, we need to copy the code, dockerfile and build an image in all these servers and run it. However, they don't really know each other. In a way, we are running our application in three different places,  but none of the servers is aware the application runs in other servers too.</p> <p>Hence, there is no orchestrator here, and if one server goes down, we need to manually up the server. There is no way to do autoscaling when the traffic goes high. To avoid these issues, we need an orchestrator who manages the container and monitors them. Based on the events, it will decide what to do as the next steps and doesn't need any manual intervention.</p> <p>Kubernetes, Hashicorp Nomad, Apache Mesos are such an orchestrator. ECS is also one among them, but it is a simple version of it.</p>"},{"location":"aws/ecs/ecs-intro/#launch-types","title":"Launch Types","text":"<p>There are two types of launch in ECS</p> <ol> <li>EC2</li> <li>Fargate</li> </ol> <p><code>Remember that EC2 always manage only container, NOT the actual infrastructure.</code></p> <p>To run the container, you need physical infrastructure like virtual machines/physical machines/docker etc. However, ECS will not provide that infrastructure.</p>"},{"location":"aws/ecs/ecs-intro/#ec2-launch-type","title":"EC2 Launch Type","text":"<p>You need a cluster which contains the virtual machines to run the containers. It could be EC2 (Elastic compute cloud) instances as well. However, it is your responsibility to manage all those EC2 instances. It means you need to install the required patches, software, ECS agent,  docker, and firewall configuration, etc. ECS will only manage the containers, and they don't do anything else.</p> <p>ECS control panel is responsible to manage the container life cycle. It will talk to ECS agent in the EC2 instance and plan the containers accordingly (create/manage/destroy containers). You have full control as you are taking care of infrastructure.</p> <p></p> Your Responsibility ECS Responsibility Manage infrastructure Manage containers All installations of software, patches, firewall etc ECS don't care anything for installation"},{"location":"aws/ecs/ecs-intro/#fargate-launch-type","title":"Fargate Launch Type","text":"<p>In this launch type, we don't need to manage infrastructure. Instead, Fargate takes that responsibility.  This is a serverless architecture that means there is no actual physical machine to run the container. When we deploy the application in ECS, ECS checks if there is any machine to deploy the container and there is no machine, ECS will talk to Fargate and ask the infrastructure. Then, Fargate will create a physical machine to deploy the container. Hence, AWS manages the underlying resources (infrastructure).</p> <p></p> Your responsibility ECS Responsibility (AWS Fargate) Create the application and deploy in ECS Deploy the application as a container No need to manage infrastructure ECS/Fargate takes care of infrastructure <p>Autoscaling will automatically take care of Fargate, and we need to pay as per use.</p>"},{"location":"aws/ecs/ecs-intro/#ecs-task-definition","title":"ECS Task Definition","text":"<p>Task definition file is nothing but a collection of instructions tell that how the containers should be deployed and what configuration a container should have like CPU, memory, etc.  In short, it is a blueprint of the container.</p> <p>Task is nothing but a running container created using this task definition file. If we want to run three different instances (containers), you need to create three tasks using task definition file.</p> <p></p>"},{"location":"aws/ecs/ecs-intro/#ecs-services","title":"ECS Services","text":"<p>ECS services ensure that the number of tasks is running all the time.This is the kind of orchestrator who manages the tasks. Also, it will monitor EC2 instances. If any of the EC2 instances fails, then it will create a task using task definition file and run it in working EC2 instance.</p> <p></p>"},{"location":"aws/ecs/ecs-intro/#ecs-load-balancer","title":"ECS Load balancer","text":"<p>Load balancer will route the traffic to the containers via services. This will help us to ensure all the containers are sharing the load and also autoscaling when the demand increases.</p> <p></p>"},{"location":"aws/iam/intro/","title":"IAM (Identity and Access Management)","text":"<p>IAM is a global service in AWS. We can create users and these users can be grouped as well. User can belong to the multiple groups, and the permission can be aggregated in this case. At the same time, user cannot be part of any group either. </p> <p><code>Group should have only users, not any other group.</code></p>"},{"location":"aws/iam/intro/#permission-policy","title":"Permission (Policy)","text":"<p>IAM permission is nothing but a policy document assigned to the user or group. This is just a JSON document that describes what the user or group can do or cannot do.</p> <p><code>Remember that you have to follow least privileage to the user in AWS. Do not give more permissions to the user than what they really needed</code></p>"},{"location":"aws/iam/intro/#creating-group","title":"Creating Group","text":"<ol> <li>Go to IAM service. Click on 'User Groups' menu.</li> </ol> <ol> <li>Click on 'Create Group' button</li> </ol> <ol> <li>Enter the group name and select the predefined policy based on your requirement. Here I am selecting administrator  policy that has full access.</li> </ol> <ol> <li>Click on 'Create Group' button</li> </ol>"},{"location":"aws/iam/intro/#creating-user","title":"Creating User","text":"<ol> <li>Go to IAM service. Click on 'Users' menu.</li> </ol> <ol> <li>Click on 'Create User' button</li> </ol> <ol> <li>Enter the username and click on 'Next' menu.</li> </ol> <ol> <li>Here we are creating the new user and mapping the user with 'admin' group. It means that the new user has all  the permissions provided to the group 'admin'</li> </ol> <p>Select the permission option and choose the group. Click on 'Next' button.</p> <p></p> <ol> <li>Click on 'Add tag' button</li> </ol> <p></p> <ol> <li>Click on 'Create user' button</li> </ol> <p></p> <p>Now the user is created and this user is part of 'admin' group.</p> <p></p> <ol> <li>Create the password for newly created user. Click on the username and then click on 'Security Credential' tab. Click on an 'Enable console access' button.</li> </ol> <p></p> <ol> <li>Select a custom password option and then enter the password. Click on an 'Enable console access' button.</li> </ol> <p></p> <p><code>Now, the user is created and have a console access. This user can login using his/her IAM username and password</code></p>"},{"location":"aws/iam/intro/#iam-policy-inheritance","title":"IAM Policy Inheritance","text":"<p>Assume that there are three different groups (Developers, Audit, Operations) and six team members (Alice, Bob, Charles, David, Edward and Fred) are part of those groups. </p> <p></p> <p>Each group has different policy and hence they have different access to the services. User <code>Fred</code>  is not part of any group, and he has his own permission. Users 'Charles and David' belong to two different groups, and they have both group policy access as well.</p>"},{"location":"aws/iam/intro/#iam-policy-structure","title":"IAM Policy Structure","text":"<p>Policy defines what action user can perform on the services.</p> Name Description Version Always \"2012-10-17\" Statement One or more statements required Sid Can be anything (optional) Effect Either \"Allow\" or \"Deny\" Principal User/Group/Account to the policy applied to Action List of actions can be performed on the services Resource What kind of actions applied to the resources Condition Condition when the policy is effect <p></p>"},{"location":"aws/iam/intro/#creating-policy","title":"Creating Policy","text":"<p>I am creating the new user, and this user is not part of any group. We can define the policy what this new user can do.</p> <p></p> <p>As of now, the user 'pandian' is created and there is no policy attached to this user.</p> <p></p> <p>Now I am trying to view the users created so far using the newly created user 'pandian' It clearly shows that this new user 'pandian' doesn't have permission to view the user list (Access denied).</p> <p></p> <p>Now, let us give the permission to get and list the users for 'pandian'. To do this, you need to log in with admin-privileged user. Click on the username 'pandian' and click on 'Add permission' dropdown menu.</p> <p></p> <p>Click on 'Add permission' button</p> <p></p> <p>Choose 'Attach policies directly' option. Search 'IAMReadOnlyAccess' policy.</p> <p></p> <p>Click on 'Next' button. Review the details and Click on 'Add permission' button.</p> <p></p> <p>Now the newly created user 'pandian' has one policy attached to it.</p> <p></p> <p>User 'pandian' is able to view the user list now. But he cannot create any group or user.</p> <p></p> <p></p>"},{"location":"aws/iam/intro/#create-inline-policy","title":"Create inline policy","text":"<p>Let us give the permission to get and list the users for 'pandian'. To do this, you need to log in with admin-privileged user. Click on the username 'pandian' and click on 'Add permission' dropdown menu.</p> <p></p> <p>Click on 'Create inline policy' button</p> <p></p> <p>Select the services from the dropdown. Choose the access level (List/Read/Write/Permission Management/Tagging) for the service.</p> <p></p> <p>Enter the policy name, and click on 'create policy' button.</p> <p></p>"},{"location":"aws/iam/intro/#create-user-group-policy-in-aws-cli","title":"Create User, Group, Policy in AWS CLI","text":"<p>Prerequisite</p> <ol> <li>AWS cli should be installed.</li> <li>AWS access key should be created.</li> <li>AWS configure should be done. </li> </ol>"},{"location":"aws/iam/intro/#create-access-key","title":"Create Access Key","text":"<ol> <li>Create the access key for IAM user.</li> </ol> <p>Let us create an access key for the user 'satheesh'.</p> <p></p> <ol> <li>Click on the 'create access' button.</li> <li>Select 'CLI' option and click on 'Next' button.</li> </ol> <p></p> <ol> <li>Click on 'create access key' button</li> </ol> <p></p> <ol> <li>Note down the access key and secret key. This will be used for configuration.</li> </ol>"},{"location":"aws/iam/intro/#aws-configuration-steps","title":"AWS Configuration Steps","text":"<ol> <li>AWS CLI installed already on my machine. Refer \"https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\" for the steps.</li> </ol> <ol> <li>Execute <code>aws configure</code> command. Provide the required details like access key, secret key, region and output format.</li> </ol> <p>Now, it is configured. We can run any aws command from the terminal.</p>"},{"location":"aws/iam/intro/#create-user","title":"Create User","text":"<ol> <li>Run the command</li> </ol> <pre><code>aws iam create-user --user-name &lt;UserName&gt;\n</code></pre> <p><pre><code>aws iam create-user --user-name praba\n</code></pre> </p> <p></p> <ol> <li>Attach the IAM policy to the user</li> </ol> <pre><code>aws iam attach-user-policy --user-name &lt;UserName&gt; --policy-arn arn:aws:iam::aws:policy/&lt;Policy name&gt;\n</code></pre> <pre><code>aws iam attach-user-policy --user-name praba --policy-arn arn:aws:iam::aws:policy/IAMReadOnlyAccess\n</code></pre> <p></p> <ol> <li>Verify the user</li> </ol> <pre><code>aws iam list-users\n</code></pre> <p></p>"},{"location":"aws/iam/intro/#create-group","title":"Create Group","text":"<ol> <li>Run the command</li> </ol> <pre><code>aws iam create-group --group-name &lt;GroupName&gt;\n</code></pre> <p><pre><code>aws iam create-group --group-name sre-team-members\n</code></pre> </p> <p></p> <ol> <li>Attach the IAM policy to the group</li> </ol> <pre><code>aws iam attach-group-policy --group-name &lt;GroupName&gt; --policy-arn arn:aws:iam::aws:policy/&lt;Policy name&gt;\n</code></pre> <pre><code>aws iam attach-group-policy --group-name sre-team-members --policy-arn arn:aws:iam::aws:policy/IAMFullAccess\n</code></pre> <p></p> <ol> <li>Add the user to the group</li> </ol> <pre><code>aws iam add-user-to-group --group-name &lt;GroupName&gt; --user-name &lt;UserName&gt;\n</code></pre> <pre><code>aws iam add-user-to-group --group-name sre-team-members --user-name praba\n</code></pre> <p></p> <ol> <li>Verify the group</li> </ol> <pre><code>aws iam list-groups\n</code></pre> <p></p>"},{"location":"aws/iam/intro/#create-policy","title":"Create Policy","text":"<ol> <li>Define the policy in the JSON format.  Below policy gives full access to IAM service</li> </ol> <p><code>policy.json</code></p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"iam:*\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p></p> <ol> <li>Create policy</li> </ol> <pre><code>aws iam create-policy --policy-name my-iam-full-access-policy --policy-document file://policy.json\n</code></pre> <p></p> <ol> <li>Verify the policy</li> </ol> <pre><code>aws iam list-policies\n</code></pre> <p></p>"},{"location":"aws/iam/intro/#iam-roles","title":"IAM Roles","text":"<p>If some of the AWS services want to perform some action on our behalf, then we need to assign permission to those services. This permission is called as 'Roles'.</p> <p></p>"},{"location":"aws/iam/intro/#create-role","title":"Create Role","text":"<ol> <li>Go to IAM service. Click on the 'Roles' menu.</li> </ol> <ol> <li>Select the service and click on 'Next' button</li> </ol> <ol> <li>Select the policy and click on 'Next' menu.</li> </ol> <ol> <li>Enter the role name and click on 'Create role' button.</li> </ol> <p>Now the role is created.</p>"},{"location":"aws/iam/intro/#create-role-using-aws-cli","title":"Create Role using AWS CLI","text":"<ol> <li>Define policy</li> </ol> <p><code>policy.json</code></p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre> <ol> <li>Create the role</li> </ol> <pre><code>aws iam create-role --role-name &lt;RoleName&gt; --assume-role-policy-document file://trust-policy.json\n</code></pre> <pre><code>aws iam create-role --role-name my-ec2-role --assume-role-policy-document file://policy.json\n</code></pre> <p></p> <ol> <li>Attach a Policy to the Role</li> </ol> <pre><code>aws iam attach-role-policy --role-name &lt;RoleName&gt; --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n</code></pre> <pre><code>aws iam attach-role-policy --role-name my-ec2-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n</code></pre> <p></p> <ol> <li>Verify the Role</li> </ol> <pre><code>aws iam get-role --role-name &lt;RoleName&gt;\n</code></pre> <pre><code>aws iam get-role --role-name my-ec2-role\n</code></pre> <p></p>"},{"location":"aws/iam/intro/#iam-security-tools","title":"IAM Security Tools","text":"<ol> <li>IAM Credential Report (Account level) - Report the list of users and their current status in account level.</li> <li>IAM Access Advisor (User level) - Shows the service permission for each user and when the user last accessed the service.</li> </ol>"},{"location":"aws/vpc/create-vpc/","title":"Creating Virtual Private Cloud (VPC)","text":"<p>To create VPC in AWS, there are steps to follow.</p> <ol> <li>Create VPC and assign the CIDR block </li> <li>Create two subnets (one for public and one for private) in two different availability zones and assign the CIDR block for both subnets</li> <li>Create Internet Gateway and attach it to VPC</li> <li>Create two routing tables (one for public and one for private) and attach them with respective subnets</li> <li>Map routing table created for public subnet with Internet Gateway</li> <li>Create two security groups (one for public and one for private).</li> <li>Update the inbound rules for a public security group and allow access to <code>0.0.0.0/0</code> for SSH, HTTP, HTTPS, RDP. </li> <li>Update the inbound rules for a private security group and allow access to a public security group created at step # 4</li> <li>Create two EC2 instances (one for public and one for private) and map the respective security group</li> <li>Login into EC2 instance created in public subnet and check the internet connectivity. This should connect the internet</li> <li>Login into EC2 instance created in private subnet     (You cannot log in directly as it is private. You have to connect public EC2 instance and from there,     you can connect private EC2 instance) and check the internet connectivity. This should not connect the internet</li> <li>Create NAT Gateway and attach it private routing table</li> <li>Check if Elastic IP created</li> <li>Login into EC2 instance created in private subnet and check the internet connectivity. This should connect the internet</li> </ol> <p></p> <p>Let us explore the steps now.</p>"},{"location":"aws/vpc/create-vpc/#1-create-vpc-and-assign-the-cidr-block","title":"1. Create VPC and assign the CIDR block","text":"<p>To do so, open the VPC services and click on the \"Create VPC\" button</p> <p></p> <p>Update the fields as per snapshot. Then click on the \"Create VPC\" button.</p> <p></p> <p></p> <p>## 2. Create two subnets (one for public and one for private) in two different availability zones and assign the CIDR block for both subnets</p> <p>To do so, click on the \"Subnets\" menu and click on the \"Create Subnet\" button.</p> <p></p> <p>Update the fields as per snapshot. This is for public subnet</p> <p></p> <p>Click on the \"Create Subnet\" button</p> <p></p> <p></p> <p>Update the fields as per snapshot. This is for private subnet</p> <p></p> <p>Click on the \"Create Subnet\" button</p> <p></p> <p></p>"},{"location":"aws/vpc/create-vpc/#3-create-internet-gateway-and-attach-it-to-vpc","title":"3. Create Internet Gateway and attach it to VPC","text":"<p>To do so, click on the \"Internet gateways\" menu and click on the \"Create internet gateway\" button.</p> <p></p> <p>Update the fields as per snapshot.</p> <p></p> <p>Click on the \"Create internet gateway\" button</p> <p></p> <p>Select the internet gateway created and click on the \"Action\" dropdown menu</p> <p></p> <p>Click on the \"Attach to VPC\" option</p> <p></p> <p>Select the VPC (we created in the step # 1) and click on the \"Attach internet gateway\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#4-create-two-routing-tables-one-for-public-and-one-for-private-and-attach-them-with-respective-subnets","title":"4. Create two routing tables (one for public and one for private) and attach them with respective subnets","text":"<p>To do so, click on the \"Route tables\" menu and click on the \"Create route table\" button.</p> <p></p> <p>Update the fields as per snapshot. This is for public subnet.</p> <p></p> <p>Click on the \"Create route table\" button</p> <p></p> <p>To do so, click on the \"Route tables\" menu and click on the \"Create route table\" button.</p> <p></p> <p>Update the fields as per snapshot. This is for private subnet.</p> <p></p> <p>Click on the \"Create route table\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#attach-routing-table-with-respective-subnets","title":"Attach routing table with respective subnets","text":"<p>To do so, select the public routing table and click on the \"Subnet Association\" tab</p> <p></p> <p>Click on the \"Edit subnet association\" button</p> <p></p> <p>Select the public subnet created and click on the \"Save association\" button</p> <p></p> <p>Repeat the same steps for private routing table.</p> <p></p>"},{"location":"aws/vpc/create-vpc/#5-map-public-routing-table-with-internet-gateway","title":"5. Map public routing table with Internet Gateway","text":"<p>To do so, click on the \"Route tables\" menu and select the public routing table</p> <p></p> <p>Click on the \"Routes\" tab</p> <p></p> <p>Click on the \"Edit routes\" button</p> <p></p> <p>Click on the \"Add route\" button, and the destination value is \"0.0.0.0/0,\" select the \"internet gateway\" and then select the internet gateway we created from the option.</p> <p></p> <p>Click on the \"Save changes\" button.</p> <p></p>"},{"location":"aws/vpc/create-vpc/#6-create-two-security-groups-one-for-public-and-one-for-private","title":"6. Create two security groups (one for public and one for private).","text":"<p>To do so, click on the \"Security group\" menu and click on the \"Create security group\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#7-update-the-inbound-rules-for-a-public-security-group-and-allow-access-to-00000-for-ssh-http-https-rdp","title":"7. Update the inbound rules for a public security group and allow access to <code>0.0.0.0/0</code> for SSH, HTTP, HTTPS, RDP.","text":"<p>Update the fields as per snapshot and click on the \"Add rule\" button for inbound rules. This is for public security group.</p> <p></p> <p>Add the rules as per screenshot.</p> <p></p> <p>Click on the \"Create security group\" button</p> <p></p> <p>Click on the \"Security group\" menu and click on the \"Create security group\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#8-update-the-inbound-rules-for-a-private-security-group-and-allow-access-to-a-public-security-group-created-at-step-4","title":"8. Update the inbound rules for a private security group and allow access to a public security group created at step # 4","text":"<p>Update the fields as per snapshot and click on the \"Add rule\" button for inbound rules. This is for private security group.</p> <p></p> <p>Add the rules as per snapshot.</p> <p></p> <p>Click on the \"Create security group\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#9-create-two-ec2-instances-one-for-public-and-one-for-private-and-map-the-respective-security-group","title":"9. Create two EC2 instances (one for public and one for private) and map the respective security group","text":"<p>To do so, open the \"EC2\" service. Click on the \"Launch instance\" button. This is for public instance</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Create a new key pair for secured login.</p> <p></p> <p>Under network settings, click on the \"Edit\" button and update the details as per snapshot.</p> <p></p> <p>Click on the \"Launch instance\" button</p> <p></p> <p>Open the \"EC2\" service. Click on the \"Launch instance\" button. This is for private instance</p> <p></p> <p>Update the fields as per snapshot</p> <p></p> <p>Create a new key pair for secured login.</p> <p></p> <p>Under network settings, click on the \"Edit\" button and update the details as per snapshot.</p> <p></p> <p>Click on the \"Launch instance\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#10-login-into-ec2-instance-created-in-public-subnet-and-check-the-internet-connectivity","title":"10. Login into EC2 instance created in public subnet and check the internet connectivity.","text":"<p>Connect the public EC2 instance via terminal.</p> <p></p> <p></p> <p>Now, check if you are able to connect <code>google.com</code>. You are able to connect the <code>google.com</code> website.</p> <p></p>"},{"location":"aws/vpc/create-vpc/#11-login-into-ec2-instance-created-in-private-subnet","title":"11. Login into EC2 instance created in private subnet","text":"<p>You cannot log in directly as it is private. You have to connect public EC2 instance and from there,  you can connect private EC2 instance</p> <p>To do so, create the filename \"my-private-ec2-instance-key.pem\" in public EC2 instance. Copy the content from key created for private EC2 instance to this above file. Update the file permission.</p> <p></p> <p></p> <p>Now, check if you are able to connect <code>google.com</code>. You are unable to connect the <code>google.com</code> website.</p> <p></p>"},{"location":"aws/vpc/create-vpc/#12-create-nat-gateway-and-attach-it-private-routing-table","title":"12. Create NAT Gateway and attach it private routing table","text":"<p>To do so, open \"VPC\" services, click on the \"NAT gateway\" menu. Click on the \"Create NAT gateway\" button</p> <p></p> <p>Update the fields as per snapshot. Remember to choose public subnet because the internet will be provided from public to nat gateway. Then click on \"Allocate Elastic IP\" button.</p> <p></p> <p>Click on the \"Create NAT gateway\" button.</p> <p></p> <p></p>"},{"location":"aws/vpc/create-vpc/#13-check-if-elastic-ip-created","title":"13. Check if Elastic IP created","text":"<p>Elastic IP will be automatically created now. This IP address will act as frontend for private subnets.</p> <p></p>"},{"location":"aws/vpc/create-vpc/#attach-nat-gateway-with-private-routing-table","title":"Attach NAT gateway with private routing table","text":"<p>Open the private routing table and click on \"Route\" tab. Then click on the \"Edit routes\" button.</p> <p></p> <p>Update the fields as per snapshot.</p> <p></p> <p>Click on the \"Save changes\" button</p> <p></p>"},{"location":"aws/vpc/create-vpc/#14-login-into-ec2-instance-created-in-private-subnet-and-check-the-internet-connectivity","title":"14. Login into EC2 instance created in private subnet and check the internet connectivity.","text":"<p>Connect public EC2 instance and check the internet connection</p> <p></p> <p>Connect private EC2 instance from public EC2 instance and check the internet connection</p> <p></p> <p>Now the internet is flowing from end user to public EC2 and from public EC2 to private EC2 instance.</p>"},{"location":"aws/vpc/vpc-intro/","title":"Virtual Private Cloud (VPC)","text":"<p>Before knowing AWS VPC, we need some basic understanding of networking. Let us understand the OSI model now.</p>"},{"location":"aws/vpc/vpc-intro/#osi-model-open-systems-interconnection","title":"OSI Model (Open Systems Interconnection)","text":"<p>There are seven different layers in the OSI model.</p> <ol> <li>Physical layer</li> </ol> <p>This is just a physical components such as ports where a plug connects, cable etc.  You have network interface card (NIC) that connects the port via network adapter. The communication happens using electronic signals and bits (0 and 1).</p> <p></p> <ol> <li>Data link layer</li> </ol> <p>We have MAC address, switches, ethernet protocol in layer 2. There is a unique address associated with NIC called as media access control (MAC) address. Ethernet is the protocol used in layer 2 (DLL), and it is also referred as 802.1x.  Switches is used to connect two different devices using its MAC address. Switch creates MAC address table like below to know where the MAC address is and which port it is associated with.</p> Address Port No MAC1 Port1 MAC2 Port2 <p>Based on the above details, the connection is made from one port to another port.</p> <p></p> <ol> <li>Network layer</li> </ol> <p>We have IP address, routers and routing protocol (ICMP - Internet Control Message Protocol) in this layer. All the routing infrastructure available in layer 3.  Routers route the traffic based on IP address and their associated interface. Routers maintain the routing table like below</p> Destination Interface 10.0.0.1 eth0 10.4.6.4 eth1 <p>Interfaces associated with different network and routers routing the traffic based on IP address and its interface.</p> <p></p> <ol> <li>Transport layer</li> </ol> <p>We have TCP (Transmission Control Protocol) protocol in this layer and is associated with IP address. It is a connection oriented protocol. It is a three-way handshake, and the connection is established in this layer. Source sends a request named as \"SYN\" - synchronise your sequence bits (first-way) and gets the acknowledgment back from destination named as \"SYN-ACK.(second-way)\" Then the source sends the acknowledgment named as \"ACK\" (third-way) and the connection is made between source and destination. This will constantly check the connection is working and run correctly. </p> <p>We have UDP (User Datagram Protocol) as well in this layer, and it is connectionless. This is splits the data and sends them. There is no tracking of the data sent. But it will not be sure if it reaches in  the destination.</p> <p></p> <ol> <li>Session layer</li> </ol> <p>We have TCP and RPC (Remote Procedure Call) in this layer, and the interhost communication happens in this layer.</p> <ol> <li>Presentation layer</li> </ol> <p>We have SSL (Single Socket Layer), SSH (Secure Shell), FTP (File Transfer Protocol) in this layer. It is all about the data encryption and data representation in this layer.</p> <ol> <li>Application layer</li> </ol> <p>We have HTTP, FTP, SSH, DNS in this layer. It's all about the network services to the end user application.</p> Layer Component Protocol/Components Communication Physical Bits Fiber, Cable, Wireless Media, signal, binary transmission Data link Frames Ethernet, 802.1x Physical addressing (MAC) Network Packets IP, IP security, ICMP Path determination (Routing) Transport Segments TCP, UDP, TLS End to end connection and reliability Session Data TCP, RPC Interhost communication Presentation Data SSL, FtP, HTML, SSH Data representation and encryption Application Data HTTP, FTP, DNS, SSH Network services to end use application"},{"location":"aws/vpc/vpc-intro/#tcp-encapsulation","title":"TCP Encapsulation","text":"<p>One computer wants to send some data to another computer. The data is from layer 5\u20137 splits into chunks and put into layer 4 (Transport layer). Transport layer added the TCP header which contains the port number to connect. Then it goes to the network layer. In the network layer, IP header is added to the data, and it forms the packets. IP header has all the destination details. Then it goes to the data link layer. It adds an Ethernet header and puts it in to the network. Then the data goes to the physical layer where it converts to bits, and the process will be reversed once it reaches the destination.</p>"},{"location":"aws/vpc/vpc-intro/#routers-switches-and-firewall","title":"Routers, Switches and Firewall","text":"<p>Network is nothing but the connection between computers. Computers are connected via switches in the network.</p> <p>If there are two different networks (IP address range is different), then they are connecting via routers. Routers maintain the routing table associated with IP address and its interface. Based on it, it will connect two different networks. </p> <p></p> <p>In the above picture, Subnet A and Subnet B have different IP address range. Subnet A connects Subnet B using routers using its interface information maintained in the routing table.</p> <p>Firewall is a security devices. It checks the incoming and outgoing traffic in our network and filters them if it meets certain rules. Firewall rules are maintained in a table, and that will decide if the traffic is allowed or denied based on the protocol, source and destination.</p> <p></p> <p>Firewall can be placed in different layers in the OSI model. It can be placed in the server level such as EC2 instance security group. </p> <p></p>"},{"location":"aws/vpc/vpc-intro/#structure-of-ip-address","title":"Structure of IP Address","text":"<p>IP address written in dotted decimal notation. Each number is a binary octave, and it should be between 0 and 255. IP address has network id and host id. If the IP address is 193.5.6.93, then 193.5.6 is network id and 93 is host id.</p> <p>Example</p> <p>193.5.6.8 is correct IP. 10.245.344.53 is not correct IP as it has a binary octave value more than 255.</p>"},{"location":"aws/vpc/vpc-intro/#cidr-classless-inter-domain-routing","title":"CIDR - Classless Inter-Domain Routing","text":"<p>It is a method of allocating IP address for IP routing. Let us assume that there are 100 machines available in a network, and we want to assign IP address to all of them. Remember, IP address is unique across the world. In a network, network id is constant (unless its range is exceeded), and host id will vary based on the  number of machines available in a network. If we want to assign IP address from 195.8.4.0 to 195.8.4.99 to all 100 machines, then we can update like 195.8.4.0/<code>&lt;value&gt;</code>.</p> <p><code>value</code> decides the number of IP addresses. </p>"},{"location":"aws/vpc/vpc-intro/#how-ip-address-ipv4-forms","title":"How IP address (IPv4) forms","text":"<p>Let us understand how the IP address forms first. As we know, IP address is a binary octave, and its 32 bits.</p> <p><code>00110001.00100101.11011010.00001101</code> is actual IP address in the network (All are in 0 and 1 - binary).</p> <p>Let us split the individual octave and form the number. Consider the first octave</p> <p></p> <p>Similarly, we calculated for other three octaves as well.</p> <p></p> <p>So the final IP address (IPv4) is <code>49.37.218.13</code></p>"},{"location":"aws/vpc/vpc-intro/#how-many-numbers-of-ip-addresses-ipv4-decide-in-a-network","title":"How many numbers of IP addresses (IPv4) decide in a network?","text":"<p>As it is an octave (eight digits), the bits are defined as 8 + 8 + 8 + 8 = 32 bits (Maximum value is 32). So, if the IP address is mentioned as <code>49.37.218.0/32</code>, it means that there is only one IP address <code>49.37.218.0</code> If the IP address is mentioned as <code>49.37.218.0/31</code>, it means that there are two IP addresses <code>49.37.218.0</code> and <code>49.37.218.1</code>. <code>The pattern is 2 to the power of (32 - value after /).</code></p> <p>For example, the IP address is mentioned as <code>49.37.218.0/24</code>, then it is calculated as <code>32-24 = 8</code>. The number of IP addresses is 2 to the power of 8 which is 256. So the IP address range is <code>49.37.218.0 to 49.37.218.255</code></p> CIDR value Number of IP Address IP Address Range 49.37.218.0/32 1 49.37.218.0 49.37.218.0/31 2 49.37.218.0 to  49.37.218.1 49.37.218.0/30 4 49.37.218.0 to  49.37.218.3 49.37.218.0/29 8 49.37.218.0 to  49.37.218.7 49.37.218.0/28 16 49.37.218.0 to  49.37.218.15 49.37.218.0/27 32 49.37.218.0 to  49.37.218.31 49.37.218.0/26 64 49.37.218.0 to  49.37.218.63 49.37.218.0/25 128 49.37.218.0 to  49.37.218.127 49.37.218.0/24 256 49.37.218.0 to  49.37.218.255 49.37.218.0/23 512 49.37.218.0 to  49.37.218.255 49.37.219.0 to  49.37.219.255 ... ... ... 49.37.218.0/1 2147483647 This range is too long. Hence, it is not updated here"},{"location":"aws/vpc/vpc-intro/#how-ip-address-ipv6-forms","title":"How IP address (IPv6) forms","text":"<p>As you know, IPv4 is 32 bits in size. However, IPv6 is 128 bits in size.  </p> <p></p> <p>Hence, all the binary numbers need to be converted into hexadecimal to form an IP address.</p> <p></p> <p>So in the above case, the ip address is <code>2001:0db8:0000:0000:a111:b222:c333:abcd</code></p> <p>There are rules to follow in IPv6.</p> <ol> <li>If IP address has <code>0000:0000</code>, then it can be written as <code>::</code></li> <li>If IP address has <code>0000</code>, then it can be written as <code>0</code></li> </ol> <p>So the above IP address can be rewritten as <code>2001:0db8::a111:b222:c333:abcd</code>.</p>"},{"location":"aws/vpc/vpc-intro/#stateful-vs-stateless-firewall","title":"Stateful Vs Stateless Firewall","text":"<p>Let us assume that client is sending a request to the webserver and web server is responding to the request. We need to apply the inbound rule in web server to accept the request from the client machine. If a web server is responding the request to the client without opening any outbound rule, then it is a stateful firewall.</p> <p>If a web server is responding the request to the client only if the outbound rule is created, then it is a stateless firewall.</p>"},{"location":"aws/vpc/vpc-intro/#security-group-sg-vs-nacl-network-access-control-list","title":"Security Group (SG) Vs NACL (Network Access Control List)","text":"<p>Security group and NACL are two different firewall setups. NACL is applied in the subnet level whereas SG is applied in the instance level. The instances can be part of any subnet. SG is a stateful firewall and NACL is a stateless firewall.</p> Security Group NACL Applied at the instance level Applied at the subnet level (Subnet can have multiple instances and it applies all the instances within the subnet Support allow rules only Support allow and deny rules Stateful firewall Stateless firewall Evaluate all rules at one shot Process the rule in the order and stop if any rule satisfies. After that it will not check subsequent rules"},{"location":"aws/vpc/vpc-peering/","title":"VPC Peering","text":"<p>Let us assume that there are two VPCs created in two different regions (One in N.Virginia and one in Mumbai). There are few EC2 instances running in both VPCs. If EC2 instances running in one VPC need to talk communicate to other EC2 instances running in another VPC, then we need VPC peering.</p> <p></p> <p>Rules</p> <ol> <li>If one VPC from one region needs to talk to another VPC, VPC peering is needed.</li> <li>Assume there are three VPCs (A, B, C). A is peering with B, B is peering with C.  So, A cannot talk to C unless there is peering configured between A and C </li> <li>We can do VPC peering from VPC in one account to VPC in another account as well.</li> </ol>"},{"location":"aws/vpc/vpc-peering/#steps-for-vpc-peering","title":"Steps for VPC Peering","text":"<ol> <li>Create two VPCs in two different regions</li> <li>Create subnets for each VPC</li> <li>Create internet gateway for each VPC</li> <li>Create route tables for each VPC</li> <li>Create a security group for each VPC</li> <li>Create EC2 instances for each VPC</li> <li>Create VPC peering connection and associate them with VPC</li> <li>Login and check connectivity</li> </ol>"},{"location":"aws/vpc/vpc-peering/#create-two-vpcs-in-two-different-regions","title":"Create two VPCs in two different regions","text":"<p>For N.Virginia Region</p> <p>To do so, open the VPC services and click on the \"Create VPC\" button</p> <p></p> <p>Update the fields as per snapshot. Then click on the \"Create VPC\" button.</p> <p></p> <p></p> <p>For Mumbai Region</p> <p>To do so, open the VPC services and click on the \"Create VPC\" button</p> <p></p> <p>Update the fields as per snapshot. Then click on the \"Create VPC\" button.</p> <p></p> <p></p>"},{"location":"aws/vpc/vpc-peering/#2-create-subnets-for-each-vpc","title":"2. Create subnets for each VPC","text":"<p>For N.Virginia Region</p> <p>To do so, click on the \"Subnets\" menu and click on the \"Create Subnet\" button.</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p></p> <p>For Mumbai Region</p> <p>To do so, click on the \"Subnets\" menu and click on the \"Create Subnet\" button.</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p></p>"},{"location":"aws/vpc/vpc-peering/#3-create-internet-gateway-for-each-vpc-and-attach-it-with-vpc","title":"3. Create internet gateway for each VPC and attach it with VPC","text":"<p>For N.Virginia Region</p> <p>To do so, click on the \"Internet gateways\" menu and click on the \"Create internet gateway\" button.</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Click on the \"Create internet gateway\" button</p> <p></p> <p>Select the internet gateway created and click on the \"Action\" dropdown menu</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Click on the \"Attach internet gateway\" button</p> <p></p> <p>For Mumbai Region</p> <p>To do so, click on the \"Internet gateways\" menu and click on the \"Create internet gateway\" button.</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Click on the \"Create internet gateway\" button</p> <p></p> <p>Select the internet gateway created and click on the \"Action\" dropdown menu</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Click on the \"Attach internet gateway\" button</p> <p></p>"},{"location":"aws/vpc/vpc-peering/#4-create-route-tables-for-each-vpc","title":"4. Create route tables for each VPC","text":"<p>For N.Virginia Region</p> <p>To do so, click on the \"Route tables\" menu and click on the \"Create route table\" button.</p> <p></p> <p>Update the fields as per snapshot</p> <p></p> <p>Click on the \"Create route table\" button</p> <p></p> <p>Click on the \"Subnet association\" tab. Click on the \"Edit subnet association\" button</p> <p></p> <p>Select subnet and click on the \"Save association\" button</p> <p></p> <p>Click on the \"Routes\" tab and click on the \"Edit routes\" button</p> <p></p> <p>Click on the \"Add route\" button and update the fields like below</p> <p></p> <p>Click on the \"Save changes\" button</p> <p>For Mumbai Region</p> <p>To do so, click on the \"Route tables\" menu and click on the \"Create route table\" button.</p> <p></p> <p>Update the fields as per snapshot</p> <p></p> <p>Click on the \"Subnet association\" tab. Click on the \"Edit subnet association\" button</p> <p></p> <p>Select subnet and click on the \"Save association\" button</p> <p></p> <p>Click on the \"Routes\" tab and click on the \"Edit routes\" button</p> <p></p> <p>Click on the \"Add route\" button and update the fields like below</p> <p></p> <p>Click on the \"Save changes\" button</p>"},{"location":"aws/vpc/vpc-peering/#5-create-a-security-group-for-each-vpc","title":"5. Create a security group for each VPC","text":"<p>For N.Virginia Region</p> <p>To do so, click on the \"Security group\" menu and click on the \"Create security group\" button</p> <p></p> <p>Update the fields as per snapshot</p> <p></p> <p>Click on the \"Create security group\" button</p> <p></p> <p>For Mumbai Region</p> <p>To do so, click on the \"Security group\" menu and click on the \"Create security group\" button</p> <p></p> <p>Update the fields as per snapshot</p> <p></p> <p>Click on the \"Create security group\" button</p> <p></p>"},{"location":"aws/vpc/vpc-peering/#6-create-ec2-instances-for-each-vpc","title":"6. Create EC2 instances for each VPC","text":"<p>For N.Virginia Region</p> <p>To do so, open the \"EC2\" service. Click on the \"Launch instance\" button. </p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Create a key pair</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Click on the \"Launch instance\" button</p> <p></p> <p>For Mumbai Region</p> <p>To do so, open the \"EC2\" service. Click on the \"Launch instance\" button. </p> <p></p> <p>Follow the similar steps to launch EC2 instance like the Virginia region.</p> <p>Create a key pair</p> <p></p> <p>Update the fields as per snapshot. </p> <p></p> <p>Click on the \"Launch instance\" button</p> <p></p>"},{"location":"aws/vpc/vpc-peering/#7-create-vpc-peering-connection-in-only-one-region-and-associate-them-with-vpc-in-another-region","title":"7. Create VPC peering connection (in only one region) and associate them with VPC (in another region)","text":"<p>To do so, click on the \"Peering connection\" menu and click on the \"Create peering connection\" button</p> <p></p> <p>Update the fields as per snapshot.</p> <p></p> <p>Now VPC peering connection is created in the N.Virginia region.</p> <p></p> <p>It needs to be accepted in the Mumbai region now.</p> <p></p> <p></p>"},{"location":"aws/vpc/vpc-peering/#8-login-and-check-connectivity","title":"8. Login and check connectivity","text":"<p>Connect the EC2 instance in the N.Virginia region</p> <p></p> <p></p> <p>Pinging google.com works fine.</p> <p></p> <p>We will try to connect the EC2 instance created in the Mumbai region from here.</p> <p></p> <p>It is not able to connect. So, VPC peering from N.Virginia region to the Mumbai region is not working. The reason is that routing tables need to be modified to accept the subnet from both VPCs.</p> <p>Update the routing table in the N.Virginia region. Add the subnet range of Mumbai region VPC.</p> <p></p> <p>Update the routing table in the Mumbai region. Add the subnet range of N.Virginia region VPC.</p> <p></p> <p>Now try to ping the EC2 instance from Virginia to Mumbai EC2 instance. It works fine.</p> <p></p>"},{"location":"bash/advanced-if-else/","title":"Advanced if usage","text":""},{"location":"bash/advanced-if-else/#ifthenelse-constructs","title":"if/then/else constructs","text":"<p>This is used to take one course of action if the condition is true, and another if condition is false.</p>"},{"location":"bash/advanced-if-else/#syntax","title":"Syntax","text":"<pre><code>if [ condition ]\nthen\n  statement belongs to condition is true\nelse\n  statement belongs to condition is false\nfi\n</code></pre> <p>Example <pre><code>#!/bin/bash\nnumber1=5\nnumber2=6\nif [ $number1 -eq $number2 ]\nthen\n  echo \"$number1 is equal to $number2\"\nelse\n  echo \"$number1 is not equal to $number2\"\nfi\n</code></pre> </p> <p>Getting input from commandline in runtime <pre><code>first_person_age=$1\nsecond_person_age=$2\nif [ $first_person_age -gt $second_person_age ]\nthen\n  echo \"first_person age $first_person_age is elder than second_person age $second_person_age\"\nelse\n  echo \"first_person age $first_person_age is younger than second_person age $second_person_age\"\nfi\n</code></pre> </p> <p>NOTE</p> <p>$0 refers to the name of the script</p> <p>$1, $2 ... $n refers to the positional parameters</p> <p>$# refers to the number of command line arguments</p> <p>$@ refers to all the positional parameters passed to a script as separate words</p> <p>$* refers to all the positional parameters passed to a script as single words</p> <p>Examples <pre><code>#!/bin/bash\nfirst_person_name=$1\nsecond_person_name=$2\nif [ $first_person_name = $second_person_name ]\nthen\n  echo \"$first_person_age is elder than $second_person_age\"\nelse\n  echo \"$first_person_age is younger than $second_person_age\"\nfi\n</code></pre> In below snapshot, $0 =&gt; if_else.sh, $1 =&gt; 25 and $2 =&gt; 8 (Positional parameters)  </p> <p><pre><code>#!/bin/bash\necho \"Total number of arguments are $number\"\n</code></pre> In below snapshot, $# counts the number of arguments passed to the shell script  </p> <p><pre><code>#!/bin/bash\nfor number in \"$@\"\ndo\n  echo \"Argument $number\"\ndone\n</code></pre> In below snapshot, each argument is considered as separate word  </p> <p><pre><code>#!/bin/bash\nfor number in \"$*\"\ndo\n  echo \"Argument $number\"\ndone\n</code></pre> In below snapshot, all arguments are considered as one word  </p>"},{"location":"bash/advanced-if-else/#ifthenelifelse-constructs","title":"if/then/elif/else constructs","text":""},{"location":"bash/advanced-if-else/#syntax_1","title":"Syntax","text":"<pre><code>if [ condition ]\nthen\n  statement belongs to condition is true\nelif [ some_condition ]\nthen\n  statement belongs to some_condition is true  \nelse\n  statement belongs to condition is false\nfi\n</code></pre> <p>NOTE: Multiple elif block can be possible.</p> <p>Example</p> <p><pre><code>#!bin/bash\nmonth_number=$1\n\nif [ $# -ne 2 ]\nthen\n  echo \"No month number given. Please enter a month number as a command line argument.\"\n  echo \"eg: ./print-month-number 5\"\n  exit\nfi\n\nif [[ $month_number -lt 1 ]] || [[ $month_number -gt 4 ]]\nthen\n  echo \"Invalid month number given. Please enter a valid number - 1 to 12.\"\n  exit\nfi\n\nif [ $month_number -eq 1 ]\nthen\n  echo \"First quarter\"\nelif [ $month_number -eq 2 ]\nthen\n  echo \"Second quarter\"\nelif [ $month_number -eq 3 ]\nthen\n  echo \"Third quarter\"\nelif [ $month_number -eq 4 ]\nthen\n  echo \"Fourth quarter\"\nfi\n</code></pre> </p>"},{"location":"bash/advanced-if-else/#nested-if-statements","title":"Nested if statements","text":""},{"location":"bash/advanced-if-else/#syntax_2","title":"Syntax","text":"<pre><code>if [ condition ]\nthen\n  statement belongs to condition is true\nelif [ some_condition ]\nthen\n  statement belongs to some_condition is true\n  if [ anothe_condition]\n  then\n    statement belongs to another_condition is true\n  else\n    statement belongs to another_condition is false\n  fi\nelse\n  statement belongs to condition is false\nfi\n</code></pre> <p>Example</p> <p><pre><code>#!/bin/bash\nyear=`date +%Y`\n\nif [ $[$year % 400] -eq \"0\" ]; then\n  echo \"This is a leap year.  February has 29 days.\"\nelif [ $[$year % 4] -eq 0 ]; then\n        if [ $[$year % 100] -ne 0 ]; then\n          echo \"This is a leap year, February has 29 days.\"\n        else\n          echo \"This is not a leap year.  February has 28 days.\"\n        fi\nelse\n  echo \"This is not a leap year.  February has 28 days.\"\nfi\n</code></pre> </p>"},{"location":"bash/advanced-if-else/#using-the-exit-statement","title":"Using the exit statement","text":"<ol> <li> <p><code>exit</code> statement terminates execution of the entire script. It is used if the user input is incorrect, if a statement did not run successfully or if some other error occurred.</p> </li> <li> <p><code>exit</code> statement takes an optional argument. This argument is the integer (exit status code), which is passed back to the parent and stored in the $? variable.</p> </li> <li> <p>A zero exit status code means that the script ran successfully</p> </li> <li> <p>Any other exit status code may be used to pass back different messages to the parent, so that different actions can be taken according to failure or success of the child process</p> </li> <li> <p>If no argument is given to the exit command, the parent shell uses the current value of the $? variable.</p> </li> </ol> <p>NOTE</p> <p><code>exit</code> is different from <code>return</code>. <code>exit</code> is used to stop the script whereas <code>return</code> is used to stop the function.</p> <p>Example</p> <p><pre><code>#!/bin/bash\n\nif [ $# != 1 ]; then\n  echo \"Usage: $0 10\"\n  exit 1\nfi\nif [ $1 -eq 10 ] ; then\n  echo \"Given argument is 10\"\nelse\n  echo \"Given argument is NOT 10\"\n  exit 10\nfi\n</code></pre> </p>"},{"location":"bash/advanced-if-else/#using-case-statements","title":"Using case statements","text":"<p>case statement will be useful when you are working on menu-driven script.</p>"},{"location":"bash/advanced-if-else/#syntax_3","title":"Syntax","text":"<p>In the below code snippet, if any of the \"option\" matches with \"value\", then the respect block of statements will be executed. If none of the \"option\" matches with \"value\", then * block of statements will be executed. <pre><code>case &lt; value &gt; in\n  option) statement\n    ;;\n  option) statement\n    ;;\n  *) statement\n    ;;\nesac\n</code></pre></p> <p>NOTE</p> <p>If any of the block executed, then the rest will be omitted. In other words, at any point of time, ONLY ONE block will be executed (If * is included)</p> <p>Example</p> <p><pre><code>#!/bin/bash\nspace=`df -h | awk '{print $5}' | grep % | grep -v Use | sort -n | head -1 | cut -d \"%\" -f1 -`\n\ncase $space in\n[1-6]*)\n  Message=\"All is quiet\"\n  ;;\n[7-8]*)\n  Message=\"Start thinking about cleaning out some stuff.  There's a partition that is $space % full\"\n  ;;\n9[1-8])\n  Message=\"Better hurry with that new disk...  One partition is $space % full\"\n  ;;\n99)\n  Message=\"I'm drowning here!  There's a partition at $space %!\"\n  ;;\n*)\n  Message=\"I seem to be running with an nonexistent amount of disk space\"\n  ;;\nesac\n\necho $Message as $space disk is used\n</code></pre> </p>"},{"location":"bash/condition-statement/","title":"Introduction to conditional statement (If)","text":"<p>Sometimes, you need to take different action depending on the success or failure of a command.  The if statement helps you in such conditions.</p>"},{"location":"bash/condition-statement/#syntax","title":"Syntax","text":"<p><pre><code>if [ condition ]\nthen \n    statements 1\n    statements 2\n    .\n    .\n    statements n\nfi\n</code></pre> <pre><code>if [[ condition ]]\nthen \n    statements 1\n    statements 2\n    .\n    .\n    statements n\nfi\n</code></pre> The condition is true or returns 0 (zero), the statements get executed.</p>"},{"location":"bash/condition-statement/#note","title":"NOTE:","text":"<ol> <li> <p>There should be a space with in square brackets at starting and ending.  Picture courtesy: https://kodekloud.com/</p> <p>Example <code>if [[ 5 -gt 0 ]]</code></p> </li> <li> <p>The condition often involves numerical or string comparison tests.</p> </li> <li>The condition returns a status of zero when it succeeds and some other status when it fails.</li> <li>The condition may be a command (e.g <code>if [ $(whoami) = \"root\" ]</code>)</li> <li>Unary expressions are often used to examine the status of a file</li> </ol>"},{"location":"bash/for-loop/","title":"The for loop","text":"<p>In general, for loop can be used in the below scenarios.</p> <ul> <li>Executes a command or set of commands many times</li> <li>Iterate through files</li> <li>Iterate through lines within the file</li> <li>Iterate through output of the command </li> </ul>"},{"location":"bash/for-loop/#how-does-it-work","title":"How does it work?","text":"<p>This loop allows for specification of a list of values. A list of commands is executed for each value in the list.</p>"},{"location":"bash/for-loop/#syntax","title":"Syntax","text":"<pre><code>for &lt;variable&gt; in [ LIST ] \ndo \n    COMMANDS/statements\ndone\n</code></pre> <p>whereas, LIST can be any list of words, strings or numbers or generated by any command. </p> <p>The first time through the loop, \"variable\" is set to the first item in LIST.  The second time, its value is set to the second item in the list, and so on. </p> <p>The loop terminates when no items are left in LIST. The return status is the exit status of the last command that executes in the for loop.  If no commands are executed, the return status is zero.</p>"},{"location":"bash/for-loop/#with-strings","title":"With Strings","text":""},{"location":"bash/for-loop/#example","title":"Example","text":"<p><pre><code>#!/bin/bash\nfor name in Sachin Rahul Saurav Viru Anil Javagal Kapil Zaheer Harbhajan\ndo\n  echo \"${name}\"\ndone\n</code></pre> </p>"},{"location":"bash/for-loop/#note","title":"NOTE:","text":"<ul> <li>When [ LIST ] is empty and number of arguments are passed through runtime as environmental variable, then [ LIST] is replaced with in $@ </li> </ul>"},{"location":"bash/for-loop/#example_1","title":"Example","text":"<p>Assume below code snippet saved as shell script and while executing this shell script, we need to pass the input list as an arguments. <pre><code>for name in $@\ndo\n  echo \"${name}\"\ndone\n</code></pre> </p> <ul> <li>We can pass the list from the file as well. We can use backquotes to pass the input file as shown below. This is one way of passing the input list. <pre><code>for name in `cat names.txt`\ndo\n  echo \"${name}\"\ndone\n</code></pre> </li> </ul>"},{"location":"bash/for-loop/#output","title":"Output","text":"<ul> <li>Another way of passing the input list is within parenthesis. This is the best practice to follow. The reason is that you can pass multiple commands within parenthesis. <pre><code>for name in $(cat names.txt | grep -i il)\ndo\n  echo \"${name}\"\ndone\n</code></pre> </li> </ul>"},{"location":"bash/for-loop/#output_1","title":"Output","text":""},{"location":"bash/for-loop/#remember","title":"REMEMBER:","text":"<ul> <li>You can pass multiple commands within backquotes too (like below). But that is not a suggestible way. <pre><code>for name in `cat names.txt | grep -i il`\ndo\n  echo \"${name}\"\ndone\n</code></pre></li> </ul>"},{"location":"bash/for-loop/#with-numbers","title":"With Numbers","text":"<p><pre><code>for number in 1 2 3 4 5\ndo\n  echo \"${number}\"\ndone\n</code></pre> </p> <p>The same code can be little modified with {starting_number..ending_number} instead of individual number. <pre><code>for number in {1..5}\ndo\n  echo \"${number}\"\ndone\n</code></pre></p> <p>Note</p> <p>There is no space within the curly braces. If there is any space, then that will be considered as separate string</p> <pre><code>for number in { 1..5 }\ndo\n echo \"${number}\"\ndone\n</code></pre> <p>Output will be</p> <pre><code>{\n1..5\n}\n</code></pre>"},{"location":"bash/for-loop/#c-style-for-loop-with-numbers","title":"C style for loop with numbers","text":"<p><pre><code>for((index=1; index&lt;=5; index++))\ndo\n  touch file${index}.txt\n  echo \"${index}\" &gt; file\"${index}.txt\"\n  echo \"Number of lines in file${index}:$(cat file${index}.txt |wc -l)\"\ndone\n</code></pre> </p>"},{"location":"bash/for-loop/#use-of-break-command","title":"Use of break command","text":"<p>Break command is used to exit of the immediate loop.</p>"},{"location":"bash/for-loop/#syntax_1","title":"Syntax","text":"<pre><code>for &lt;variable&gt; in [ LIST ]\ndo\n  statements/COMMANDS\n  if [[ condition ]]\n  then\n    break # In general, break can be used in loops to check if certain condition is met. If the condition is met, then exit of the loop\n  fi\ndone\n</code></pre>"},{"location":"bash/for-loop/#example_2","title":"Example","text":"<p>Let us assume, if the file contains many names. We would like to search a particular name if it exists in the file. If found, we can stop the search. To do this, <pre><code>#!/bin/bash\n# This script is to just print the given number if it is less than 4.\nfor name in $(cat names.txt)\ndo\n  echo \"${name}\"\n  if [[ \"${name}\" == \"Kapil\" ]]\n  then\n    echo \"Name ${name} found\"\n    break\n  fi\ndone\n</code></pre> </p> <p>Another example with nested for loop along with break command <pre><code>#!/bin/bash\n## This script is to just print the given number and also names for each number\n\nfor number in {1..5} # OUTER FOR LOOP\ndo\n  echo ${number}\n  for name in Satheesh Jeganathan Adhira Praba # INNER FOR LOOP\n  do\n    echo ${number}. ${name}\n    if [[ \"${name}\" == \"Jeganathan\" ]]; then\n      echo ${name} is found and breaking my immediate loop\n      break # BREAK ONLY INNER FOR LOOP\n    fi\n  done\n\n  if [[ ${number} -eq 3 ]]; then\n    break # BREAK ONLY OUTER FOR LOOP\n  fi\ndone\n</code></pre> </p>"},{"location":"bash/for-loop/#use-of-continue-command","title":"Use of continue command","text":"<p>Continue command is used to skip the current iteration and move to next iteration in the loop.</p>"},{"location":"bash/for-loop/#syntax_2","title":"Syntax","text":"<pre><code>for &lt;variable&gt; in [ LIST ]\ndo\n  statements/COMMANDS\n  if [[ condition ]]\n  then\n    continue # In general, continue can be used in loops to check if certain condition is met. If the condition is met, then skip that iteration \n  fi\ndone\n</code></pre>"},{"location":"bash/for-loop/#example_3","title":"Example","text":"<p><pre><code>#!/bin/bash\n## This script is to just print the player who's name is not Saurav.\nfor name in $(cat names.txt)\ndo\n  if [[ \"${name}\" == \"Saurav\" ]]\n  then\n    continue # Skip the rest of the statements in for loop (immediate)\n  fi\n  echo \"${name} is selected for the tournament\"\ndone\n</code></pre> </p>"},{"location":"bash/ifs/","title":"IFS (Internal Field Separator)","text":"<ul> <li>IFS is used to split the word, array, list by using a delimiter. </li> <li>By default, IFS is set to space, tab, or new line.</li> <li>If the string/word has any of the above values (space/tab/new line), Shell uses IFS implicitly to split and manipulate. Example is below. We don't need to mention IFS in our coding. <pre><code>#!/usr/bin/env bash\nelements=\"satheesh pandian jeganathan\"\nfor element in ${elements}\ndo\n  echo ${element} is seperated\ndone \n</code></pre> </li> </ul> <p><code>NOTE: In the above example, elements in for loop is not surronded by quote. If it is surrounded, then all the values considered as one word. Example, refer below</code> <pre><code>#!/usr/bin/env bash\nelements=\"satheesh pandian jeganathan\"\nfor element in \"${elements}\"\ndo\n  echo ${element} is seperated\ndone \n</code></pre> </p> <ul> <li>We can also overwrite the default value by specifying custom IFS i.e   <code>IFS=\",\"</code>. Comma is a separator in this case. In the below example, \":\" is used as a separator <pre><code>#!/usr/bin/env bash\nIFS=\":\"\nelements=\"satheesh:pandian:jeganathan\"\nfor element in ${elements}\ndo\n  echo ${element} is seperated\ndone \n</code></pre> </li> </ul> <p>In case of mixed list (spaces and custom IF values), the priority goes to custom IFS value. <pre><code>#!/usr/bin/env bash\nIFS=\":\"\nelements=\"satheesh:pandian jeganathan:from bangalore:karnataka\"\nfor element in ${elements}\ndo\n  echo ${element} is seperated\ndone \n</code></pre> </p> <p>So far, we have assigned a value into the variable and used the variable in for loop. Instead, we can use the value directly in for loop without assigning them into variable. Let us see some examples. <pre><code>#!/usr/bin/env bash\nIFS=\":\"\nfor element in \"satheesh:pandian jeganathan:from bangalore:karnataka\"\ndo\n  echo ${element} is seperated\ndone \n</code></pre> <pre><code>#!/usr/bin/env bash\nIFS=\":\"\nfor element in satheesh:pandian jeganathan:from bangalore:karnataka\ndo\n  echo ${element} is seperated\ndone \n</code></pre> <code>NOTE: If we are passing the value directly in for loop without quotes surrounded, then the priority goes to default, NOT custom IFS. In the above example, \":\" is NOT priority. Instead, space(\" \") is the priority</code></p>"},{"location":"bash/ifs/#remember-that-ifs-only-takes-effect-on-expanded-values-if-we-want-to-override-the-values-of-ifs-to-something-custom-inside-the-script-etc-then-must-go-through-expansion-process","title":"Remember that IFS ONLY takes effect on expanded values. If we want to override the values of IFS to something custom inside the script (\":\",\",\" etc) then $@/$* must go through expansion process","text":""},{"location":"bash/looping-concepts/","title":"Looping Concepts","text":"<p>There are three looping can be used in shell scripting. They are</p> <ul> <li><code>For Loop</code>.</li> <li><code>While Loop</code>.</li> <li><code>Until Loop</code>.</li> </ul> <p>To change the flow of the loop, there are two commands can be used. They are</p> <ul> <li><code>break</code> - To break the loop and come out of the immediate loop execution</li> <li><code>continue</code> - To skip the iteration and continue the next iteration of the loop</li> </ul> <p>Let's see how each loop is working</p>"},{"location":"bash/primary-expression/","title":"Primary expressions with If condition","text":"Primary Expression Meaning <code>[ -a FILE ]</code> True if FILE exists <code>[ -b FILE ]</code> True if FILE exists and is a block-special file <code>[ -c FILE ]</code> True if FILE exists and is a character-special file <code>[ -d FILE ]</code> True if FILE exists and is a directory <code>[ -e FILE ]</code> True if FILE exists <code>[ -f FILE ]</code> True if FILE exists and is a regular file <code>[ -h FILE ]</code> True if FILE exists and is a symbolic link <code>[ -p FILE ]</code> True if FILE exists and is a named pipe (FIFO) <code>[ -r FILE ]</code> True if FILE exists and is readable <code>[ -s FILE ]</code> True if FILE exists and has a size greater than zero <code>[ -w FILE ]</code> True if FILE exists and is writable <code>[ -x FILE ]</code> True if FILE exists and is executable <code>[ -z STRING ]</code> True of the length if \"STRING\" is zero <code>[ -n STRING]</code> True if the length of \"STRING\" is non-zero <code>[ STRING1 == STRING2 ]</code> True if the strings are equal. \"=\" may be used instead of \"==\" <code>[ STRING1 != STRING2 ]</code> True if the strings are not equal <code>[ ARG1 OP ARG2 ]</code> \"ARG1\" and \"ARG2\" are integers. Based on arithmetic binary operators return true <p><code>[ -a FILE ]</code></p> <p>Assume that you are working in your home directory, and you would like to check if a file exists in your home directory. If a file exists, then you would like to print a message \"FILE is exists\". Else, you need to print an error message. In such scenario, you can use the following code snippet <pre><code>if [ -a &lt;path of the file&gt; ]\nthen\n    echo \"FILE is exists\"\nelse\n    echo \"No such file\"\nfi\n</code></pre></p>"},{"location":"bash/primary-expression/#example","title":"Example","text":"<p>I have a file \"mkdocs.yml\" in the below location. Now, I will check if the file exists or not</p> <p></p> <p><pre><code>#!bin/bash\n\nif [ -a \"/Users/satheeshpandian.j/Documents/Sats/Learning/makedocs/mkdocs.yml\" ]\nthen\n    echo \"FILE is exists\"\nelse\n    echo \"No such file\"\nfi\n</code></pre> </p> <p><code>[ -r FILE ]</code></p> <p>Assume that you are working in your home directory, and you would like to check if a file is readable in your home directory. If a file exists, then you would like to print a message \"FILE is readable\". Else, you need to print an error message. In such scenario, you can use the following code snippet <pre><code>if [ -r &lt;path of the file&gt; ]\nthen\n    echo \"FILE is readable\"\nelse\n    echo \"FILE is not readable\"\nfi\n</code></pre></p>"},{"location":"bash/primary-expression/#example_1","title":"Example","text":"<p>I have a file \"mkdocs.yml\" in the below location. Now, I will check if the file is readable or not</p> <p></p> <p><pre><code>#!bin/bash\n\nif [ -r \"/Users/satheeshpandian.j/Documents/Sats/Learning/makedocs/mkdocs.yml\" ]\nthen\n    echo \"FILE is readable\"\nelse\n    echo \"FILE is not readable\"\nfi\n</code></pre> </p> <p><code>[ -z STRING ]</code></p> <p>Assume that you have string variable, and you would like to check string variable length. In such scenario, you can use the following code snippet <pre><code>if [ -z &lt;STRING&gt; ]\nthen\n    echo \"STRING length is zero\"\nelse\n    echo \"STRING length is not zero\"\nfi\n</code></pre></p>"},{"location":"bash/primary-expression/#example_2","title":"Example","text":"<p>In the below code snippet, variable name is empty. We are checking if the variable is empty (length). <pre><code>#!bin/bash\nname=\"\"\nif [ -z \"$name\" ]\nthen\n    echo \"STRING length is zero\"\nelse\n    echo \"STRING length is not zero\"\nfi\n</code></pre> </p> <p><code>[ STRING1 == STRING2 ]</code></p> <p>Assume that you have two string variables, and you would like to check if both string variables are same. In such scenario, you can use the following code snippet <pre><code>if [ STRING1 == STRING2 ]\nthen\n    echo \"STRINGS are same\"\nelse\n    echo \"STRINGS are not same\"\nfi\n</code></pre></p>"},{"location":"bash/primary-expression/#example_3","title":"Example","text":"<p>In the below code snippet, variables first_name and last_name are defined. We are checking if two names are same. <pre><code>#!bin/bash\nfirst_name=\"Satheesh\"\nlast_name=\"Pandian\"\nif [ $first_name == $last_name ]\nthen\n    echo \"STRINGS are same\"\nelse\n    echo \"STRINGS are not same\"\nfi\n</code></pre> </p>"},{"location":"bash/primary-expression/#note","title":"NOTE:","text":"<ol> <li>if [ $first_name = $last_name ] =&gt; Also works</li> <li>if [[ $first_name == $last_name ]] =&gt; Also works</li> </ol> <p><code>[ ARG1 OP ARG2 ]</code> </p> <p>In general, ARG1 and ARG2 are numbers. OP is nothing but an arithmatic operator.</p> Arithmatic Operator Meaning <code>eq</code> is equal to <code>ne</code> not equal to <code>lt</code> less than <code>le</code> less than or equal to <code>gt</code> greater than <code>ge</code> greater than or equal to <pre><code>if [ ARG1 -eq ARG2 ]\nthen\n    echo \"Both numbers are same\"\nelse\n    echo \"Both numbers are not same\"\nfi\n</code></pre>"},{"location":"bash/primary-expression/#example_4","title":"Example","text":"<p>In the below code snippet, two numbers are compared to see if both numbers are same or not. <pre><code>#!bin/bash\n\nif [ 10 -eq 7 ]\nthen\n    echo \"Both numbers are same\"\nelse\n    echo \"Both numbers are not same\"\nfi\n</code></pre> </p>"},{"location":"bash/primary-expression/#high-level-summary","title":"High Level Summary","text":"Expression Expression Result <code>[ xyz = xyz ]</code> <code>[[ xyz = xyz ]]</code> true (both strings are equal) <code>[ xyz != xyz ]</code> <code>[[ xyz != xyz ]]</code> false (both strings are equal) <code>[ 10 -lt 15 ]</code> <code>[[ 10 -lt 15 ]]</code> true (10 is less than 15) <code>[ 10 -gt 15 ]</code> <code>[[ 10 -gt 15 ]]</code> false (10 is not greater than 15) <code>[ 10 -ne 15 ]</code> <code>[[ 10 -ne 15 ]]</code> true (10 is not equal to 15)"},{"location":"bash/primary-expression/#note_1","title":"NOTE","text":"<ol> <li> <p>[[ condition ]] is preferred when there are patterns involved</p> <p>Example</p> </li> </ol> <pre><code>        #!bin/bash\n        name=\"Shelling\"\n        if [[ $name == *hell* ]]\n        then\n          echo \" name contains the pattern\"\n        fi\n</code></pre> Expression Meaning <code>[[ \"xyz\" = *y* ]]</code> true (string xyz contains y) <code>[[ \"xyz\" = xy[az] ]]</code> true (string xyz contains z after xy)This can be considered as<code>[[ xyz = xy[az] ]] or [[ xya = xy[az] ]]</code> <code>[[ \"xyz\" = \"xy[ab]\" ]]</code> false (string xyz does not contain z after xy) <code>[[ \"xyz\" &gt; \"abc\" ]]</code> true (x comes after  a when sorted alphabetically) <code>[[ \"xyz\" &lt; \"abc\" ]]</code> false (z does not come after  x when sorted alphabetically) <ol> <li><code>[[</code> prevents word splitting of variable values. So, if VAR=\"var with spaces\", you do not need to double quote $VAR in the condition. <pre><code>name=\"Satheesh Pandian\" # name has a value with space. So, quote matters in condition\nif [[ $name = \"Sats\" ]] # $name is not required to be surrounded by quotes as it is in double square brackets\nthen\n  echo \"$name is Sats\"\nelse\n  echo \"$name is NOT Sats\"\nfi\n\nif [ \"$name\" = \"Sats\" ] # $name is required to be surrounded by quotes as it is in single square bracket\nthen\n  echo \"$name is Sats\"\nelse\n  echo \"$name is NOT Sats\"\nfi\n</code></pre></li> </ol>"},{"location":"bash/primary-expression/#combining-expressions-with-if-condition","title":"Combining expressions with If condition","text":"Expression Meaning [ condition1 ] || [ condition2 ]  or [[ condition1 || condition2 ]] or  [ condition1 -o condition2 ] True if any one condition is successNOTE:[[ condition1 -o condition2 ]] will not work. Double square brackets with flag (-o) for multiple conditions will not work [ condition1 ] &amp;&amp; [ condition2 ]  or [[ condition1 &amp;&amp; condition2 ]] or  [ condition1 -a condition2 ] True if both conditions are successNOTE:[[ condition1 -a condition2 ]] will not work. Double square brackets with flag (-a) for multiple conditions will not work <p>Example</p> Expression Meaning [ 5 -gt 3 ] &amp;&amp; [ 5 -lt 10 ] true (Both conditions are true) [ 5 -gt 13 ] || [ 5 -lt 10 ] true (Second condition is true) [ 5 -gt 3 -a 5 -lt 10 ] true (Both conditions are true) [[ 5 -gt 3 &amp;&amp; 5 -lt 10 ]] true (Both conditions are true) [[ 5 -gt 13 || 5 -lt 10 ]] true (Second condition is true) [ 5 -gt 13 -o 5 -lt 1 ] false (None of conditions are true) <p>Note</p> <p>Most programmers will prefer to use the test built-in command, which is equivalent to using square brackets for comparison</p> <p>Example <pre><code>name=\"satheesh\"\ntest \"$(name)\" != 'satheesh' &amp;&amp; (echo Hey $name; exit 1)\n</code></pre></p>"},{"location":"bash/sed/","title":"Introduction to sed","text":"<p><code>sed</code> is a stream editor. It is used to perform basic text transformation.</p> <p>Syntax <pre><code>sed SCRIPT INPUTFILE\n</code></pre></p>"},{"location":"bash/sed/#substitution","title":"Substitution","text":"<p>Replace first occurrence in each the lines with new value <pre><code>sed 's/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace first occurrences of \u2018pandian\u2019 to \u2018satheesh\u2019 in each lines in the file input.txt</p> <p>Output </p> <p><code>pandian</code> is replaced by <code>satheesh</code> in each line ONLY in the first occurrence. If you see the input snapshot, third line has <code>pandian</code> in two places. The <code>sed</code> command just replaces in the first occurrence. In the second occurrence, it is still showing <code>pandian</code>.</p> <p>Replace n<sup>th</sup> occurrence in each line with new value <pre><code>sed 's/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/&lt;OCCURRENCE_NUMNBER&gt;' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace specific occurrence of \u2018pandian\u2019 to \u2018satheesh\u2019 in each lines in the file input.txt</p> <p>Output </p> <p><code>pandian</code> is replaced by <code>satheesh</code> in each line ONLY in the second occurrence. If you see the input snapshot, third line has <code>pandian</code> in two places. The <code>sed</code> command just replaces in the second occurrence. In the fist occurrence, it is still showing <code>pandian</code> and also other lines still show <code>pandian</code> in the first occurrence.</p> <p>Replace all occurrences in entire file with new value <pre><code>sed 's/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/g' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace all occurrences of \u2018pandian\u2019 to \u2018satheesh\u2019 in the file input.txt</p> <p>Output </p> <p><code>pandian</code> is replaced by <code>satheesh</code> in every single place in the file.</p> <p>Replace from n<sup>th</sup> occurrences in each line with new value <pre><code>sed 's/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/&lt;OCCURRENCE_NUMNBER&gt;g' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace 2nd occurrences onwards of \u2018pandian\u2019 to \u2018satheesh\u2019 in each of the file input.txt</p> <p>Output </p> <p><code>pandian</code> is replaced by <code>satheesh</code> from 2nd occurrence in each line. If you look at first two lines, there is only ONE <code>pandian</code> value. Hence, these lines are not impacted. However, if you look at third and fourth lines, <code>pandian</code> occurrences more than once. Hence, from second occurrence onwards, <code>pandian</code> is replaced by <code>satheesh</code>.</p> <p>Replace the first occurrences in specific line with new value <pre><code>sed '&lt;OCCURRENCE_NUMNBER&gt; s/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace first occurrences of \u2018pandian\u2019 to \u2018satheesh\u2019 in third line of the file input.txt</p> <p>Output </p> <p>First occurrence of <code>pandian</code> is replaced by <code>satheesh</code> in the third line and NO OTHER CHANGE in the file.</p> <p>Replace from n<sup>th</sup> occurrences onwards in specific line with new value <pre><code>sed '&lt;OCCURRENCE_NUMNBER&gt; s/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/g' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace second occurrences onwards of \u2018pandian\u2019 to \u2018satheesh\u2019 in fourth line of the file input.txt</p> <p>Output </p> <p>Second occurrences onwards of <code>pandian</code> is replaced by <code>satheesh</code> in the fourth line and NO OTHER CHANGE in the file.</p> <p>Replace all the occurrences in specific line with new value <pre><code>sed '&lt;OCCURRENCE_NUMNBER&gt; s/&lt;OLD_VALUE&gt;/&lt;NEW_VALUE&gt;/g' &lt;INPUT_FILE&gt;\n</code></pre> Example </p> <p>To replace all the occurrences of \u2018pandian\u2019 to \u2018satheesh\u2019 in third line of the file input.txt</p> <p>Output  All the occurrence of <code>pandian</code> is replaced by <code>satheesh</code> in the third line and NO OTHER CHANGE in the file.</p>"},{"location":"bash/special-parameters/","title":"Special Parameters ($@ and $*)","text":"<ul> <li><code>$@</code> is used to manipulate all the arguments passed to the shell while running as each item considered as SEPARATE item.</li> <li><code>$*</code> is used to manipulate all the arguments passed to the shell while running as each item considered as ONE item.</li> </ul>"},{"location":"bash/special-parameters/#without-quotes","title":"$@ without quotes","text":"<p>This will loop through all the items individually like below <pre><code>#!/bin/bash\nfor item in $@\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p>"},{"location":"bash/special-parameters/#with-quotes","title":"$@ with quotes","text":"<p>This will loop through all the items individually like below <pre><code>#!/bin/bash\nfor item in \"$@\"\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p> <p><code>$@ behaves same with quotes or without quotes. Remember there is NO custom IFS</code></p>"},{"location":"bash/special-parameters/#without-quotes-along-with-ifscustom-separator","title":"$@ without quotes along with IFS(Custom Separator)","text":"<p>This will loop through all the items individually like below <pre><code>#!/bin/bash\nIFS=\",\"\nfor item in $@\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p>"},{"location":"bash/special-parameters/#with-quotes-along-with-ifs","title":"$@ with quotes along with IFS","text":"<p>This will loop through all the items at once like below <pre><code>#!/bin/bash\nIFS=\",\"\nfor item in \"$@\"\ndo\n  echo \"${item}\"\ndone\n</code></pre> <code>$@ behaves differently with quotes or without quotes when custom IFS involves.</code></p> <p><code>- With quotes + custom IFS, it considered as ONE word</code></p> <p><code>- Without quotes  + custom IFS, it is considered as SEPARATE word</code></p>"},{"location":"bash/special-parameters/#conclusion-for","title":"CONCLUSION for $@","text":""},{"location":"bash/special-parameters/#without-quotes_1","title":"<code>Without quotes</code>","text":"<p>- $@ will split the item INDIVIDUALLY, Doesn't matter if custom IFS is there or not</p>"},{"location":"bash/special-parameters/#with-quotes_1","title":"<code>With quotes</code>","text":"<p>- $@ will split the item INDIVIDUALLY when if custom IFS is not involved</p> <p>- $@ will NOT split the item INDIVIDUALLY when if custom IFS is involved</p>"},{"location":"bash/special-parameters/#without-quotes_2","title":"$* without quotes","text":"<p>This will loop through all the items individually like below <pre><code>#!/bin/bash\nfor item in $*\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p>"},{"location":"bash/special-parameters/#with-quotes_2","title":"$* with quotes","text":"<p>This will loop through all the items individually like below <pre><code>#!/bin/bash\nfor item in \"$*\"\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p>"},{"location":"bash/special-parameters/#behaves-differently-with-quotes-or-without-quotes","title":"$* behaves differently with quotes or without quotes.","text":"<ul> <li>With quotes, it considered as ONE word</li> <li>Without quotes, it is considered as SEPARATE word</li> </ul>"},{"location":"bash/special-parameters/#without-quotes-along-with-ifscustom-separator_1","title":"$* without quotes along with IFS(Custom Separator)","text":"<p><pre><code>#!/bin/bash\nIFS=\",\"\nfor item in $*\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p>"},{"location":"bash/special-parameters/#with-quotes-along-with-ifs_1","title":"$* with quotes along with IFS","text":"<p>This will loop through all the items at once like below <pre><code>#!/bin/bash\nIFS=\",\"\nfor item in \"$*\"\ndo\n  echo \"${item}\"\ndone\n</code></pre> </p> <p><code>$* behaves differently with quotes or without quotes when custom IFS involves.</code></p> <p><code>- With quotes + custom IFS, it considered as ONE word</code></p> <p><code>- Without quotes  + custom IFS, it is considered as SEPARATE word</code></p>"},{"location":"bash/special-parameters/#conclusion-for_1","title":"CONCLUSION for $*","text":""},{"location":"bash/special-parameters/#without-quotes_3","title":"<code>Without quotes</code>","text":"<p>- $* will split the item INDIVIDUALLY, Doesn't matter if custom IFS is there or not</p>"},{"location":"bash/special-parameters/#with-quotes_3","title":"<code>With quotes</code>","text":"<p>- $* will NOT split the item, Doesn't matter if custom IFS is there or not</p>"},{"location":"bash/until-loop/","title":"The until loop","text":"<p>In general, until loop can be used similarly like while loop.</p>"},{"location":"bash/until-loop/#how-does-it-work","title":"How does it work?","text":"<p>The until allows for repetitive execution of a list of commands, as long as the condition defined in the until loop is met.</p>"},{"location":"bash/until-loop/#syntax","title":"Syntax","text":"<p><pre><code>until &lt;condition&gt; \ndo \n  COMMANDS/Statements\ndone\n</code></pre> Whereas, condition can be any command(s) that can exit with a success or failure status. The COMMANDS/Statements can be any program or script.</p> <p>As soon as the condition met, the loop exits. In a script, the command following the done statement is executed.</p> <p>The return status is the exit status of the last COMMANDS/Statement, or zero if none was executed.</p>"},{"location":"bash/until-loop/#example","title":"Example","text":"<p><pre><code>#!/bin/bash\n## This script is to just print the numbers until it reaches 5.\nnumber=1\nuntil [[ \"${number}\" -eq 5 ]]\ndo\n    echo \"${number} is not equal to 5\"\n    number=$((number+1))\ndone\n</code></pre> </p>"},{"location":"bash/while-loop/","title":"The while loop","text":"<p>In general, while loop can be used in the below scenarios.</p> <ul> <li>Executes a command or set of commands many times, but you are not sure how many times</li> <li>Executes a command or set of commands until specific condition occurs </li> <li>Create an infinite loop</li> <li>Menu driven programs </li> </ul>"},{"location":"bash/while-loop/#how-does-it-work","title":"How does it work?","text":"<p>The while allows for repetitive execution of a list of commands, as long as the condition defined in the while loop executes successfully (exit status of zero).</p>"},{"location":"bash/while-loop/#syntax","title":"Syntax","text":"<p><pre><code>while &lt;condition&gt; \ndo \n  COMMANDS/Statements\ndone\n</code></pre> Whereas, condition can be any command(s) that can exit with a success or failure status. The COMMANDS/Statements can be any program or script.</p> <p>As soon as the condition fails, the loop exits. In a script, the command following the done statement is executed.</p> <p>The return status is the exit status of the last COMMANDS/Statement, or zero if none was executed.</p>"},{"location":"bash/while-loop/#example","title":"Example","text":"<p><pre><code>#!/bin/bash\n# This script is to just print the given number if it is less than 4.\ni=\"0\" # we can define i=0 as well. Both works\nwhile [ $i -lt 4 ]\ndo\n  i=$[$i+1]\n  echo $i\ndone\n</code></pre> </p>"},{"location":"bash/while-loop/#menu-driven-program-example","title":"Menu Driven Program Example","text":"<p><pre><code>#!/bin/bash\n## This script is to show how the menu driven program works in while loop\n\nwhile true\ndo\n  echo \"1. Print the content\"\n  echo \"2. Count the line in the file\"\n  echo \"3. Append the content\"\n  echo \"4. Remove the file\"\n  echo \"5. Exit\"\n\n  read -p \"Enter your choice: \" choice\n\n  if [[ \"${choice}\" -eq 1 ]]\n  then\n    cat names.txt\n  elif [[ \"${choice}\" -eq 2 ]]\n  then\n    cat names.txt | wc -l\n  elif [[ \"${choice}\" -eq 3 ]]\n  then\n    echo \"${choice}\" &gt;&gt; names.txt\n  elif [[ \"${choice}\" -eq 4 ]]\n  then\n    rm -rf names.txt\n  else\n    exit 0\n  fi\ndone\n</code></pre> </p>"},{"location":"css/box-model/","title":"Box Model","text":"<p>Most of the website HTML elements are layed out as a box. Box surrounds each HTML element.</p>"},{"location":"css/box-model/#height-html-element","title":"Height HTML Element","text":"<pre><code>  &lt;body&gt;\n    &lt;p&gt;Bangalore&lt;/p&gt;\n    &lt;br /&gt;\n    &lt;p id=\"sats\"&gt;Satheesh&lt;/p&gt;\n    &lt;br /&gt;\n    &lt;p&gt;Pandian&lt;/p&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#sats{\n    height: 100px\n}\n</code></pre>"},{"location":"css/box-model/#width-html-element","title":"Width HTML Element","text":"<pre><code>  &lt;body&gt;\n    &lt;img src=\"./css_2.png\" alt=\"image\" height=\"200px\" id=\"app-image\"/&gt;\n    &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#app-image{\n    width: 200px\n}\n</code></pre> <pre><code>#app-image{\n    width: 50px\n}\n</code></pre> <p>Remember</p> <ul> <li>Height and width properties can be mentioned as pixel values (px) or percentage (%).</li> </ul> <p>Ex:  <pre><code>#app-image{\n    width: 50px;\n    height: 200px;\n}\n</code></pre> <pre><code>#app-image{\n    width: 60%;\n    height: 70%;\n}\n</code></pre></p>"},{"location":"css/box-model/#border-html-element","title":"Border HTML Element","text":"<p><pre><code>  &lt;body&gt;\n    &lt;p&gt;Bangalore&lt;/p&gt;\n    &lt;p id=\"sats\"&gt;Satheesh&lt;/p&gt; \n    &lt;p&gt;Pandian&lt;/p&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#sats{\n    border: 10px solid black\n}\n</code></pre></p> <p></p> <p><code>If border property value is increased, it will extend outwwards only. It will not affect height and width of the element inside in it.</code></p> <p></p> <p></p> <p>We can change or remove any border line thickness </p> <pre><code>  &lt;body&gt;\n   &lt;div id=\"sats\"&gt;&lt;/div&gt;\n  &lt;/body&gt;\n</code></pre> <p><pre><code>#sats{\n  border: 50px solid black;\n  border-bottom: 0px;\n  height: 100px;\n  width: 100px;\n  background-color: red;\n}\n</code></pre> </p> <p>Border width property is working clockwise.</p> <pre><code>```html\n  &lt;body&gt;\n   &lt;div id=\"sats\"&gt;&lt;/div&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#sats{\n  border: 50px solid black;\n  border-width: 10px 20px 30px 40px;\n  height: 100px;\n  width: 100px;\n  background-color: red;\n}\n</code></pre> <p></p> <p>Border width property may have two values alone.</p> <pre><code>```html\n  &lt;body&gt;\n   &lt;div id=\"sats\"&gt;&lt;/div&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#sats{\n  border: 50px solid black;\n  border-width: 30px 60px;\n  height: 100px;\n  width: 100px;\n  background-color: red;\n}\n</code></pre> <p></p>"},{"location":"css/box-model/#padding-html-element","title":"Padding HTML Element","text":"<p>Padding element is the space between border and HTML element inside in it. It pushes the border with padding property value. However, it does not impact the height and width of an element inside in it.</p> <p><pre><code>  &lt;body&gt;\n   &lt;div id=\"sats\"&gt;\n       &lt;p&gt;Satheesh Pandian Jeganathan&lt;/p&gt;\n   &lt;/div&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#sats{\n  border: 5px solid black;\n  height: 100px;\n  width: 100px;\n  background-color: red;\n  padding: 10px;\n}\n</code></pre></p> <p>In this example, padding pushes the border to 10px outwards.</p> <p></p> <p>In this example, padding pushes the border to 100px outwards.</p> <p></p>"},{"location":"css/box-model/#margin-html-element","title":"Margin HTML Element","text":"<p>Margin is the space between the body and the border of the element. </p> <p><pre><code>  &lt;body&gt;\n   &lt;div id=\"sats\"&gt;\n    &lt;p id=\"app\"&gt;Satheesh Pandian Jeganathan&lt;/p&gt;\n   &lt;/div&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>#sats{\n  margin: 80px;\n  border: 30px solid black;\n  height: 100px;\n  width: 100px;\n  background-color: red;\n  padding: 100px;\n}\n\n#app{\n  border: 5px solid blue;\n}\n</code></pre></p> <p></p> <p>Remember</p> <p>All three properties (border, padding, and margin can have 4 values or 2 values. It will be clockwise, and it starts from the top.)</p> <p>4 values <pre><code>border: 10px 20px 30px 40px\nmargin: 10px 20px 30px 40px\npadding: 10px 20px 30px 40px\n</code></pre></p> <p>2 values</p> <p>Top and bottom are grouped and right/left are grouped <pre><code>border: 10px 20px\nmargin: 10px 20px\npadding: 10px 20px\n</code></pre></p>"},{"location":"css/box-model/#content-division-element","title":"Content Division Element","text":"<p>We can group the boxes using the div element. This element is invisible in the website.</p> <pre><code>  &lt;body&gt;\n   &lt;div id=\"art\"&gt;\n    &lt;p class=\"app\"&gt;Satheesh Pandian Jeganathan&lt;/p&gt;\n   &lt;/div&gt;\n   &lt;div class=\"sats\"&gt;\n    &lt;p class=\"app\"&gt;SRE&lt;/p&gt;\n   &lt;/div&gt;\n  &lt;/body&gt;\n</code></pre> <pre><code>.sats{\n  border: 30px solid black;\n  height: 100px;\n  width: 100px;\n  background-color: red;\n  padding: 10px;\n}\n\n#art{\n  border: 30px solid orange;\n  background-color: blue;\n  height: 100px;\n  width: 100px;\n  padding: 10px;\n}\n</code></pre> <p></p>"},{"location":"css/cascade-priority/","title":"Order of Style","text":"<p>Let's say, if there are two CSS implementations mentioned for one HTML document,  then which CSS rule will be applied to that HTML element.  The order of style implementation matters when you work with CSS. There are four other important rules to implement CSS rules in the HTML document.</p> <p>1. Position</p> <p>The position where the rule defines matter. The latest CSS rule will be applied in the HTML document.</p> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n    &lt;style&gt;\n        li {\n            color: blue;\n            color:yellowgreen\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;ul&gt;\n        &lt;li style=\"color:red\"&gt;Satheesh&lt;/li&gt;\n        &lt;li&gt;Pandian&lt;/li&gt;\n        &lt;li&gt;Jeganathan&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>In the above case, text color is two different colors (blue and yellow-green). When we implement this rule, yellow-green will be implemented as it is the latest rule (from TOP to BOTTOM approach)</p> <p></p> <p>Let us assume it, if there are two rules created for same <code>li</code> component like below</p> <p><pre><code>    &lt;style&gt;\n        li {\n            color: blue;\n            color: yellowgreen\n        }\n\n        li {\n            color: brown;\n            color: orangered\n        }\n    &lt;/style&gt;\n</code></pre> In the above case, orangered is the latest color (from TOP to BOTTOM), and it will be implemented in <code>li</code> HTML element.</p> <p></p> <p>Summary:</p> <p></p> <p>2. Specificity </p> <p>Based on how specific a selector is applied CSS rule.</p> <p>Priority order</p> <ol> <li>ID</li> <li>Attribute</li> <li>Class</li> <li>HTML element</li> </ol> <p>Example</p> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;ul&gt;\n        &lt;li id=\"satheesh\" draggable=\"true\" class=\"satheesh-class\"&gt;Satheesh&lt;/li&gt;\n        &lt;li&gt;Pandian&lt;/li&gt;\n        &lt;li&gt;Jeganathan&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p><pre><code>li {\n  color: green;\n}\nli[draggable] {\n  color: yellow;\n}\n#satheesh {\n  color: red;\n}\n.satheesh-class {\n  color: blue;\n}\n</code></pre> In the above example, there are four rules applied in the list # 1 component.  Out of these four rules, ID takes high priority.  Hence, it is displayed in red color.</p> <p></p> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;ul&gt;\n        &lt;li draggable=\"true\" class=\"satheesh-class\"&gt;Satheesh&lt;/li&gt;\n        &lt;li&gt;Pandian&lt;/li&gt;\n        &lt;li&gt;Jeganathan&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>In the above example, there are three rules applied in the list # 1 component.  Out of these three rules, Attribute (draggable) takes high priority.  Hence, it is displayed in yellow color.</p> <p></p> <p><pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;ul&gt;\n        &lt;li class=\"satheesh-class\"&gt;Satheesh&lt;/li&gt;\n        &lt;li&gt;Pandian&lt;/li&gt;\n        &lt;li&gt;Jeganathan&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> In the above example, there are two rules applied in the list # 1 component.  Out of these two rules, class takes high priority.  Hence, it is displayed in blue color.</p> <p></p> <p>Summary</p> <p></p> <p></p> <p></p> <p>3. Type of the style</p> <ul> <li>Inline style takes the highest priority</li> <li>Internal style takes next place</li> <li>External style takes the least priority out of these three.</li> </ul> <p>Example</p> <pre><code>&lt;html&gt;\n\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n    &lt;style&gt;\n        li {\n            color: blue\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;ul&gt;\n        &lt;li style=\"color:red\"&gt;Satheesh&lt;/li&gt;\n        &lt;li&gt;Pandian&lt;/li&gt;\n        &lt;li&gt;Jeganathan&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> <p><pre><code>li {\n  color: green;\n}\n</code></pre> </p> <p>Explanation</p> <p>Though external CSS style rule for <code>li</code> component color is green, internal CSS style rule is applied for list # 2 and 3.  This is because internal style takes more priority than external style. At the same time, list # 1 color is red because it takes inline style rule which is higher priority than internal and external style.</p> <p>Summary</p> <p></p> <p></p> <p></p> <p>4. Importance</p> <p>When <code>important</code> keyword mentioned in the property, this takes the highest priority compared to all other cases.</p> <pre><code>li[draggable] {\n  color: yellow;\n}\n#satheesh {\n  color: red;\n}\n.satheesh-class {\n  color: blue;\n}\nli {\n  color: brown;\n  color:#17fa07a3 !important;\n}\n</code></pre> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;ul&gt;\n        &lt;li id=\"satheesh\" class=\"satheesh-class\" draggable=\"true\"&gt;Satheesh&lt;/li&gt;\n        &lt;li&gt;Pandian&lt;/li&gt;\n        &lt;li&gt;Jeganathan&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>In the above example, all the rules applied in the list are #1 component. Out of all these rules, <code>important</code> takes high priority.  Hence, it is displayed in green color.  Note, as <code>important</code> keyword applied in the list component, it applies all the list values.</p> <p></p> <p>Remember</p> <p>If all the rules are applied in the HTML element, the priority order is</p> <ol> <li>Importance</li> <li>Type</li> <li>Specificity</li> <li>Position</li> </ol>"},{"location":"css/cascade-priority/#combining-css-selectors","title":"Combining CSS selectors","text":"<p>We can select multiple selectors and apply the rule in a single HTML element or group of elements.</p> <p>1. Group Selector </p> <p>Apply both the selectors</p> <p>Syntax <pre><code> selector1, selector2 {\n    color: firebrick;\n}\n</code></pre></p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;Combining CSS Selectors&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;To Do List&lt;/h1&gt;\n  &lt;h2&gt;Monday&lt;/h2&gt;\n  &lt;div class=\"box\"&gt;\n    &lt;p class=\"done\"&gt;Do these things today!&lt;/p&gt;\n    &lt;ul class=\"list\"&gt;\n      &lt;li&gt;Wash Clothes&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Read&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Maths Questions&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n  &lt;ul&gt;\n    &lt;p class=\"done\"&gt;Other items&lt;/p&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;The best preparation for tomorrow is doing your best today.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>If I want to apply different colors ONLY to h1 and h2. Note, you can select <code>n</code> number of selectors and group them.</p> <pre><code>h1,h2 {\n    color: blueviolet;\n}\n</code></pre> <p></p> <p>Summary</p> <p></p> <p>2. Child </p> <p>This will apply the CSS rule to the direct child element (only one level nested inside means first generation only)</p> <p>Syntax <pre><code> Parent &gt; Child {\n    color: firebrick;\n}\n</code></pre></p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;Combining CSS Selectors&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;To Do List&lt;/h1&gt;\n  &lt;h2&gt;Monday&lt;/h2&gt;\n  &lt;div class=\"box\"&gt;\n    &lt;p class=\"done\"&gt;Do these things today!&lt;/p&gt;\n    &lt;ul class=\"list\"&gt;\n      &lt;li&gt;Wash Clothes&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Read&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Maths Questions&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n  &lt;ul&gt;\n    &lt;p class=\"done\"&gt;Other items&lt;/p&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;The best preparation for tomorrow is doing your best today.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> If I want to apply different color ONLY to <code>p</code> which is under <code>div</code>. </p> <p>Example <pre><code>.box &gt; .done {\n  color: firebrick;\n}\n</code></pre></p> <p></p> <p>Summary</p> <p></p> <p>3. Descendant: </p> <p>This will apply the CSS rule to all child elements under parent selector (As long as a child element  comes under parent in any nested level inside)</p> <p>Syntax <pre><code> Parent Child {\n    color: blue;\n}\n</code></pre></p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;Combining CSS Selectors&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;To Do List&lt;/h1&gt;\n  &lt;h2&gt;Monday&lt;/h2&gt;\n  &lt;div class=\"box\"&gt;\n    &lt;p class=\"done\"&gt;Do these things today!&lt;/p&gt;\n    &lt;ul class=\"list\"&gt;\n      &lt;li&gt;Wash Clothes&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Read&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Maths Questions&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n  &lt;ul&gt;\n    &lt;p class=\"done\"&gt;Other items&lt;/p&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;The best preparation for tomorrow is doing your best today.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> If I want to apply different color ONLY to <code>li</code> under <code>div</code>.</p> <p>Example <pre><code>.box li{\n    color: blue;\n}\n</code></pre> </p> <p>Summary:</p> <p></p> <p>4. Chaining </p> <p>This will apply the CSS rule to the all selectors are TRUE.  We can pick a specific element using this selector. If the content has class, id and element, then the selector rule always starts with an element.</p> <p>Example:</p> <p><pre><code>&lt;h1 id=\"name\" class=\"sats pan\"&gt;Satheesh&lt;/h1&gt;\n&lt;h1 class=\"sats pan\"&gt;Pandian&lt;/h1&gt;\n</code></pre> Then if we want to select <code>Satheesh</code>, then the CSS rule is</p> <pre><code>h1#name.sats.pan{\n  color: red\n}\n</code></pre> <p>As you see the above CSS code snippets, it starts with h1 element instead of id or class name.</p> <p>Syntax <pre><code> selector1selector2 {\n    color: seagreen;\n}\n</code></pre></p> <p><pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;Combining CSS Selectors&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;To Do List&lt;/h1&gt;\n  &lt;h2&gt;Monday&lt;/h2&gt;\n  &lt;div class=\"box\"&gt;\n    &lt;p class=\"done\"&gt;Do these things today!&lt;/p&gt;\n    &lt;ul class=\"list\"&gt;\n      &lt;li&gt;Wash Clothes&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Read&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Maths Questions&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n  &lt;ul&gt;\n    &lt;p class=\"done\"&gt;Other items&lt;/p&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;The best preparation for tomorrow is doing your best today.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> If I want to apply different color ONLY to <code>li</code> (last two items) under <code>div</code>. </p> <p>Example <pre><code>.box li{\n    color: seagreen;\n}\n</code></pre> </p> <p>Summary </p> <p></p> <p>5. Combine combiners </p> <p>This is similar to descendant.  However, the first one is ancestor(parent) and the second one is chaining selector</p> <p>Syntax</p> <pre><code> selector1 selector2selector3 {\n    font-size: 0.5rem;\n}\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;Combining CSS Selectors&lt;/title&gt;\n  &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;To Do List&lt;/h1&gt;\n  &lt;h2&gt;Monday&lt;/h2&gt;\n  &lt;div class=\"box\"&gt;\n    &lt;p class=\"done\"&gt;Do these things today!&lt;/p&gt;\n    &lt;ul class=\"list\"&gt;\n      &lt;li&gt;Wash Clothes&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Read&lt;/li&gt;\n      &lt;li class=\"done\"&gt;Maths Questions&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n  &lt;ul&gt;\n    &lt;p class=\"done\"&gt;Other items&lt;/p&gt;\n  &lt;/ul&gt;\n  &lt;p&gt;The best preparation for tomorrow is doing your best today.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>If I want to apply different font size for <code>p</code> and class is <code>done</code></p> <p>Example <pre><code>ul p.done{\n    font-size: 0.5rem;\n}\n</code></pre> </p> <p>Summary:</p> <p></p>"},{"location":"css/color/","title":"CSS Colors","text":"<p>In CSS, Colors are specified using predefined color names, HEX, or RGB.</p>"},{"location":"css/color/#color-names","title":"Color Names","text":"<p>Color can be specified by using a predefined color name. CSS supports 14o color names. <pre><code>&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1 style=\"background-color:blue;\"&gt;This is blue&lt;/h1&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>  There are few commonly used color properties in CSS. 1. <code>background-color</code> This is to set the background color of the HTML element    You can refer the above example for how to set background for h1 element. 2. <code>color</code> This is to set the text color of the HTML element <pre><code>&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1 style=\"color:darkgreen;\"&gt;This is dark green&lt;/h1&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </p> <ol> <li><code>border</code> This is use to set the color for the border <pre><code>&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1 style=\"border:4px solid brown;\"&gt;This is border, solid line and 4px width and in brown color&lt;/h1&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </li> </ol>"},{"location":"css/color/#color-values","title":"Color Values","text":"<p>Colors can also be specified using RGB values, HEX values</p>"},{"location":"css/color/#rgb","title":"RGB","text":"<p><pre><code>&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1 style=\"color:rgb(255, 99, 71);\"&gt;This is RGB color value&lt;/h1&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </p>"},{"location":"css/color/#hex","title":"HEX","text":"<p><pre><code>&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1 style=\"color:#3cb371;\"&gt;This is HEX color value&lt;/h1&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </p>"},{"location":"css/color/#font-properties","title":"Font Properties","text":""},{"location":"css/color/#font-size","title":"Font size","text":"<p>Font size is used to define the size of the text.</p> <ul> <li>1px (pixel) = 1/96 th of inch</li> <li>1pt (point) = 1/72nd of inch (12pt is equal to MS Word size is 12. In MS Word, pt is mentioned)</li> <li>1em (M) = 100% of the parent size. In below code, 2em means (2 x medium) as <code>body</code> is parent of <code>p</code></li> <li>1rem = 100% of the root size (html). In below code, 2rem means (2 x 20px) as <code>html</code> is root of <code>p</code></li> <li>Named font size is also application such as xx-large, small, medium, and large. <pre><code>#pixel {\n  font-size: 18px;\n}\n#point {\n  font-size: 15pt;\n}\n#em {\n  font-size: 2em;\n}\n#rem {\n  font-size: 2rem;\n}\nbody {\n  font-size: medium;\n}\nhtml {\n  font-size: 20px;\n}\n</code></pre> <pre><code>&lt;html&gt;\n   &lt;head&gt;\n       &lt;link rel=\"stylesheet\" href=\"./style.css\"&gt;\n   &lt;/head&gt;\n   &lt;body&gt;\n       &lt;p id=\"pixel\"&gt;This is pixel&lt;/p&gt;\n       &lt;p id=\"point\"&gt;This is point&lt;/p&gt;\n       &lt;p id=\"em\"&gt;This is em&lt;/p&gt;\n       &lt;p id=\"rem\"&gt;This is rem&lt;/p&gt;\n   &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> </li> </ul>"},{"location":"css/color/#font-weight","title":"Font weight","text":"<p>Font weight is like bold, normal, lighter (-100), bolder(+100), 100-900 <pre><code>&lt;body&gt;\n    &lt;h1&gt;This is h1 with normal font weight&lt;/h1&gt;\n    &lt;p&gt;This is p with bold font weight&lt;/p&gt;\n    &lt;h3&gt;This is h3 with lighter font weight&lt;/h3&gt;\n    &lt;p draggable=\"true\"&gt;This is p draggable with bolder font weight&lt;/p&gt;\n    &lt;h5&gt;This is h5 with number(200) range between 100-900 font weight&lt;/h5&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>p[draggable] {\n  font-weight: bolder;\n}\np {\n  font-weight: bold;\n}\nh1 {\n  font-weight: normal;\n}\nh3 {\n  font-weight: lighter;\n}\nh5 {\n  font-weight: 200;\n}\n</code></pre> </p>"},{"location":"css/color/#font-family","title":"Font family","text":"<p>This determines how your text look like. There are two values passed to <code>font-family</code>. First value is the primary font. In case, if the first font is not available in the system, second font will be picked automatically and second font type is generic type. <pre><code>h1{\n   font-family: \"Verdana\",\"sans-serif\"\n}\n</code></pre> <pre><code>&lt;body&gt;\n    &lt;h1&gt;\n       This is h1 with font family of Verdana\n    &lt;/h1&gt;\n&lt;/body&gt;\n</code></pre> </p>"},{"location":"css/color/#text-align","title":"Text align","text":"<p>This determines where your text should be placed.  <pre><code>h1{\n   text-align: \"center\"\n}\n</code></pre> <pre><code>&lt;body&gt;\n    &lt;h1&gt;\n       This is h1 aligning at center\n    &lt;/h1&gt;\n&lt;/body&gt;\n</code></pre> </p>"},{"location":"css/css-display/","title":"CSS Display","text":"<p>By default, all the element display property is block scope. It means that it occupies the entire row space.</p> <pre><code>&lt;p&gt;Satheesh&lt;/p&gt;&lt;p&gt;Pandian&lt;/p&gt;\n</code></pre> <p></p> <p><code>Span</code> element is different and by default the display scope of a span element is inline. </p> <p><pre><code>&lt;p&gt;Satheesh Pandian is a &lt;span&gt;SRE professional from&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Bangalore&lt;/p&gt;\n</code></pre> </p> <p>There are three commonly used display properties even though there are many display properties.</p> <ol> <li>block</li> <li>inline</li> <li>inline-block</li> </ol> <p>block</p> <p>By default, almost all elements have block display scope. It means it occupies the entire row space. You cannot increase the width of property</p> <p>inline </p> <p>The elements go in the same line next to each other. It cannot increase width and height properties. You cannot set the size for the element. It just fits the element size.</p> <p></p> <p>inline-block </p> <p>The elements go in the same line next to each other, and at the same time it can increase height and width properties as well.</p> <p></p> <p><code>If display property sets as none, then the element will be disappeared from the website.</code></p>"},{"location":"css/css-display/#css-float","title":"CSS Float","text":"<p>When you have an image element and another element, another element will go down or up based on the place where it is defined. Another element never wraps around the image element. If we wanted to wrap around the image, we need to use <code>float</code> property to the image element.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;title&gt;Document&lt;/title&gt;\n  &lt;link href=\"./extra.css\" rel=\"stylesheet\" &gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;img src=\"./baby.png\" alt=\"baby\" height=\"100\"&gt;\n  &lt;p&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. \n    Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n    when an unknown printer took a galley of type and scrambled it to make a type specimen book. \n    It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. \n    It was popularised in the 1960s with the release of Letraset sheets containing \n    Lorem Ipsum passages, and more recently with desktop publishing software like Aldus \n    PageMaker including versions of Lorem Ipsum.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p></p> <pre><code>img{\n    float: left;\n}\n</code></pre> <p></p> <p>Let us assume that we have a footer element paced around image.</p> <p></p> <p>However, a footer element should be placed across the page. In order to achieve this, we need to use <code>clear</code> property to the footer element.</p> <pre><code>footer{\nclear: left;  /* Based on the image alignment, the value will change (left/right/both) /*\n}\n</code></pre> <p></p> <p><code>Remember that we can apply float property to any element to place them in the website</code></p>"},{"location":"css/css-display/#create-responsive-websites","title":"Create Responsive Websites","text":"<p>Below are the main ways to create a responsive websites.</p> <ol> <li>Media Queries</li> <li>CSS Grid</li> <li>CSS Flexbox</li> <li>Bootstrap</li> </ol>"},{"location":"css/css-display/#media-queries","title":"Media Queries","text":"<p>This is working based on the screensize. If we set the screensize and if the screensize goes below to the size, then we need to provide another CSS configuration to refer for that screensize. We can use min width as well.</p> <pre><code> @media (max-width: 600px) {\n\n      /* CSS for screens below or equal to 600px wide */\n      div {\n        height: 200px;\n        width: 200px;\n      }\n    }\n</code></pre> <p></p> <p></p>"},{"location":"css/css-display/#css-grid","title":"CSS Grid","text":"<p>CSS Grid will divide the screen in 2D format (Horizontally and Vertically). The screen is divided into columns and rows. Then elements will be placed in those columns and rows. When the screensize reduces, the website automatically reduces the column and row values.</p> <pre><code>.grid-container {\n  display: grid; /* This is what defines the website is GRID */ \n  grid-template-columns: 1fr 1fr; /*1fr means 1 fraction */\n  grid-template-rows: 100px 200px 200px;\n  gap: 30px;\n}\n</code></pre> <p></p>"},{"location":"css/css-display/#css-flexbox","title":"CSS Flexbox","text":"<p>CSS Flexbox will divide the screen either horizontally or vertically (Basically 1D layout). The screen is divided into boxes. Then elements will be placed in those boxes. When the screensize reduces, the website automatically reduces the boxes.</p> <pre><code>.card {\n  display: flex\n  background: rebeccapurple;\n  border: 30px solid black;\n  height: 100px;\n  flex: 1;\n}\n</code></pre> <p></p>"},{"location":"css/css-display/#bootstrap-framework","title":"Bootstrap Framework","text":"<p>The Website is divided 12 different columns, and the code has already been written for those columns. As it is an external framework, all the code is already packaged, and we are just using them to build the layout.</p> <p></p>"},{"location":"css/intro/","title":"Introduction","text":"<p>Styling sheet will help to arrange the things in a specific way to look at our website more beautiful. To do this, there are many languages are available. Some of them are below.</p> <ol> <li>CSS (Cascading Style Sheet)</li> <li>Sass (Syntactically Awesome Style Sheet)</li> <li>Less (Leaner CSS)</li> </ol>"},{"location":"css/intro/#cascading-style-sheet-css","title":"Cascading Style Sheet (CSS)","text":"<ul> <li>It is a language to specify how the things should look in our website.</li> <li>It allowed us to attach the style such as colors, font, spacing, etc. to our HTML documents.</li> </ul> <p><code>TRIVIA: Who created CSS?</code></p> <p><code>Answer: Hakon Wium Lie</code></p>"},{"location":"css/intro/#how-to-add-css","title":"How to add CSS","text":"<p>There are three ways we can add CSS into our HTML documents.</p> <ul> <li>Inline</li> </ul> <pre><code>&lt;tag style=\"PROPERTY:VALUE\"/&gt;\n</code></pre> <ul> <li>Internal</li> </ul> <pre><code>&lt;style&gt;css&lt;/style&gt;\n</code></pre> <ul> <li>External</li> </ul> <pre><code>&lt;link href=\"style.css\"&gt;\n</code></pre>"},{"location":"css/intro/#1-inline","title":"1. Inline","text":"<p>This will be applied in the same line as the HTML element. It is useful when you work with a single HTML element. However, this is NOT useful if you are dealing with multi page and multiple HTML elements . Hence, it is NOT recommended for those cases. <pre><code>&lt;body style=\"background-color:red\"&gt;\n&lt;p&gt;Satheesh Pandian&lt;/p&gt;\n&lt;/body&gt;\n</code></pre></p> <p></p>"},{"location":"css/intro/#2-internal","title":"2. Internal","text":"<p>This will be applied via <code>style</code> tag in the HTML document under <code>head</code> element. The main difference between inline and internal is that inline can be applied anywhere in the HTML document, whereas internal can be applied only under <code>head</code> element. However, it can have the selector element that applies the internal CSS rule to that selector element. This can be useful if you are working in a single HTML document and NOT useful for multi page website.  <pre><code>&lt;head&gt;\n    &lt;style&gt;\n        body {\n            background-color: blue;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n&lt;/body&gt;\n</code></pre></p> <p></p>"},{"location":"css/intro/#3-external","title":"3. External","text":"<p>The main difference between internal and external is that all style components are stored in external file and will be invoked in HTML file and apply the style. This will be applied using <code>link</code> element under <code>head</code> section in the HTML document. This is useful for multi page website.</p> <pre><code>&lt;head&gt;\n    &lt;link href=\"style.css\" rel=\"stylesheet\" /&gt; &lt;! \u2013\u2013 rel means relationship --&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>body{\n    background-color: yellow\n}\n</code></pre> <p></p>"},{"location":"css/positioning/","title":"CSS Positioning","text":"<p>The position property specifies the type of positioning method used for an element.  Elements are then positioned using the top, bottom, left, and right properties.  However, these properties will not work unless the position property is set first.  They also work differently depending on the position value.</p> <p><code>1. Static Positioning</code> This is HTML default view. Static positioned elements are not affected by the top, bottom, left, and right properties.</p> <pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            div.static {\n              position: static;\n              border: 3px solid red;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n        &lt;div class=\"static\"&gt;\n            Bangalore;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>In the above example, <code>Bangalore</code> element is placed in default position, which is a top left corner of the browser.</p> <p></p> <p><code>2. Relative Positioning</code> An element is positioned relative to its normal position.</p> <p><pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            div.static {\n              position: relative;\n              border: 3px solid red;\n              left: 30px;\n              top: 50px;\n              right: 100px;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n        &lt;div class=\"static\"&gt;\n            Bangalore;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> In the above example, <code>Bangalore</code> element is placed in relative position to its default position.  Also, the relative position is 30px from the left and 50px from the top.</p> <p></p> <p><code>3. Absolute Positioning</code> An element is positioned relative to the nearest positioned ancestor,  or it uses a top left corner of the web page (When there is no ancestor).</p> <p><pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            div.static {\n              position: absolute;\n              border: 5px solid red;\n              left: 50px;\n              top: 100px;\n            }\n            .para-clss {\n                position: relative;\n                border: 5px solid blue;\n                top: 10px;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"para-clss\"&gt;Satheesh Pandian\n            &lt;div class=\"static\"&gt;\n                Bangalore\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> In the above example, <code>Bangalore</code> element is placed based on its ancestor <code>para-class</code> element.  Also, 50px from the left and 110px from the top (10px from its ancestor and 100px from this element class).</p> <p></p> <p><code>4. Fixed Positioning</code> An element is positioned from a top left corner of the browser window.  When you scroll the page, this fixed HTML element is also displayed along with scroll (which means this element will display all the time in the screen in the same position).</p> <p><pre><code>&lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            div.static {\n                position: fixed;\n                border: 5px solid red;\n                left: 50px;\n                top: 100px;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"para-clss\"&gt;\n        &lt;div class=\"static\"&gt;\n            Bangalore\n        &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> In the above example, <code>Bangalore</code> element is placed 50px from the left and 100px from top.</p> <p></p>"},{"location":"css/positioning/#z-index","title":"Z index","text":"<p>When elements are positioned, they can overlap other elements.  <code>z-index</code> property specifies which element should be placed in front of, or behind, the others.  It can be positive or negative.  If two positioned elements overlap each other without a z-index specified, the element defined last in the HTML code will be shown on top.</p> <p><pre><code>&lt;html&gt;\n\n&lt;head&gt;\n    &lt;style&gt;\n        .bangalore {\n            position: absolute;\n            background: red;\n            color: white;;\n            height: 100px;\n            width: 500px;\n            z-index: 1;\n        }\n\n        .chennai {\n            position: absolute;\n            color: white;\n            background: blue;\n            height: 100px;\n            width: 500px;\n            top: 20px;\n            left: 20px;\n            z-index: 2;\n        }\n\n        .mumbai {\n            position: absolute;\n            background: greenyellow;\n            height: 100px;\n            width: 500px;\n            top: 50px;\n            left: 50px;\n            z-index: -1;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;div class=\"para-clss\"&gt;\n        &lt;div class=\"bangalore\"&gt;Bangalore&lt;/div&gt;\n        &lt;div class=\"chennai\"&gt;Chennai&lt;/div&gt;\n        &lt;div class=\"mumbai\"&gt;Mumbai&lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;\n</code></pre> In the above example, class <code>chennai</code> has highest <code>z-index</code> value and hence it is displayed top of all other components.  class <code>mumbai</code> has the least <code>z-index</code> value than other elements.  Hence, it is behind than all other elements.</p> <p></p>"},{"location":"css/selector/","title":"CSS Selectors","text":""},{"location":"css/selector/#normal-selector","title":"Normal selector","text":"<pre><code>&lt;head&gt;\n    &lt;link href=\"./style.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;h3&gt;This is h3&lt;/h3&gt;\n    &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n&lt;/body&gt;\n</code></pre> <p><pre><code>h3 {\n  color: blue;\n}\np {\n  color: red;\n}\n</code></pre> In the above example, h3 and p are called as selectors as they are updated in CSS file and changing the property  based on the value mentioned in CSS file. </p> <p></p>"},{"location":"css/selector/#class-selector","title":"Class selector(.)","text":"<p>You can select any HTML element as a normal selector</p> <pre><code>&lt;head&gt;\n    &lt;link href=\"./style.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;h3&gt;This is h3&lt;/h3&gt;\n    &lt;p class=\"class-name\"&gt;Satheesh Pandian&lt;/p&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>h3 {\n  color: blue;\n}\np {\n  color: red;\n}\n.class-name{\n    color: violet\n}\n</code></pre> <p></p> <p>In the above example, though <code>p</code> element is defined in CSS file, <code>class-name</code> value has been applied due to high priority.</p> <p>class selector starts with <code>.</code> in the CSS file. You can apply class name to multiple HTML elements.</p> <pre><code>&lt;head&gt;\n    &lt;link href=\"./style.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;p class=\"class-name\"&gt;Satheesh Pandian&lt;/p&gt;\n    &lt;h3&gt;This is h3&lt;/h3&gt;\n    &lt;h5 class=\"class-name\"&gt;This is h5&lt;/h5&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>h3 {\n  color: blue;\n}\np {\n  color: red;\n}\n.class-name{\n    color: deeppink\n}\n</code></pre> <p></p>"},{"location":"css/selector/#id-selector","title":"ID selector (#)","text":"<pre><code>&lt;head&gt;\n    &lt;link href=\"./style.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;h3&gt;This is h3&lt;/h3&gt;\n    &lt;p id=\"satheesh-id\"&gt;Satheesh Pandian&lt;/p&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>h3 {\n  color: blue;\n}\np {\n  color: red;\n}\n#satheesh-id{\n    color: darkorange\n}\n</code></pre> <p>ID selector starts with <code>#</code> in the CSS file. This is unique, and you CANNOT apply id more than one HTML element.</p>"},{"location":"css/selector/#attribute-selector","title":"Attribute selector","text":"<pre><code>&lt;head&gt;\n    &lt;link href=\"./style.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;p class=\"class-name\" draggable=\"true\"&gt;Satheesh Pandian&lt;/p&gt;\n    &lt;h3&gt;This is h3&lt;/h3&gt;\n    &lt;p draggable=\"true\"&gt;This is draggable&lt;/p&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>p {\n  color: red;\n}\np[draggable] {\n  color: darkgreen;\n}\n.class-name {\n  color: deeppink;\n}\n</code></pre> <p>Here, draggable is an attribute. You can select any attribute</p> <p></p>"},{"location":"css/selector/#all-selector","title":"All selector (*)","text":"<pre><code>&lt;head&gt;\n    &lt;link href=\"./style.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;p&gt;Satheesh Pandian&lt;/p&gt;\n    &lt;h3&gt;This is h3&lt;/h3&gt;\n    &lt;p draggable=\"true\"&gt;This is draggable&lt;/p&gt;\n&lt;/body&gt;\n</code></pre> <pre><code>* {\n  color: darkmagenta;\n}\np[draggable] {\n  color: darkgreen;\n}\nh3 {\n  color: deeppink;\n}\n</code></pre>"},{"location":"haproxy/acl/","title":"Access Control List (ACL)","text":"<p>Let us assume that we are traveling on national highway, and sometimes we see the sign board saying that heavy vehicle lane. It means that heavy vehicles go in that lane and other vehicles are not allowed to go in that lane. Similarly, we can route our incoming traffic to specific server based on conditions.  This is called access control list. </p> <p>ACL in HA Proxy are rules or conditions that allow you to define patterns to match incoming requests.  These rules help you make decisions about how to route or manage traffic within your infrastructure. It will read all the http request information and take a decision based on that information. Here decision is nothing but to divert the request to different servers.</p>"},{"location":"haproxy/acl/#use-of-acl","title":"Use of ACL","text":"<p>1. Security : Using ACL, we can allow or deny the request. This helps to avoid DDos attack. </p> <p>2. Control over traffic : Using ACL, we can filter and route the traffic based on specific criteria.  This will help to avoid an unnecessary load on a specific server. Also, this will help to improve performance of the system as we segregate static data in one server  and dynamic data in different server.</p> <p>Syntax</p> <pre><code>acl &lt;acl name&gt; &lt;condition&gt;\nuse_backend &lt;specific backend name&gt; if &lt;acl name&gt;\n</code></pre>"},{"location":"haproxy/acl/#routing-traffic-based-on-url-path","title":"Routing Traffic Based on URL Path","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    acl is_static path_beg /static\n    use_backend static_backend if is_static\n    default_backend servers\n\nbackend static_backend\n    server server2 app2:5002 check\n\nbackend servers\n    balance roundrobin\n    server server1 app1:5001 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <p><pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/static\")\ndef data():\n    return \"Static Data from Flask App 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)  # Run the app on port 5001\n</code></pre> app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/static\")\ndef data():\n    return \"Static Data from Flask App 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)  # Run the app on port 5002\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/static\")\ndef data():\n    return \"Static Data from Flask App 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5003)  # Run the app on port 5003\n</code></pre> <p>docker-compose.yaml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5001:5001\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5002:5002\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5003:5003\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the URL in the browser with just \"/\" and after that hit the url with \"/static\"</p> <p></p> <p></p> <p></p> <p>Check the logs</p> <p></p>"},{"location":"haproxy/acl/#block-the-traffic-from-specific-ip","title":"Block the traffic from specific IP","text":"<p>I am running HA proxy in EC2 ubuntu machine.  I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    acl block_ip src 172.31.28.60/24\n    http-request deny if block_ip\n    default_backend servers\n\nbackend servers\n    balance roundrobin\n    server server1 app1:5001 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <p><pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/static\")\ndef data():\n    return \"Static Data from Flask App 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)  # Run the app on port 5001\n</code></pre> app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/static\")\ndef data():\n    return \"Static Data from Flask App 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)  # Run the app on port 5002\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/static\")\ndef data():\n    return \"Static Data from Flask App 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5003)  # Run the app on port 5003\n</code></pre> <p>docker-compose.yaml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5001:5001\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5002:5002\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5003:5003\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit url from another machine whose IP address is <code>172.31.28.60</code></p> <p></p>"},{"location":"haproxy/acl/#route-traffic-based-on-http-method","title":"Route Traffic Based on HTTP method","text":"<p>I am running HA proxy in EC2 ubuntu machine.  I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    acl is_post_method method POST\n    use_backend static_backend if is_post_method\n    default_backend servers\n\nbackend static_backend\n    server server2 app2:5002 check\n\nbackend servers\n    balance roundrobin\n    server server1 app1:5001 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <p><pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/static\", methods=['POST'])\ndef data():\n    return \"Static Data from Flask App 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)  # Run the app on port 5001\n</code></pre> app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/static\", methods=['POST'])\ndef data():\n    return \"Static Data from Flask App 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)  # Run the app on port 5002\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/static\", methods=['POST'])\ndef data():\n    return \"Static Data from Flask App 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5003)  # Run the app on port 5003\n</code></pre> <p>docker-compose.yaml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5001:5001\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5002:5002\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5003:5003\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit url using postman for GET method call</p> <p></p> <p>Hit url using postman for POST method call</p> <p></p>"},{"location":"haproxy/haproxy/","title":"HA Proxy Configuration","text":"<p>HA Proxy configuration is the place where you define all the servers are involved, the application running port, log setting, load balancing algorithm, etc. This is written in the file located at  <code>/etc/haproxy/haproxy.cfg</code></p> <p>HA Proxy configuration has four main parts</p> <ol> <li>Global</li> <li>Defaults</li> <li>Frontend</li> <li>Backend</li> </ol>"},{"location":"haproxy/haproxy/#global","title":"Global","text":"<p>The parameters defined under global are applicable to the process and OS. Once you defined them, you don't need to change often.</p> <pre><code>global\n    log &lt;IP address of the machine where logs stores&gt; &lt;storage drive&gt;\n    log &lt;log file path&gt; &lt;storage drive&gt; &lt;log level&gt;\n    maxconn &lt;number of connections&gt;\n    chroot /var/lib/haproxy # This line tells HAProxy to run in a restricted area of the file system\n    # This sets up a special socket (a kind of communication endpoint) for administrative commands. \n    # The mode 660 part defines the permissions for this socket, allowing specific users to manage HAProxy.\n    stats socket /run/haproxy/admin.sock mode 660 \n    user haproxy\n    group haproxy\n    daemon\n</code></pre> <p></p>"},{"location":"haproxy/haproxy/#defaults","title":"Defaults","text":"<p>This is the place where you define all default settings for HAProxy. It applies to both frontend and backend as well unless it is overridden </p> <pre><code>defaults\n    # log specifies the sys log entry location\n    log global\n\n    # mode specifies the protocol. If http request, mode is http. Else, it is tcp\n    mode http\n\n    # Option httplog tells enable logging for http request, response, header, session, etc\n    option  httplog\n\n    # dontlognull tells do not log any null values (means if there is no data transfered)\n    option  dontlognull\n\n    # Number of connection attempts to be made when the server is refused the connection or times out.\n    retries &lt;number of retries allowed&gt;\n\n    # Client request should be completed before this time. Else, it will get timeout.\n    timeout http-request &lt;time in ms&gt;\n\n    # Time to wait before a connection is dropped and a client receives a 503 or \"Service Unavailable\" error\n    timeout queue &lt;time in ms&gt;\n\n     # Time to wait to make a successful connection\n    timeout connect &lt;time in ms&gt;\n\n    # Time to wait for a client remain inactive (it neither accepts nor sends data).\n    timeout client  &lt;time in ms&gt;\n\n    # Time to wait for a server to accept or send data before timeout occurs\n    timeout server  &lt;time in ms&gt;\n</code></pre> <p></p>"},{"location":"haproxy/haproxy/#frontend","title":"Frontend","text":"<p>It is a configuration where the server listens to sockets for a client connection requests.</p> <pre><code>frontend &lt;Name of the front end&gt;\n    bind &lt;IP Address&gt;:&lt;port&gt;\n    default_backend &lt;Name of the backt end where front end requests redirect to&gt;\n</code></pre> <p></p>"},{"location":"haproxy/haproxy/#backend","title":"Backend","text":"<p>It is the place where we specify the real server IP addresses as well as the load balancer scheduling algorithm.</p> <pre><code>backend &lt;Name of the backt end where front end requests redirect to&gt;\n    balance &lt;Algorithm Type&gt;\n    server &lt;application/service name 1&gt; &lt;IP address of the service/app1 running&gt;:&lt;port&gt; check\n    server &lt;application/service name 2&gt; &lt;IP address of the service/app2 running&gt;:&lt;port&gt; check inter &lt;time&gt; rise 4 fall 3\n    server &lt;application/service name 3&gt; &lt;IP address of the service/app3 running&gt;:&lt;port&gt; check\n    server &lt;application/service name 4&gt; &lt;IP address of the service/app4 running&gt;:&lt;port&gt; backup\n</code></pre> <p><code>check</code> ensures the server is up and running (health check), and it checks the server periodically. If the server is down,  then it will ensure that no requests are redirected to that server</p> <p><code>inter</code> is the health check interval.</p> <p><code>rise</code> is the number of consecutive valid health checks before considering the server is up and running</p> <p><code>fall</code> is the number of consecutive valid health checks before considering the server is down</p> <p></p>"},{"location":"haproxy/intro/","title":"Introduction of Load Balancing and HA Proxy","text":""},{"location":"haproxy/intro/#load-balancer","title":"Load Balancer","text":"<p>Let us assume that you are running your application in three-tier architecture, and you configure the webserver to allow up to 1000 connections. When it exceeds 1000 connections, you have to increase the connections to accommodate more.</p> <p></p> <p>However, at one point of time, your web server can reach the threshold limit, and it cannot accommodate anymore connections. In that situation, your option is to increase the number of web servers and balance the load.</p> <p></p> <p>All the incoming requests go to load balancer, and that will distribute the requests to the web servers based on the algorithm configured.</p>"},{"location":"haproxy/intro/#ha-proxy","title":"HA Proxy","text":"<ol> <li>TCP proxy: It can accept a TCP connection by listening socket, and connect the server by attaching  the socket to allow the traffic in both directions.</li> <li>HTTP reverse proxy: It can act as a server and receive HTTP connections over listening to a socket and  send the request to the different servers using different connections.</li> <li>SSL terminator: It ensures the connection is secured.</li> <li>TCP normalizer: It drops the pockets or incomplete connections when the abnormal traffic happens.</li> <li>HTTP normalizer: It also drops HTTP request which is invalid. </li> <li>Load balance: It acts as a load balance and accepts TCP mode and HTTP mode.</li> </ol> Mode Description TCP Whole connection can be affected due to load balancing technique HTTP Connection can be affected based on the HTTP request <ol> <li>Traffic regulator: It is helpful to set the rate limiting for the server.</li> </ol> <p>Why HA Proxy</p> <ul> <li>Reliable</li> <li>Fast</li> <li>Load balancer</li> <li>Statistics and monitoring</li> <li>Open source</li> </ul> <p>HA Proxy Tasks</p> <p>It mainly performs two different tasks.</p> <ol> <li>Processing the incoming request</li> <li>Check the health of the servers periodically. So, it will pass the request to the servers that are up and running.</li> </ol>"},{"location":"haproxy/intro/#ha-proxy-components","title":"HA Proxy Components","text":"<ol> <li>Frontend component: It defines the IP address and port where the socket/proxy is listening to.</li> <li>Backend component: It has all the server details and algorithm details.</li> </ol>"},{"location":"haproxy/least-connection/","title":"Least Connection Algorithm","text":"<p>It distributes the requests to the servers which have the least number of connections. </p> <p></p> <p>Let us assume that there are three servers running in the cluster, and the incoming requests are distributed among these three servers and the scheduling algorithm is configured as <code>round-robin</code> algorithm. </p> <p>There are three incoming requests to the load balancer. The first server is handling ten connections, and the second  server is handling seven connections, and the third server is handling five connections. We are getting some another three incoming requests, and the first two requests are assigned to the third server as it has five connections, and now it has seven connections. At this point, the second and third servers are serving seven connections, and the first server is serving ten connections. The third incoming request will be assigned either the third server or the second server as they are having the least number of connections at the moment.</p>"},{"location":"haproxy/least-connection/#ha-proxy-configuration-for-least-connection","title":"HA Proxy configuration for Least Connection","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    default_backend servers\n\nbackend servers\n    balance leastconn\n    server server1 app1:5001 check\n    server server2 app2:5002 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)\n</code></pre> <p>Dockerfile.app1</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app1.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n\n# Run app.py when the container launches\nCMD [\"python\", \"app1.py\"]\n</code></pre> <p>Dockerfile.app2</p> <p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app2.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5001 available to the world outside this container\nEXPOSE 5001\n\n# Run app.py when the container launches\nCMD [\"python\", \"app2.py\"]\n</code></pre> Dockerfile.app3</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app3.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5002 available to the world outside this container\nEXPOSE 5002\n\n# Run app.py when the container launches\nCMD [\"python\", \"app3.py\"]\n</code></pre> <p>Dockerfile.haproxy</p> <pre><code># Use the official HAProxy image from Docker Hub\nFROM haproxy:latest\n\n# Copy the custom HAProxy configuration file into the container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n\n# Expose the port for HAProxy\nEXPOSE 8888\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5000:5000\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5001:5001\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5002:5002\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Now, we have all the configurations are in place. </p> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the load balance url a couple of times. Each time the request goes to different server.</p> <p></p> <p></p> <p></p>"},{"location":"haproxy/monitoring/","title":"HA Proxy Monitoring","text":"<p>We can enable the statistics of HA proxy to monitor the server status. To do so, we need to update the haproxy configuration file. Add the below content in the configuration file under frontend section.</p> <pre><code>    stats enable\n    stats auth admin:password\n    stats hide-version\n    stats show-node\n    stats refresh 10s\n    stats uri /haproxy?statistics\n</code></pre> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the URL in the browser</p> <p></p> <p>Enter the user credential, and you will be able to see the monitoring page.</p> <p></p> <p>As you have seen, all three backend servers are up and running. Let us shutdown one server from these list.</p> <p></p> <p>Now, refresh the browser where you are monitoring.</p> <p></p>"},{"location":"haproxy/monitoring/#tcp-vs-http-mode","title":"TCP Vs HTTP Mode","text":"<p>When you are working on HA Proxy configuration, you need to choose mode. There are two modes supported by HA Proxy.</p> <ol> <li>TCP</li> <li>HTTP</li> </ol> <p>We can choose any one of them and update the configuration.</p>"},{"location":"haproxy/monitoring/#tcp-mode","title":"TCP Mode","text":"<ol> <li>It works on the transport layer in the OSI model. </li> <li>Actually, TCP protocol carries the HTTP data.</li> <li>TCP Protocol has general information about</li> <li>Source and destination ports</li> <li>Specific flags such as an acknowledgement, sync to guarantee to receive data ordering data.</li> </ol> <p><code>Whenever we use round-robin algorithm, we need to use TCP mode. Also, if we are not using scheduling algorithm, we need to use TCP mode.</code></p>"},{"location":"haproxy/monitoring/#http-mode","title":"HTTP Mode","text":"<ol> <li>It is more specific to the HTTP request.</li> <li>Basically, it works reading the http header information such as User-agent, URI path, etc.</li> </ol> <p><code>Whenever we use scheduling algorithm that needs information from http header or access list that reads http header,  then we need to use HTTP mode.</code></p>"},{"location":"haproxy/monitoring/#forward-for-option","title":"Forward For Option","text":"<p>As we know, a client makes a request goes to load balancer, and load balancer redirects the request to the backend server based on the algorithm configured. </p> <p></p> <p>However, backend server doesn't know the actual client who made the request. It has the visibility till the load balancer. So if you check the server logs, the ip address always HA proxy machine IP address. </p> <p>If we want to know the real client who made the actual request, then we need to use <code>forwardfor</code> option in the configuration. Also, add <code>%{X-Forwarded-For}i</code> in the log configuration of the application.</p> <p></p>"},{"location":"haproxy/round-robin/","title":"Round-Robin Algorithm","text":"<p>It distributes the requests to the servers sequentially one by one. It doesn't consider the server capacity or load.</p> <p></p> <p>Let us assume that there are three servers running in the cluster, and the incoming requests are distributed among these three servers and the scheduling algorithm is configured as <code>round-robin</code> algorithm. </p> <p>There are three incoming requests to the load balancer. The First request is going to first server,   and the second request is going to second server and the third request is going to third server.</p> <p>Let us assume that the first request is taking 10 seconds to process. The Second request is taking 8 seconds to process, and the third request is taking 3 seconds to process.</p> <p>After three seconds, we are getting some another three incoming requests, and these requests are assigned to the respective servers. At this point, the first and third servers are serving two requests, and the second server is serving only one request. It is clearly visible that two of the servers are overloaded and one server is not overloaded.</p> <p></p>"},{"location":"haproxy/round-robin/#ha-proxy-configuration-for-round-robin","title":"HA Proxy configuration for round-robin","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    default_backend servers\n\nbackend servers\n    balance roundrobin\n    server server1 app1:5001 check\n    server server2 app2:5002 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)\n</code></pre> <p>Dockerfile.app1</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app1.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n\n# Run app.py when the container launches\nCMD [\"python\", \"app1.py\"]\n</code></pre> <p>Dockerfile.app2</p> <p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app2.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5001 available to the world outside this container\nEXPOSE 5001\n\n# Run app.py when the container launches\nCMD [\"python\", \"app2.py\"]\n</code></pre> Dockerfile.app3</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app3.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5002 available to the world outside this container\nEXPOSE 5002\n\n# Run app.py when the container launches\nCMD [\"python\", \"app3.py\"]\n</code></pre> <p>Dockerfile.haproxy</p> <pre><code># Use the official HAProxy image from Docker Hub\nFROM haproxy:latest\n\n# Copy the custom HAProxy configuration file into the container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n\n# Expose the port for HAProxy\nEXPOSE 8888\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5000:5000\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5001:5001\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5002:5002\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Now, we have all the configurations are in place. </p> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the load balance url a couple of times. Each time the request goes to different server.</p> <p></p> <p></p> <p></p> <p>Check the logs of the ha proxy container</p> <p></p>"},{"location":"haproxy/source-ip/","title":"Source IP Algorithm","text":"<p>It distributes the requests to the servers based on the client IP address. It is always forward a request coming from the same client to a specific server. </p> <p></p> <p>Let us assume that there are three servers running in the cluster, and the incoming requests are distributed among these three servers and the scheduling algorithm is configured as <code>Source IP</code> algorithm.</p> <p>If client A is sending a request from IP address 192.168.132.25 to the load balancer, and load balancer forwards the request to server 2. If client B is sending a request from IP address 192.168.132.28 to the load balancer, and load balancer forwards the request to server 1. </p> <p>From now on, the incoming requests coming from client A which is IP address 192.168.132.25 always go to server 2. Similarly, the incoming requests coming from client B which is IP address 192.168.132.28 always go to server 1.</p>"},{"location":"haproxy/source-ip/#ha-proxy-configuration-for-source-ip","title":"HA Proxy configuration for Source IP","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    default_backend servers\n\nbackend servers\n    balance source\n    server server1 app1:5001 check\n    server server2 app2:5002 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)\n</code></pre> <p>Dockerfile.app1</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app1.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n\n# Run app.py when the container launches\nCMD [\"python\", \"app1.py\"]\n</code></pre> <p>Dockerfile.app2</p> <p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app2.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5001 available to the world outside this container\nEXPOSE 5001\n\n# Run app.py when the container launches\nCMD [\"python\", \"app2.py\"]\n</code></pre> Dockerfile.app3</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app3.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5002 available to the world outside this container\nEXPOSE 5002\n\n# Run app.py when the container launches\nCMD [\"python\", \"app3.py\"]\n</code></pre> <p>Dockerfile.haproxy</p> <pre><code># Use the official HAProxy image from Docker Hub\nFROM haproxy:latest\n\n# Copy the custom HAProxy configuration file into the container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n\n# Expose the port for HAProxy\nEXPOSE 8888\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5000:5000\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5001:5001\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5002:5002\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Now, we have all the configurations are in place. </p> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the load balance url a couple of times. Check the logs of the ha proxy container.</p> <p></p>"},{"location":"haproxy/url-param/","title":"URL Hashing Algorithm - Based on Parameter","text":"<p>It distributes the requests to the servers based on the url parameter defined. .</p> <p></p> <p>Let us assume that there are three servers running in the cluster, and the incoming requests are distributed among these three servers and the scheduling algorithm is configured as <code>url param</code> algorithm.</p> <p>If there is an incoming request along with url parameter defined to the load balancer, then it will be served by any one of the web servers. From this moment, whatever request comes with the same url parameter goes to this web server only. </p>"},{"location":"haproxy/url-param/#ha-proxy-configuration-for-url-hashing","title":"HA Proxy configuration for URL Hashing","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    default_backend servers\n\nbackend servers\n    balance url_param userid\n    server server1 app1:5001 check\n    server server2 app2:5002 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)\n</code></pre> <p>Dockerfile.app1</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app1.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n\n# Run app.py when the container launches\nCMD [\"python\", \"app1.py\"]\n</code></pre> <p>Dockerfile.app2</p> <p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app2.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5001 available to the world outside this container\nEXPOSE 5001\n\n# Run app.py when the container launches\nCMD [\"python\", \"app2.py\"]\n</code></pre> Dockerfile.app3</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app3.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5002 available to the world outside this container\nEXPOSE 5002\n\n# Run app.py when the container launches\nCMD [\"python\", \"app3.py\"]\n</code></pre> <p>Dockerfile.haproxy</p> <pre><code># Use the official HAProxy image from Docker Hub\nFROM haproxy:latest\n\n# Copy the custom HAProxy configuration file into the container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n\n# Expose the port for HAProxy\nEXPOSE 8888\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5000:5000\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5001:5001\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5002:5002\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Now, we have all the configurations are in place. </p> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the load balance url with <code>userid</code> parameter for few times. </p> <p></p> <p></p> <p>Check the logs of the ha proxy container</p> <p></p>"},{"location":"haproxy/url-param/#url-hashing-algorithm-based-on-user-agent","title":"URL Hashing Algorithm - Based on User Agent","text":"<p>It distributes the requests to the servers based on the user agent such as chrome, firefox, etc.</p> <p></p> <p>Let us assume that there are three servers running in the cluster, and the incoming requests are distributed among these three servers and the scheduling algorithm is configured as <code>user-agent</code> algorithm.</p> <p>If there is an incoming request coming from chrome browser to the load balancer, then it will be served by any one of the web servers. From this moment, whatever request comes with the same browser goes to this web server only.  Similarly, If there is an incoming request coming from firefox browser to the load balancer, then it will be served by any one of the web servers. From this moment, whatever request comes with the same browser goes to this web server only. </p>"},{"location":"haproxy/url-param/#ha-proxy-configuration-for-url-hashing_1","title":"HA Proxy configuration for URL Hashing","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    default_backend servers\n\nbackend servers\n    balance hdr(User-Agent)\n    server server1 app1:5001 check\n    server server2 app2:5002 check\n    server server3 app3:5003 check\n</code></pre> <p></p> <p>app1.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)\n</code></pre> <p>Dockerfile.app1</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app1.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n\n# Run app.py when the container launches\nCMD [\"python\", \"app1.py\"]\n</code></pre> <p>Dockerfile.app2</p> <p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app2.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5001 available to the world outside this container\nEXPOSE 5001\n\n# Run app.py when the container launches\nCMD [\"python\", \"app2.py\"]\n</code></pre> Dockerfile.app3</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app3.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5002 available to the world outside this container\nEXPOSE 5002\n\n# Run app.py when the container launches\nCMD [\"python\", \"app3.py\"]\n</code></pre> <p>Dockerfile.haproxy</p> <pre><code># Use the official HAProxy image from Docker Hub\nFROM haproxy:latest\n\n# Copy the custom HAProxy configuration file into the container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n\n# Expose the port for HAProxy\nEXPOSE 8888\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5000:5000\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5001:5001\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5002:5002\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Now, we have all the configurations are in place. </p> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the load balance url from chrome browser a few times. </p> <p></p> <p>Hit the load balance url from firefox browser a few times. </p> <p></p> <p>Check the logs of the ha proxy container</p> <p></p>"},{"location":"haproxy/weight/","title":"Weighted Round-Robin Algorithm","text":"<p>It distributes the requests to the servers based on the weightage defined. It doesn't consider the server capacity or load.</p> <p></p> <p>Let us assume that there are three servers running in the cluster, and the incoming requests are distributed among these three servers and the scheduling algorithm is configured as <code>weighted round-robin</code> algorithm. The weightages of the servers are 2,1,3 respectively (Higher number is the higher priority).</p> <p>If there are six incoming requests to the load balancer, three of the incoming requests go to the third server, two of the incoming requests go to the first server, and one request goes to the second server.</p>"},{"location":"haproxy/weight/#ha-proxy-configuration-for-weighted-round-robin","title":"HA Proxy configuration for weighted round-robin","text":"<p>I am running HA proxy in EC2 ubuntu machine. I am also creating three APIs running on different ports using Flask application. I created docker images for these three APIs and created docker image for HA proxy configuration as well. I am running everything in docker compose file.</p> <p>HAproxy.cfg</p> <pre><code>global\n    log stdout format raw local0\n    daemon\n\ndefaults\n    log     global\n    mode    http\n    option  httplog\n    option  dontlognull\n    timeout connect 5000ms\n    timeout client  50000ms\n    timeout server  50000ms\n\nfrontend http_front\n    bind *:8888\n    default_backend servers\n\nbackend servers\n    balance roundrobin\n    server server1 app1:5001 weight 2 check\n    server server2 app2:5002 weight 1 check\n    server server3 app3:5003 weight 3 check\n</code></pre> <p></p> <p>app1.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 1!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 1!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n</code></pre> <p>app2.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 2!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 2!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n</code></pre> <p>app3.py</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/', methods=['GET'])\ndef home():\n    return \"Hello from Flask from app 3!\"\n\n@app.route(\"/data\")\ndef data():\n    return \"Data from Flask app 3!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5002)\n</code></pre> <p>Dockerfile.app1</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app1.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n\n# Run app.py when the container launches\nCMD [\"python\", \"app1.py\"]\n</code></pre> <p>Dockerfile.app2</p> <p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app2.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5001 available to the world outside this container\nEXPOSE 5001\n\n# Run app.py when the container launches\nCMD [\"python\", \"app2.py\"]\n</code></pre> Dockerfile.app3</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY app3.py .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install flask\n\n# Make port 5002 available to the world outside this container\nEXPOSE 5002\n\n# Run app.py when the container launches\nCMD [\"python\", \"app3.py\"]\n</code></pre> <p>Dockerfile.haproxy</p> <pre><code># Use the official HAProxy image from Docker Hub\nFROM haproxy:latest\n\n# Copy the custom HAProxy configuration file into the container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n\n# Expose the port for HAProxy\nEXPOSE 8888\n</code></pre> <p>docker-compose.yml</p> <pre><code>version: '3'\n\nservices:\n  app1:\n    build:\n      context: .\n      dockerfile: Dockerfile.app1\n    container_name: flask_app1\n    ports:\n      - \"5000:5000\"\n\n  app2:\n    build:\n      context: .\n      dockerfile: Dockerfile.app2\n    container_name: flask_app2\n    ports:\n      - \"5001:5001\"\n\n  app3:\n    build:\n      context: .\n      dockerfile: Dockerfile.app3\n    container_name: flask_app3\n    ports:\n      - \"5002:5002\"\n\n  haproxy:\n    build:\n      context: .\n      dockerfile: Dockerfile.haproxy\n    container_name: haproxy\n    ports:\n      - \"8888:8888\"\n    depends_on:\n      - app1\n      - app2\n      - app3\n</code></pre> <p>Now, we have all the configurations are in place. </p> <p></p> <p>Let us create the containers using <code>docker compose up -d</code> command</p> <p></p> <p>Hit the load balance url a couple of times. Each time the request goes to different server.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Check the logs of the ha proxy container</p> <p></p>"},{"location":"helm/basic-commands/","title":"Basic Helm Commands","text":"<p>All the helm commands are executed in command line interface (helm cli).</p>"},{"location":"helm/basic-commands/#1-helm-help","title":"1. helm help","text":"<p>This is the command that helps you find out the right command to do something.</p> <p></p>"},{"location":"helm/basic-commands/#2-helm-search","title":"2. helm search","text":"<p>There are two ways to search the chart.</p> <p>2.1. Search in the artifacthub.io - <code>helm search hub &lt;chart name&gt;</code> </p> <p>2.2. Search in the repository - <code>helm search repo &lt;chart name&gt;</code> </p> <p>But before run <code>helm search repo &lt;chart name&gt;</code> command, you must ensure that the repo has been added in your machine. To do so, run the command <code>helm repo add &lt;repository url&gt;</code></p> <p></p>"},{"location":"helm/basic-commands/#3-helm-install","title":"3. helm install","text":"<p>Before installing any chart, you must need to ensure that the repository is added locally. To do so,  run the command <code>helm repo add &lt;repository url&gt;</code></p> <p></p> <p>To verify if the repo is added or not, use <code>helm repo list</code> command</p> <p></p> <p>Now, install the chart using <code>helm install &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt;</code></p> <p></p> <p>In the above snapshot, my-prometheus is the release name, prometheus-community is the repo name, and prometheus is the  chart name. After installing the chart, validate the resources created in the k8s cluster.</p> <p></p> <p>To install a specific version, run the below command <code>helm install &lt;release name&gt; &lt;repo name&gt;/&lt;chart name&gt; --version &lt;version number&gt;</code></p> <p></p>"},{"location":"helm/basic-commands/#4-helm-list","title":"4. helm list","text":"<p>This is the command that helps you list all the releases in your k8s cluster.</p> <p></p>"},{"location":"helm/basic-commands/#5-helm-uninstall","title":"5. helm uninstall","text":"<p>This is the command that helps you uninstall the releases in your k8s cluster.</p> <p></p> <p>After uninstalling the chart, validate the resources deleted in the k8s cluster.</p> <p></p>"},{"location":"helm/basic-commands/#sub-commands","title":"Sub Commands","text":"<p>You can find the sub commands using <code>helm &lt;command&gt; help</code></p> <p></p>"},{"location":"helm/basic-commands/#6-helm-repo-list","title":"6. helm repo list","text":"<p>This command lists all the repository added in your local machine.</p> <p></p>"},{"location":"helm/basic-commands/#7-helm-repo-remove","title":"7. helm repo remove","text":"<p>This command lists all the repository removed in your local machine.</p> <p></p> <p><code>Remember that when you install any chart, default values will be picked and created k8s objects in the cluster. You cannot modify that during the installation of the chart</code></p> <p>If you want to override the default values, there are three ways.</p> <p>First approach</p> <p>Overrides the values through runtime arguments like below</p> <pre><code>helm install --set &lt;variable name&gt;=&lt;value to override&gt; &lt;repo name&gt;/&lt;chart name&gt;\n\nExample:\nhelm install --set replicaCount=3 prometheus-community/prometheus\n\nhelm install --set replicaCount=3 --set maintainer=\"satheeshpandianj@gmail.com\" prometheus-community/prometheus\n</code></pre> <p>Second approach</p> <p>Keep all the values to be overridden in a separate YAML file and pass that file during installation.</p> <pre><code>custom-file.yaml\n\nreplicaCount: 1\nmaintainer: \"satheeshpandianj@gmail.com\"\n</code></pre> <p><pre><code>helm install --values &lt;YAML file name including the path&gt; &lt;repo name&gt;/&lt;chart name&gt;\n\nExample:\nhelm install --values ~/custom-file.yaml prometheus-community/prometheus\n</code></pre> Third approach</p> <p>Download the chart directory locally and update <code>values.yaml</code> file with updated values. To do so, use the below command</p> <p><code>helm pull &lt;repo name&gt; or helm pull --untar &lt;repo name&gt;</code></p> <p></p> <p></p> <p>Then install the chart from locally. To do so, use the below command <code>helm install &lt;release name&gt; &lt;local repo directory path&gt;</code></p> <p></p>"},{"location":"helm/basic-commands/#8-helm-history","title":"8. helm history","text":"<p>This command helps to see all the revisions for a particular release. <code>helm history &lt;release name&gt;</code></p> <p></p>"},{"location":"helm/basic-commands/#9-helm-rollback","title":"9. helm rollback","text":"<p>This command helps to roll back to a previous version for a particular release. <code>helm rollback &lt;release name&gt;</code></p> <p></p>"},{"location":"helm/chart-hooks/","title":"Chart Hooks","text":"<p>Helm chart will do some extra actions beside creating k8s objects. For example, helm chart can send email before upgrading and after upgrading. These extra actions called as hooks.</p> <p>Helm Life cycle is \"helm install -&gt; verify -&gt; render -&gt; install\"</p> <p>Hooks are in two categories. 1. Pre 2. Post</p> Annotations Description pre-install Executes after templates are rendered and before any k8s resources are created pre-upgrade Executes on an upgrade request after templates are rendered, but before any resources are updated pre-delete Executes on a deletion request before any resources are deleted from Kubernetes pre-rollback Executes on a rollback request after templates are rendered, but before any resources are rolled back post-install Executes after all resources are loaded into Kubernetes post-upgrade Executes on an upgrade request after all resources have been upgraded post-delete Executes on a deletion request after all of the release's resources have been deleted post-rollback Executes on a rollback request after all resources have been modified <p></p> <p><code>job.yaml</code></p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello\n  annotations:\n    # This is what defines this resource as a hook. Without this line, the\n    # job is considered part of the release.\n    \"helm.sh/hook\": pre-install\nspec:\n  template:\n    spec:\n      containers:\n      - name: hello\n        image: busybox:1.28\n        command: \n        - /bin/sh\n        - -c\n        - date; echo Pre-install hook is successful\n      restartPolicy: OnFailure\n</code></pre> <p>The above resource will be executed before all k8s objects are created because the above resource file has pre-install hook.</p> <p></p> <p><code>Remember that we can have multiple hook in the manifest file.</code> <pre><code>annotations:\n  \"helm.sh/hook\": post-install,post-upgrade\n</code></pre> If we are having multiple hooks, then their order of execution is defined based on weight. Weight must be a string and it can be negative or positive values. Order of execution is always in ascending order.</p> <pre><code>annotations:\n  \"helm.sh/hook-weight\": \"5\"\n</code></pre>"},{"location":"helm/chart-hooks/#hook-deletion-policy","title":"Hook Deletion Policy","text":"<pre><code>annotations:\n  \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n</code></pre> Annotations Description before-hook-creation Delete the previous resource before a new hook is launched (default) hook-succeeded Delete the resource after the hook is successfully executed hook-failed Delete the resource if the hook failed during execution <p><code>job.yaml</code></p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello\n  annotations:\n    # This is what defines this resource as a hook. Without this line, the\n    # job is considered part of the release.\n    \"helm.sh/hook\": pre-install\n    \"helm.sh/hook-deletion-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n      - name: hello\n        image: busybox:1.28\n        command: \n        - /bin/sh\n        - -c\n        - date; echo Pre-install hook is successful\n      restartPolicy: OnFailure\n</code></pre> <p>hook is configured as pre-install and hook deletion policy is as hook-succeeded. </p> <p></p> <p>In the above snapshot, job pod is created before all k8s resources are created, and once it is successful, it is automatically deleted as the hook deletion policy is defined as hook-succeeded.</p>"},{"location":"helm/conditions/","title":"Conditions","text":"<p>In helm, conditions are the same as other languages. This will help us to evaluate the conditions, and based on the output, we can take the decision.</p> <p>Example: If conditions.</p> <p>In helm, if the value is available in the <code>values.yaml</code> file, then it will update it in the manifest files. In case,  if the value is not available in the <code>values.yaml</code>, we can ignore that field and create the manifest files without that field. To do so, we can use if conditions.</p> <p>Let us assume that in our nginx chart, we have a service manifest file which has labels. This label needs to be picked up  from <code>values.yaml</code>.</p> <p><code>values.yaml</code></p> <p></p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  labels:\n    environment: {{ .Values.environment }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p>Let us install the helm chart now.</p> <p></p> <p>Now, let us assume that environment label is not present in the <code>values.yaml</code> file. As we know, this field is optional. Hence, it should be ignored when creating manifest file. To implement that logic, we are using if conditions.</p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  {{ if .Values.environment }}\n  labels:\n    environment: {{ .Values.environment }}\n  {{ end }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p>Now, let us try to dry run using template command and see the result.</p> <p></p> <p>As you see in the above snapshot, you don't find <code>label</code> field in the <code>service.yaml</code> manifest file because environment  label is not present in the <code>values.yaml</code>.</p> <p>However, if you notice closely the manifest file, you will be able to identify the empty line in the metadata section where <code>labels</code> field comes. This is because helm expects a field, but it is not available and hence helm leaves the entire line as empty. We can trim this empty line as well. To do so, we need to include \"-\" in the if conditional block.</p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  {{- if .Values.environment }}\n  labels:\n    environment: {{ .Values.environment }}\n  {{- end }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p></p>"},{"location":"helm/conditions/#create-manifest-files-based-on-conditions","title":"Create Manifest Files Based On Conditions","text":"<p>In case, if you want to create the manifest file only if certain conditions are met. Else, you don't need to create them. To do so, you have to update the entire YAML file in the if condition itself. Also add the required field in the <code>values.yaml</code> file.</p> <p><code>service.yaml</code></p> <p><pre><code>{{- if .Values.required }}\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  {{- if .Values.environment }}\n  labels:\n    environment: {{ .Values.environment }}\n  {{- end }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n{{- end }}\n</code></pre> <code>Remember that if condition should not have compared conditions such as eq, gt, and lt etc.</code></p> <p></p> <p></p> <p>When you run the template command, you will be able to see service.yaml file because <code>required</code> value is set to <code>true</code> in  the <code>values.yaml</code> file.</p> <p></p> <p>Let's change <code>required</code> value is as <code>false</code> and verify now.</p> <p></p> <p>When you run the template command, you will be not able to see service.yaml file because <code>required</code> value is set to <code>false</code> in  the <code>values.yaml</code> file.</p> <p></p>"},{"location":"helm/conditions/#scope-of-the-elements-using-with-statement","title":"Scope of the elements (using with statement)","text":"<p>By default, all the elements have root scope. It will start with <code>.</code> and then their absolute object path. We can reduce to write the entire object path by using <code>with</code> statement.</p> <p>Let us assume that <code>values.yaml</code> file have the following properties.</p> <p></p> <p>Update the <code>service.yaml</code> file.</p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  labels:\n    environment: {{ .Values.environment }}\n    {{- with .Values.web }}\n    name: {{ .tier.name }}\n    numberOfServers: {{ .tier.numberOfServers }}\n    {{- end }}\n    {{- with .Values.db }}\n    name: {{ .tier.name }}\n    numberOfServers: {{ .tier.numberOfServers }}\n    {{- end }}    \nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p></p> <p>When you run the template command, you will be able to see all the values. However, we didn't mention a full object path  for all the label properties.</p> <p></p> <p>If we want to include the root scope element within <code>with</code> statement block, then we must use <code>$</code> in the properties.</p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  labels:\n    environment: {{ .Values.environment }}\n    {{- with .Values.web }}\n    name: {{ .tier.name }}\n    numberOfServers: {{ .tier.numberOfServers }}\n    {{- end }}\n    {{- with .Values.db }}\n    name: {{ .tier.name }}\n    numberOfServers: {{ .tier.numberOfServers }}\n    release: {{ $.Release.Name }}\n    {{- end }}    \nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p></p>"},{"location":"helm/conditions/#using-range-statement","title":"Using range statement","text":"<p>If the properties are too many, and you don't need to include one by one as it is a tedious job. Instead, you can use  <code>range</code> statement.</p>"},{"location":"helm/conditions/#list-of-data-updated-in-manifest-file","title":"List of data updated in manifest file","text":"<p><code>values.yaml</code></p> <p></p> <p><code>configmap.yaml</code></p> <p></p>"},{"location":"helm/conditions/#dictionary-of-data-updated-in-manifest-file","title":"Dictionary of data updated in manifest file","text":"<p><code>values.yaml</code></p> <p></p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  labels:\n    environment: {{ .Values.environment }}\n    {{- range $key, $value := $.Values.web.tier }}\n    {{ $key }}: {{ $value }}\n    {{- end }}\n    {{- range $key, $value := $.Values.db.tier }}\n    {{ $key }}: {{ $value }}\n    release: {{ $.Release.Name }} \n    {{- end }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p></p> <p>When you run the template command, you will be able to see the below </p> <p></p>"},{"location":"helm/conditions/#named-templates","title":"Named Templates","text":"<p>Let us assume that there are labels which are repeated in multiple places and multiple manifest files. So, updating them in all the places and all the manifest files are complex in nature.  Instead, we update the labels in <code>_helpers.tpl</code> file and use them in the manifest files wherever required.</p> <p><code>NOTE: File name starts with '_' will be a template file and helm will not consider them as a normal kubernetes object files. Hence, it will not be considered while creating k8s objects.</code></p> <p><code>_helpers.tpl</code></p> <p></p> <pre><code>{{- define \"labels\" }}\n  app: nginx\n  environment: prod\n{{- end }}\n</code></pre> <p>Remember that indentation is important when you update the values in the <code>_helper.tpl</code> file because the same indentation will be applied in the manifest files when created.</p> <p>Let us implement the labels in the <code>service.yaml</code> file.</p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  labels:\n    {{- template \"labels\" }}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p></p> <p>Now, let us run the helm template command.</p> <p></p> <p>As you see the above snapshot,  there are only two spaces indentations applied in the service.yaml as the <code>_helpers.tpl</code> file has only two space indentations of those label values.  Also, you cannot increase the indentation in the <code>_helpers.tpl</code> file as those labels may be included in different places where different indentations are needed in those places.</p> <p>To avoid this issue, we need to use <code>indent</code> function. However, functions cannot be pipelined in <code>template</code> statement. So, we must use <code>include</code> statement instead of <code>template</code> statement</p> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\n  labels:\n    {{- include \"labels\" . | indent 2}}\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p><code>Remember that you need to change the indent value wherever you applied. For example, if the labels are applied with 4 spaces indentations, then in those places you need update indent 4</code></p> <p></p> <p>Now, let us run the helm template command.</p> <p></p>"},{"location":"helm/create-chart/","title":"Writing Helm Chart","text":"<p>To create a helm chart, we need to create a few manifest files like deployment.yaml etc. We can do that manually, and also can create using helm command. </p> <p><code>helm create &lt;chart name&gt;</code></p> <p>The above command will create a directory structure and the files required to create the chart.</p> <p></p> <p>We can modify the chart as per our needs now.</p>"},{"location":"helm/create-chart/#create-chart-for-nginx","title":"Create Chart for Nginx","text":"<p>Let us create a deployment and service for nginx. Then we can use them to create a helm chart.</p> <p><code>deployment.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30007\n</code></pre> <p>The above manifest files are basic ones and still not implemented helm templating. Before, we do helm templating, we can review the files created by the <code>helm create</code> command</p> <p><code>Chart.yaml</code></p> <p></p> <p><code>values.yaml</code></p> <p></p> <p>Let us make some changes in <code>values.yaml</code> file. Let us increase the replica to 3 from 1 and service type should be NodePort.</p> <p></p> <p>Now, let us make changes the manifest files to update the values based on <code>values.yaml</code> file.</p> <p><code>deployment.yaml</code></p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Release.Name }}-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: {{ .Values.replicaCount }}\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: {{ .Values.image.repository }}\n        image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n        ports:\n        - containerPort: {{ .Values.service.port }}\n</code></pre> <p><code>service.yaml</code></p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Release.Name }}-service\nspec:\n  type: {{ .Values.service.type }}\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - port: {{ .Values.service.port }}\n      targetPort: {{ .Values.service.port }}\n      nodePort: 30007\n</code></pre> <p></p> <p>Now, let us install the chart using <code>helm install</code> command. </p> <p></p> <p>As you see, there are three pods are running, and a service type is NodePort. So, the helm is picking up the right configuration from<code></code>values.yaml<code>and</code>Chart.yaml` files.</p> <p>On top of it, we ran a few commands before installing the chart.</p> <p><code>helm lint</code> -&gt; It helps to validate the YAML files format.</p> <p><code>helm template</code> -&gt; It helps to check if the values are picking from values.yaml and Chart.yaml files. However, it will not install the chart. This is just a dry run command.</p> <p>Let us create another release with the same chart we developed. However, this time we are going to pass the values  through command line as an arguments to override the existing values in <code>values.yaml</code> and <code>Chart.yaml</code> files.</p> <p></p>"},{"location":"helm/functions/","title":"Functions","text":""},{"location":"helm/intro/","title":"Introduction to Helm","text":"<p>Let us assume that you have an application hosted in kubernetes. This application is required to run the database, firewall rules, load balancer, and services to expose the outside world, etc., As it needs a database, it requires persistent volume, persistent volume claim and storage class. Basically this application is a collection of objects that need  to interconnect to make everything work as expected.</p> <p>All these are created using YAML file in kubernetes cluster. However, kubernetes doesn't know our application as a  whole. Instead, it understands that we created various objects and kubernetes make sure it exists in the cluster.  This makes it a little difficult to maintain when it comes to a big application which has multiple YAML files.</p> <p>Helm will solve this problem. It is a package manager for kubernetes. Helm looks all the objects as a group in a package. So, you need to tell Helm that what package it should modify. Helm will automatically take care of the objects it  needs to modify based on the package name. We DO NOT instruct that to helm.</p> <p>For example, a car has multiple components inside, and we don't need to know each component to drive the vehicle. We just   press the accelerator and hold the steering wheel. The Rest will be taken care automatically. Similarly, we just need to  instruct package name to Helm and Helm will do the magic automatically in kubernetes cluster.</p> <p>If we want to customize the object, we DO NOT need to go to YAML file of the specific object. Instead, we just need to  update the details in values.yaml file (Just one place). It is easy to upgrade the application with one single command. Helm always tracks the files which changed for each time. Hence, rolling back is again easy with one single command.  Even, we can uninstall the application with simple command.</p>"},{"location":"helm/intro/#prerequisites","title":"Prerequisites","text":"<ol> <li>Kubernetes cluster</li> <li><code>kubectl</code> should be configured</li> </ol> <p>If the above two conditions are mandatory to install Helm.</p> <p>You can verify helm installation using </p> <p><code>helm version</code></p> <p></p> <p>You can use help command for addition command details.</p> <p></p>"},{"location":"helm/intro/#difference-between-helm2-and-helm3","title":"Difference Between Helm2 and Helm3","text":""},{"location":"helm/intro/#high-level","title":"High level","text":"<p>In general, when you install helm in the machine, <code>helm cli</code> is automatically installed. <code>helm cli</code> client helps to perform the action in kubernetes cluster. It reads the command from the user and takes the action in kubernetes cluster.</p> <p>In Helm2, RBAC and CRD are not available. Hence, an extra component is installed in the kubernetes cluster and the component  is called tiller. Whenever we run the command in helm cli, it is talking to tiller and tiller communicates with kubernetes  cluster to perform the action whatever requested. By default, tiller has full privileges in k8s cluster. Any user who have  access to tiller can do anything in k8s cluster, which is really a security concern.</p> <p></p> <p>In Helm3, Tiller was removed. Helm cli directly communicates with kubernetes cluster and RBAC will take  care of the security.</p> <p>In Helm2, it tracks all the changes as a snapshot. Whenever we issue the command, it checks the current chart and  finds the differences with the previous version and apply the changes. </p> <p></p> <p>Let us assume it, we installed an initial version of a chart using Helm. Then, we are upgrading it using <code>kubectl</code> command  manually. After that if we issue <code>helm rollback</code> command, helm will NOT find any differences with previous version of  chart as the current k8s objects are upgraded manually and NOT via helm.</p> <p></p> <p>In Helm3, whenever we issue <code>helm rollback</code> command, helm will try to check the live objects in k8s cluster and compare it  with the previous version of chart and if it finds any differences, then the changes will be deployed. Even though, upgrades are done manually and helm is not tracking the changes, it is intelligent enough to check live objects and previous chart and take the decision accordingly. This is called <code>3 way stratefic merge patch</code>.</p> <p></p>"},{"location":"helm/intro/#helm-components","title":"Helm Components","text":"<p><code>helm cli</code> used to interact with k8s cluster. This is where you execute all the commands to perform actions in k8s cluster.</p> <p><code>charts</code> is nothing but a collection of files, and it contains all the instructions that helm needs to be performed (Create/ upgrade/uninstall all the objects in k8s cluster). It contains all the resource definitions necessary to run the application inside k8s cluster.</p> <p><code>release</code> - When the chart is applied in your cluster, release is created. It is nothing but a single instance of your  application. Once chart can be installed multiple times in the same k8s cluster. Each time it is installed,  a new release will be created.</p> <p>For example, httpd server chart. If you want two httpd servers run in your k8s cluster, you can install this chart twice.  Each one will have its own release and in turn own release name.</p> <p><code>Remember each release has multiple revisions. revision is nothing but the snapshot of the current state of the k8s cluster.</code></p> <p></p> <p><code>repository</code> is a place where all the charts can be collected and shared with public (only for k8s packages).  This is similar to docker hub. <code>artifacthub.io</code> is the place where almost all the charts are stored.</p> <p><code>metadata</code> - Helm needs to track all the chart that is installed, all the releases names and their revision state. All  these data is saved in the k8s cluster itself. This data is called metadata (data about data). So it can be shared among  the team members as long as k8s cluster is available.</p> <p><code>values.yaml</code> - This is the place where all the customized properties updated. Based on this file, helm will create the  objects in the k8s cluster. The actual value will be taken from this file and replace it in the template specified in the  configuration (YAML files).</p> <p></p>"},{"location":"helm/intro/#why-do-we-need-release","title":"Why do we need release?","text":"<p>When you install helm chart, you should provide the release name. The reason for this, each release considers as a  separate entity. Each release has its own revisions. To understand better, there are two releases created for your application in the k8s cluster. One for the customer and one for the developer. So, a developer can deploy the new features and do the  testing without touching customer release, and this will not impact any customer experience.</p> <p></p>"},{"location":"helm/intro/#helm-charts","title":"Helm Charts","text":"<p>In any helm chart directory, two files are mandatory. 1. value.yaml - This is the place where we define the configuration of our k8s objects 2. chart.yaml - This is the place where we provide all the information about the charts.</p> <p></p> <p><code>chart.yaml</code></p> <p>In the above snapshot,  apiVersion -&gt; Either v1 or v2. V1 is for Helm2 and v2 for Helm3. Remember that this field may NOT be available in Helm2 charts. Those charts were created long back. Now if you want to create a chart for Helm2, then this field should be v1. Let's say you are creating a chart for Helm3 and you update apiVersion as v1. Then, some fields described in the chart.yaml file may not be available because the v1 version belongs to Helm2 and Helm2 don't have those fields (Example: type, dependencies).</p> <p>appVersion -&gt; This is an application version, and it can be anything but your input. This is for information purpose.</p> <p>Version -&gt; This is a chart version. This helps to track the changes in the chart itself.</p> <p>type -&gt; Either application or library. application -&gt; for any application. library -&gt; For utilities to build chart.</p> <p>dependencies -&gt; Your chart depends on another chart. However, you don't need to maintain the dependency chart. Instead,  you are instructing helm to maintain dependency chart manifest files. These charts are placed under <code>charts</code> directory.</p> <p>keywords -&gt; Helps when searching the chart in the public repository.</p>"},{"location":"html/important-tags/","title":"HTML Elements","text":"<p>Below tags are crucial in HTML</p>"},{"location":"html/important-tags/#html","title":"HTML","text":"<p>Represents the top level element of the HTML document. Hence, It is called as a root element.</p>"},{"location":"html/important-tags/#head","title":"HEAD","text":"<p>Should contain the metadata about the document such as title, scripts, style sheet.</p> Tags Definition <code>title</code> It displays the title of the document in the browser's page <code>link</code> Specifies the relationship between the current document and external document. Generally, used to link CSS document <code>style</code> Contains the style information of the document. Mostly CSS file details <code>meta</code> Represent meta data such as base, link etc"},{"location":"html/important-tags/#header-tags-h1-to-h6","title":"Header tags (h1 to h6)","text":"<p><pre><code>&lt;h1&gt;Satheesh&lt;/h1&gt;\n&lt;h2&gt;Satheesh&lt;/h2&gt;\n&lt;h3&gt;Satheesh&lt;/h3&gt;\n&lt;h4&gt;Satheesh&lt;/h4&gt;\n&lt;h5&gt;Satheesh&lt;/h5&gt;\n&lt;h6&gt;Satheesh&lt;/h6&gt;\n</code></pre> </p> <pre><code>- header tags are ONLY from h1 to h6.\n- Keep only ONE h1 in your document as a best practice. Use another header tag for sub headers.\n- DO NOT skip header level tags. \n  - Example: Assume, you are using h1 and h2 already in your document. For next level title, you need to use h3. DO NOT go to h4 by skipping h3.\n</code></pre>"},{"location":"html/important-tags/#body","title":"BODY","text":"<p>Should contain the content of the document along with their structure</p>"},{"location":"html/important-tags/#paragraph-tags-p","title":"Paragraph tags (p)","text":"<p><pre><code>&lt;p&gt;Satheesh is a Sr.SRE&lt;/p&gt;\n&lt;p&gt;He is working in a bank&lt;/p&gt;\n</code></pre> </p>"},{"location":"html/important-tags/#void-elements-hr-and-br","title":"Void Elements (hr and br)","text":"<p>Void elements don't have content and closing tags. Instead, they have just starting tags with forward slash. They are known as self-closing tag.</p> <p><code>hr</code> = Horizontal Rule <pre><code>&lt;hr /&gt; \n</code></pre></p> <pre><code>&lt;p&gt;Satheesh&lt;/p&gt;\n&lt;hr/&gt;\n&lt;p&gt;Bangalore&lt;/p&gt;\n</code></pre> <p></p> <p><code>br</code> = Break  </p> <pre><code>&lt;p&gt;Satheesh\n&lt;br/&gt;\nBangalore&lt;/p&gt;\n</code></pre> <p></p>"},{"location":"html/important-tags/#unordered-list","title":"Unordered List","text":"<pre><code>&lt;ul&gt;\n  &lt;li&gt;Satheesh&lt;/li&gt;\n  &lt;li&gt;Pandian&lt;/li&gt;\n  &lt;li&gt;Jeganathan&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre>"},{"location":"html/important-tags/#ordered-list","title":"Ordered List","text":"<pre><code>&lt;ol&gt;\n  &lt;li&gt;Satheesh&lt;/li&gt;\n  &lt;li&gt;Pandian&lt;/li&gt;\n  &lt;li&gt;Jeganathan&lt;/li&gt;\n&lt;/ol&gt;\n</code></pre>"},{"location":"html/important-tags/#ordered-list-with-different-starting-number","title":"Ordered List with different starting number","text":"<pre><code>&lt;ol start=\"4\"&gt;\n  &lt;li&gt;Satheesh&lt;/li&gt;\n  &lt;li&gt;Pandian&lt;/li&gt;\n  &lt;li&gt;Jeganathan&lt;/li&gt;\n&lt;/ol&gt;\n</code></pre>"},{"location":"html/important-tags/#ordered-list-with-different-starting-number-and-type","title":"Ordered List with different starting number and type","text":"<pre><code>&lt;ol start=\"4\" type=\"I\"&gt;\n  &lt;li&gt;Satheesh&lt;/li&gt;\n  &lt;li&gt;Pandian&lt;/li&gt;\n  &lt;li&gt;Jeganathan&lt;/li&gt;\n&lt;/ol&gt;\n</code></pre>"},{"location":"html/important-tags/#nested-ordered-list","title":"Nested Ordered List","text":"<pre><code>&lt;ol&gt;\n  &lt;li&gt;Satheesh&lt;/li&gt;\n  &lt;li&gt;Pandian\n      &lt;ol&gt;\n          &lt;li&gt;HTML&lt;/li&gt;\n          &lt;li&gt;CSS&lt;/li&gt;\n          &lt;li&gt;Javascript&lt;/li&gt;\n      &lt;/ol&gt;\n  &lt;/li&gt;  \n  &lt;li&gt;Jeganathan&lt;/li&gt;\n&lt;/ol&gt;\n</code></pre>"},{"location":"html/important-tags/#nested-unordered-list","title":"Nested Unordered List","text":"<pre><code>&lt;ul&gt;\n  &lt;li&gt;Satheesh&lt;/li&gt;\n  &lt;li&gt;Pandian\n      &lt;ul&gt;\n          &lt;li&gt;HTML&lt;/li&gt;\n          &lt;li&gt;CSS\n              &lt;ul&gt;\n                    &lt;li&gt;Heading&lt;/li&gt;  \n                    &lt;li&gt;Paragraph&lt;/li&gt;  \n              &lt;/ul&gt;\n          &lt;/li&gt;\n          &lt;li&gt;Javascript&lt;/li&gt;\n      &lt;/ul&gt;\n  &lt;/li&gt;  \n  &lt;li&gt;Jeganathan&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre>"},{"location":"html/important-tags/#html-attribute-element","title":"HTML attribute element","text":"<pre><code>&lt;TAG attribute1=\"value\" attribute2=\"value\"&gt;Content&lt;/TAG&gt;\n</code></pre>"},{"location":"html/important-tags/#anchor-element","title":"Anchor element","text":"<pre><code>Click &lt;a href=\"https://mysrejourney.github.io/mkdocs/\" draggable=\"true\"&gt;here&lt;/a&gt; to go to my website\n</code></pre>"},{"location":"html/important-tags/#image-element","title":"Image element","text":"<pre><code>&lt;img src=\"https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.pinterest.com%2Fpin%2Fdistance--332562753704694887%2F&amp;psig=AOvVaw0CNlQL4Yy96-OPGQZnz6AU&amp;ust=1694255809398000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBAQjRxqFwoTCIjnn-_omoEDFQAAAAAdAAAAABAJ\"&gt;\n</code></pre>"},{"location":"html/introduction/","title":"HTML (Hyper Text Markup Language)","text":"<p>Hyper Text - Refers to the piece of text which links to another document in the website. This is the foundation of how HTML works.</p> <p>Markup Language - Refers to mark up the text and show them differently using tags.</p> <p><code>TRIVIA: Who created world's first website?</code></p> <p><code>Answer: Sir Tim Berners-Lee</code></p> <p>HTML defines the content and structure of the website. </p>"},{"location":"html/introduction/#element","title":"Element","text":"<p>Combination of tags and content is called as HTML Element <pre><code>&lt;STARTING TAG&gt;CONTENT&lt;/CLOSING TAG&gt;\n</code></pre></p>"},{"location":"html/introduction/#tags","title":"Tags","text":"<p>Elements are surrounded by tags. <pre><code>&lt;STARTING TAG&gt;&lt;/CLOSING TAG&gt;\n</code></pre> In General, CLOSING TAG is nothing but a STARTING TAG along with \"/\"</p> <p>Example: <pre><code>&lt;h4&gt;Satheesh&lt;/h4&gt;\n</code></pre> Whereas, </p> <ul> <li><code>&lt;h4&gt;</code> = Starting tag </li> <li><code>&lt;/h4&gt;</code> = Closing tag</li> <li><code>Satheesh</code> = Content</li> </ul>"},{"location":"html/introduction/#best-practices","title":"Best Practices","text":"<ol> <li>Keep only one h1 element in the web page</li> <li>Do not skip header elements (don't go from h1 to h3. Always go h1, h2 and so on.)</li> </ol>"},{"location":"html/web-pages/","title":"Template","text":""},{"location":"html/web-pages/#web-pages","title":"Web pages","text":"<p>In a website, there are multiple HTML documents available. Whenever we connect any website, this will connect \"index.html\" file. This should be a home page of that website, and it is stored at root level. Other HTML documents are stored under public folder. All the images are stored under assets/images folder.</p> <p>If we wanted to navigate another HTML document, those HTML documents are linked in index.html page using <code>\"&lt;a href&gt;\"</code> tags and attribute.</p>"},{"location":"html/web-pages/#html-boiler-plate","title":"<code>HTML Boiler Plate</code>","text":""},{"location":"html/web-pages/#doctype-html","title":"<code>&lt;!DOCTYPE html&gt;</code>","text":"<ul> <li>We are letting know the browser that this document code contains HTML code and the version of HTML.   By default, it will consider the latest version and the current latest version is HTML5</li> </ul>"},{"location":"html/web-pages/#html-langen","title":"<code>&lt;html lang=\"en&gt;</code>","text":"<ul> <li>This is the root level element of an HTML document. It means all other elements should be a child element of this HTML element.</li> <li><code>\"en\"</code> defines the language of the text content in that element</li> </ul>"},{"location":"html/web-pages/#head","title":"<code>&lt;head&gt;</code>","text":"<ul> <li>This is the place where we have all the information about this website. Remember, this information is NOT visible to users.</li> <li>Basically, it helps website to render its content</li> </ul>"},{"location":"html/web-pages/#meta","title":"<code>&lt;meta&gt;</code>","text":"<pre><code>&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n&lt;/head&gt;\n</code></pre> <ul> <li>Under <code>&lt;head&gt;</code>, <code>&lt;meta&gt;</code> tag for the character set should be always present.</li> <li>This ensures all the characters used in the website display properly.</li> </ul>"},{"location":"html/web-pages/#title","title":"<code>&lt;title&gt;</code>","text":"<pre><code>&lt;title&gt;\n    My website\n&lt;/title&gt;\n</code></pre> <ul> <li>Under <code>&lt;head&gt;</code>, <code>&lt;title&gt;</code> tag will tell you the title of the document, and it will be displayed in the browser, NOT in web page.</li> </ul>"},{"location":"html/web-pages/#body","title":"<code>&lt;body&gt;</code>","text":"<pre><code>&lt;body&gt;\n    CONTENT\n&lt;/body&gt;\n</code></pre> <ul> <li>Creating content and structure of the document goes in between <code>&lt;body&gt;</code> and <code>&lt;/body&gt;</code> tag</li> <li>All other tags should come under <code>&lt;body&gt;</code> tag</li> <li>The content updated under this tag is visible to users.</li> </ul>"},{"location":"k8s/basics/","title":"Basics of K8s","text":""},{"location":"k8s/basics/#pods","title":"Pods","text":"<p>Our goal is to deploy the application container in the worker nodes available k8s clusters and make sure it is up and  running all the time. But, k8s doesn't deploy containers directly in the worker nodes. Instead, k8s wrapped the container  by a k8s object known as pods.</p> <p>Pod is the single instance of an application, and it is the smallest unit in k8s.</p> <p></p> <p>If the load is increasing in the application, and you want to scale the containers. In this case, you need to create a pod.  You should NOT create a container in the existing pod.</p> <p></p> <p>In case of too many calls to the application and current node doesn't have sufficient resources to support the load, then  we can create a new node in the k8s cluster and create a pod to run the application in the new node.</p> <p></p> <p>Remember, <pre><code>pod should be one to one mapping with container. When you scale up, you need to create a pod, not a container.\nWhen you scale down, you delete a pod which interns delete the container.\n</code></pre></p> <p>Does a pod not have many containers?</p> <p>The answer is NO. Pod can have multiple containers, but not the same kind. For example, a pod can have different containers like nginx container, redis container, postgres container. But a pod can not have the same type of containers like two nginx containers.</p> <p>Multi container pods If the pod has more than one container, then it is known as multi container pod. The containers within the pod can communicate each other through localhost as both containers are in the same network.</p> <p></p> <p>To create a pod <pre><code>kubectl run &lt;pod name&gt; --image=&lt;image name&gt;\n</code></pre> Example <pre><code>kubectl run nginx-pod --image=nginx\n</code></pre> When you run the above command, pod is created first and then pull the image from repository and run it as a container.</p> <p>Remember, <pre><code>When pod is running, external user cannot connect the application as there was no network connection established.\nYou can only connect the pod within the node in k8s cluster using `localhost`.\n</code></pre></p> <p>How to create a pod using YAML in k8s?</p> <p>Below are the mandatory top level/root level properties in any of k8s object such as pod, replicaset, deployment etc., <pre><code>apiVersion:\nkind: \nmetadata:\nSpec:\n</code></pre></p> <p>apiVersion &amp; kind:</p> Kind apiVersion Pod v1 Service v1 Replication Controller v1 ReplicaSet apps/v1 ReplicaSet apps/v1 Deployment apps/v1 Namespace v1 <p>metadata This is where we need to define the pod properties such as name, labels etc., Pod should have a name which is a mandatory field. You can identify the pod based on this field only. labels are list of dictionary which is optional field.</p> <p>spec This is where we need to define the container related properties such as name, image which is going to be used, how many  containers need to be run etc.,</p> <p>Pod definition file should have below</p> <pre><code>apiVersion: v1 -----------------&gt; Based on k8s object, you can update this\nkind: Pod ----------------------&gt; k8s object\nmetadata: ----------------------&gt; Pod properties need to be updated under this\n    name: my-pod ---------------&gt; Pod name (mandatory)\n    labels: --------------------&gt; used to group pods (optional)\n        apps: sats\nspec:  -------------------------&gt; container properties need to be updated under these\n    containers: \n    -   image: nginx -----------&gt; container image\n        name: nginx-container --&gt; container name\n</code></pre> <p>If it is multi container pod, then the spec needs to be updated. <pre><code>spec:  -------------------------&gt; container properties need to be updated under this\n    containers: \n    -   image: nginx -----------&gt; first container image\n        name: nginx-container --&gt; first container name\n    -   image: redis -----------&gt; second container image\n        name: redis-container --&gt; second container name        \n</code></pre> Example</p> <p></p> <p>To create a pod <pre><code>kubectl create -f &lt;pod definition YAML file&gt;\n</code></pre> </p> <p>To view the list of pods created</p> <p><pre><code>kubectl get pods\n</code></pre> </p> <p>The READY column in the output of the kubectl get pods command <pre><code>Running containers in the pod/Total containers in the pod\n</code></pre> To check the pod related details (pod starting time, which node it is running, what are the containers are running inside pods,  what are events associated with this pod etc.)</p> <p><pre><code>kubectl describe pods &lt;pod name&gt;\n</code></pre> </p> <p>To delete the pod</p> <p><pre><code>kubectl delete pods &lt;pod name&gt;\n</code></pre> </p> <p>If the container is up and running, and you want to make some changes, then you need to use the below command <pre><code>kubectl edit pods &lt;pod name&gt;\n</code></pre> Please note that only the properties listed below are editable. spec.containers[].image spec.initContainers[].image spec.activeDeadlineSeconds spec. tolerations spec.terminationGracePeriodSeconds</p> <p>To create the pod definition YAML file using imperative command <pre><code>kubectl run &lt;pod name&gt; --image=&lt;image name&gt; --dry-run=client -o yaml &gt; pod-definition.yaml\n</code></pre> If you are not given a pod definition file, you may extract the definition to a file for the pod which is running now using the below command: <pre><code>kubectl get pod &lt;pod name&gt; -o yaml &gt; pod-definition.yaml\n</code></pre></p>"},{"location":"k8s/basics/#replicasets","title":"ReplicaSets","text":"<p>Assume that our application is not running because the pod is crashing. So, our end users cannot access your application. To avoid such scenario, we need to run our application more than one pod. So, even one pod is failed, other pods can  support the users and make sure the application is highly available. Replication controller will help us to run the multiple  instances of a single pod in k8s cluster. This will ensure the high availability.</p> <p></p> <p>Even if you run your application with only one pod, replication controller will make sure your application always run even  your current pod crashes for some reason. RC will create a new pod automatically when the existing pod crashes. Basically, RC will run the number of pods specified in the replicas in RC definition file. Also, RC helps load balancing and scaling.</p> <p></p> <pre><code>apiVersion: v1 -----------------&gt; Based on k8s object, you can update this\nkind: ReplicationController ----------------------&gt; k8s object\nmetadata: ----------------------&gt; RC properties need to be updated under this\n    name: my-pod ---------------&gt; RC name (mandatory)\n    labels: --------------------&gt; used to group RCs (optional)\n        apps: sats\nspec:  -------------------------&gt; RC properties need to be updated under this\n    template: ------------------&gt; Under this, pod details need to be updated\n        metadata:\n            name: my-pod\n            labels:\n                name: my-pod-label\n        spec: -------------------------&gt; Pods specification which can be created \n            containers: \n            -   image: nginx -----------&gt; container image\n                name: nginx-container --&gt; container name\n    replicas: 4 ----------------&gt; Number of pods to be created\n</code></pre> <p></p> <p>To create an RC and view the list of RC created <pre><code>kubectl create -f &lt;RC definition YAML file&gt;\n\nkubectl get rc\n</code></pre>  When you create replica controller, the pod name is also start with replication controller name specified in YAML file,  but NOT with pod name specified in YAML file. </p> <p>Replica set and replica controller purposes are same. The only difference is <code>selector</code> field. For RS, it is mandatory field and for RC, it is optional. </p> <p><pre><code>apiVersion: v1 -----------------&gt; Based on k8s object, you can update this\nkind: ReplicaSet ----------------------&gt; k8s object\nmetadata: ----------------------&gt; RC properties need to be updated under this\n    name: my-pod ---------------&gt; RC name (mandatory)\n    labels: --------------------&gt; used to group RCs (optional)\n        apps: sats\nspec:  -------------------------&gt; RC properties need to be updated under this\n    template: ------------------&gt; Under this, pod details need to be updated\n        metadata:\n            name: my-pod\n            labels:\n                name: my-pod-label\n        spec: -------------------------&gt; Pods specification which can be created \n            containers: \n            -   image: nginx -----------&gt; container image\n                name: nginx-container --&gt; container name\n    replicas: 4 ----------------&gt; Number of pods to be created\n    selectors: -----------------&gt; To group the pods and monitor\n        matchLabels: -----------&gt; To select the pod using label\n            name: my-pod-label ---&gt; same label mentioned as pod label\n</code></pre> matchLabels should be same as pod label mentioned under template section.</p> <p></p> <p>To create an RS and view the list of RS created <pre><code>kubectl create -f &lt;RS definition YAML file&gt;\n\nkubectl get rs\n</code></pre>  When you create replica set, the pod name is also start with replication set name specified in YAML file (mentioned as \"my-replica-set\"),  but NOT with pod name specified in YAML file (mentioned as \"nginx\").</p> <p>Assume that, there are already 3 pods already running and their label is \"apps:sats\". Now, you are trying to create RS and this RS spec has matchLabels field is \"apps:sats\" and replica is \"3\". In that case, RS won't create any new pods because there are 3 pods  with same labels are already running. In case, if any one of the pod is failed due to some reason, then RS will create a new pod  and make sure the pod replicas are maintained.</p> <p></p> <p>In the above snapshot, pod is already running for 55s and then RS with replica \"1\" is created. But RS is not created any  pods (Age is 11s for RS).</p> <p>Scale</p> <p>To scale the pods, we can use any of the below commands <pre><code>kubectl replace -f &lt;RS definition YAML file&gt; =&gt; Before running the command, you need to update replicas field manually in YAML file\nkubectl scale --replicas=6 -f &lt;RS definition YAML file&gt; =&gt; This will scale the pods, but NOT update in the YAML file\nkubectl scale --replicas=6 replicaset &lt;replicaset name&gt;\nkubectl edit replicaset &lt;replicaset name&gt; =&gt; manually edit and save\n</code></pre></p>"},{"location":"k8s/basics/#deployments","title":"Deployments","text":"<p>Assume that there are 10 pods running  in k8s cluster with v1 version of image. You want to update the v2 version of image now. So, you need to down 1 pod at a time and replace it with new pod (v2 version). This process applicable until all the existing pods are replaced. This is called  rolling updates.</p> <p>Using deployment, we can do rolling updates, rollback, pause and resume after the changes in k8s cluster.</p> <pre><code>apiVersion: apps/v1 ------------&gt; Based on k8s object, you can update this\nkind: Deployment ---------------&gt; k8s object\nmetadata: ----------------------&gt; RC properties need to be updated under this\n    name: my-pod ---------------&gt; RC name (mandatory)\n    labels: --------------------&gt; used to group RCs (optional)\n        apps: sats\nspec:  -------------------------&gt; RC properties need to be updated under this\n    template: ------------------&gt; Under this, pod details need to be updated\n        metadata:\n            name: my-pod\n            labels:\n                name: my-pod-label\n        spec: -------------------------&gt; Pods specification which can be created \n            containers: \n            -   image: nginx -----------&gt; container image\n                name: nginx-container --&gt; container name\n    replicas: 4 ----------------&gt; Number of pods to be created\n    selectors: -----------------&gt; To group the pods and monitor\n        matchLabels: -----------&gt; To select the pod using label\n            name: my-pod-label ---&gt; same label mentioned as pod label\n</code></pre> <p>To create a deployment and view the list of deployment created <pre><code>kubectl create -f &lt;deployment definition YAML file&gt;\nkubectl create deployment &lt;deployment name&gt; --image=&lt;image name&gt; --replicas=&lt;number of pods&gt; --dry-run=client -o yaml &gt; &lt;dployement YAML file&gt;\nkubectl get deployment\n</code></pre></p> <p>Deployment creates replicaset and pods by default along with deployment.</p> <p></p> <p></p>"},{"location":"k8s/basics/#namespace","title":"Namespace","text":"<p>We are creating k8s objects such as pods, deployments, RC, RS and services inside the namespace. There are three namespaces created automatically by k8s. </p> <p><code>default</code> namespace is created automatically by k8s when the cluster is set up.</p> <p>K8s created a set of pods and services for internal use such as networking, DNS resolution etc., are put into another namespace known as <code>kube-system</code>. This namespace is not accessible to the user.</p> <p><code>kube-public</code> namespace is allowed to access by users.</p> <p>You can create your own namespace in the same cluster and create pods, services for each namespace. So, we can avoid using/modifying pods, services in another namespace. Each namespace have its own policy that define the role who can do what. Also, we can assign quota for each namespace. This way we can ensure that each namespace uses only allowed limit  and will not impact other namespace.</p> <p>Resources in the namespace refers each other using their name. For example, assume that there are two different services are in the default namespace named as db-service and web-service. If web-service pod wants to talk/call to db-service pod, then it simply calls db-service.</p> <p>Resources in one namespace wants to make a call to the resources in another namespace, then it should follow the below pattern</p> <p><code>&lt;service name&gt;.&lt;namespace&gt;.&lt;subdomain&gt;.&lt;domain&gt;</code> --&gt; domain is always cluster.local</p> <p>Remember, when the service is created, DNS is automatically created in this format.</p> <p>For example, assume that there are two different namespaces (dev, prod) and db-service in the dev namespace  and web-service in the prod namespace. If web-service pod wants to talk/call to db-service pod, then it needs to follow the patter</p> <p><code>db-service.dev.svc.cluster.local</code></p> <p>By default, kubectl command retrieves the data for default namespace.  If you want to view the details for another namespace</p> <pre><code>kubectl get pods -n\nor\nkubectl get pods --namespace=&lt;namespace name&gt;\n</code></pre> <p>To create pod in another namespace</p> <pre><code>kubectl create -f pod-definition.yaml --namespace=&lt;namepace name&gt;\nor\nkubectl create -f pod-definition.yaml -n=&lt;namespace name&gt;\n</code></pre> <p>If you want to create resources to any other namespace other than default namespace , you need to update below details in the  definition file.</p> <p></p> <p>In the above case, we don't need to pass --namespace in the command <pre><code>kubectl create -f pod-definition.yaml\n</code></pre></p> <p></p> <p>To create namespace, there are two ways.</p> <pre><code>kubectl create namespace &lt;namespace name&gt;\n\nor\n\nkubectl create -f &lt;namespace definition YAML file&gt;\n</code></pre> <p></p> <p>To view the current namespace</p> <p><pre><code>kubectl config get-contexts\n</code></pre> </p> <p>To view all namespaces</p> <p></p> <p>To change the namespace permanently</p> <p><pre><code>kubectl config set-context $(kubectl config current-context) --namespace=&lt;namespace name&gt;\n</code></pre> </p> <p>To create a resource quota for namespace, we need to create a resource quota definition file.</p> <p></p> <p></p> <p></p>"},{"location":"k8s/config/","title":"Configuration","text":""},{"location":"k8s/config/#command-vs-entrypoint-in-docker","title":"Command Vs Entrypoint in Docker","text":"<p>Entrypoint is nothing but the program that will be run when the container starts. Command is nothing but the arguments passed to entrypoint.</p> <p></p> <p>If the argument is missed while running the container, then we need to pick the default argument and run. To do so, we need to include both entrypoint and CMD in the dockerfile. </p> <p><code>Remember that both Entrypoint and CMD should be in JSON format =&gt; [\"&lt;VALUE or COMMAND&gt;\"]</code></p> <p></p> <p>If you want to replace entrypoint itself, then you need to run  <pre><code>docker run --entrypoint &lt;entrypoint command&gt; &lt;image&gt; &lt;argument&gt;\n</code></pre></p>"},{"location":"k8s/config/#command-vs-entrypoint-in-k8s","title":"Command Vs Entrypoint in K8s","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name:\n    labels:\n        apps: sats\nspec:\n    containers: \n    -   image: nginx\n        name: nginx-container\n        command: [\"sleep\", \"50\"] =&gt; Option 1\n        command:  =&gt; Option 2\n        - \"sleep\"\n        - \"50\"\n        command: [\"sleep\"] =&gt; Option 3\n        args: [\"50]\n</code></pre>"},{"location":"k8s/config/#environment-values","title":"Environment Values","text":"<p>Environment values are passed to pod in three different ways. 1. Via Pod definition 2. Via Config Maps 3. Via Secrets  <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name:\n    labels:\n        apps: sats\nspec:\n    containers: \n    -   image: nginx\n        name: nginx-container\n        env: =&gt; Option 1\n        - name: color\n          value: pink\n        env: =&gt; Option 2\n        - name: color\n          valueFrom: \n            configMapKeyRef:\n        env: =&gt; Option 3\n        - name: color\n          valueFrom: \n            secretKeyRef:\n</code></pre></p> <p>1. Via Pod definition All environment values are passed under <code>env</code> section as a key value pair.  <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name:\n    labels:\n        apps: sats\nspec:\n    containers: \n    -   image: nginx\n        name: nginx-container\n        env: \n        - name: color\n          value: pink\n</code></pre></p> <p>**2. Via Config Maps Suppose there are many environment values need to be passed in the pod definition file, then adding them under <code>env</code> section is not good idea. Instead, we can create a config map and use them in pod definition is a good approach.</p> <p>There are two steps.</p> <pre><code>1. Creating config map.\n2. Injecting the config map into pod definition file.\n</code></pre> <p>Creating Config Map 1. Imperative way</p> <p><pre><code>kubectl create configmap &lt;config map name&gt; --from-literal=&lt;key1&gt;=&lt;value1&gt; --from-literal=&lt;key2&gt;=&lt;value2&gt; \nor\nkubectl create configmap &lt;config map name&gt; --from-file=&lt;file path&gt;\n</code></pre> File should have all the environment values in key value format.</p> <ol> <li>Declarative way</li> </ol> <p>First create a data in a file <pre><code>name: color\nvalue: pink\n</code></pre> Second, create a config map YAML file. <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n    name:\ndata:\n    name: color\n    value: pink\n</code></pre> <pre><code>kubectl create -f &lt;config map YAML file&gt; \n</code></pre> Now, we need to inject the config map into pod.</p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name:\n    labels:\n        apps: sats\nspec:\n    containers: \n    -   image: nginx\n        name: nginx-container\n        envFrom:\n        - configMapKeyRef:\n            name: &lt;config map name created in the above step&gt;\n</code></pre> In case, we would like to pass only one or two environment values into the pod, then <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n    name:\n    labels:\n        apps: sats\nspec:\n    containers: \n    -   image: nginx\n        name: nginx-container\n        env:\n        - name: &lt;KEY NAME&gt;\n          valueFrom:\n            - configMapKeyRef:\n                name: &lt;config map name created in the above step&gt;\n                key: &lt;Key name&gt;\n</code></pre></p> <p>To View Config Map <pre><code>kubectl get configmaps \n</code></pre></p>"},{"location":"k8s/containerd/","title":"Docker vs Containerd","text":"<p>Earlier, k8s tightly coupled with docker and k8s supports only docker (container solution) even though other docker  solution such as rkt available in the market. Over the period, k8s wants to support other container solution and introduced an interface called Container runtime (CRI).. If any vendor adheres to OCI (Open Container Initiative) standards,  they can be worked as container runtime in k8s.</p> <p>OCI consists of an Image spec and runtime spec. Image spec defines how the image should be build and runtime spec defines how  any container runtime should be developed. RKT adheres OCI standards and now it is supported by k8s via CRI. However, docker  doesn't have CRI and k8s still need to support docker without CRI. Hence, k8s introduced dockershim to support docker without CRI. However, it is a temporary one.</p> <p></p> <p>Docker consists of multiple tools such as CLI, API, Auth, Volumes, Run C etc., Run C is called container run time and it  is managed by containerd. Containerd is CRI compatible and can work with k8s directly without dockershim. It is similar like other container runtime solution (RKT). Now k8s can support docker via containerd and no need to maintain dockershim  as containerd follows OCI standards. Hence, from k8s version 1.24 onwards, dockershim was removed and support for docker  is also removed as containerd is in place to support docker.</p>"},{"location":"k8s/containerd/#containerd","title":"Containerd","text":"<p>You can install containerd alone without docker if you don't need any other docker features. After installing containerd, there is CL tool called CTR is installed as part of containerd installation. This is unfriendly  tool and supports very limited features. It is used only for debugging purpose.</p> <p>NertCtl tool is another useful CL tool which supports almost all the docker features. While running the command, you just  need to replace docker.</p> <p><pre><code>nertctl run nginx\nnertctl pull nginx\netc.,\n</code></pre> CTR and NerdCTL are only for containerd.</p> <p>CRICTL is another CL tool which supports all the container runtime solution such as RKT. This is developed by k8s community. Again, it is only for debugging purposes. Commands are similar to docker commands except <code>crictl pods</code> which is not there in docker.</p> <pre><code>crictl run nginx\ncrictl pull nginx\netc.,\n</code></pre>"},{"location":"k8s/containerd/#summary","title":"Summary","text":""},{"location":"k8s/cron-job/","title":"Cron Jobs","text":"<p>Cron jobs are built on top of the jobs which runs the job periodically against on schedule.  Schedule is the field where you can specify when the job needs to run repeatedly.</p> <p></p> <p><code>cron-job.yaml</code></p> <p><pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox:1.28\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre> </p> <p>In case if you want to run the job in every two minutes on every day, then the schedule is</p> <p><code>schedule: \"*/2 * * * *\"</code></p> <p>Points to remember</p> <ol> <li>Cron job works on the time zone set on your k8s control plane</li> <li>Cron job controller checks every 10 seconds to verify if any job is missed</li> <li>If 100 jobs are missed by job controller (maybe due to downtime), then the controller won't try anymore     to run the cron job.     <code>startingDeadlineSeconds</code> field holds the value for this.    If we update this value as 10,    then the job controller will check every 10 seconds to see if 100 jobs are missed or not.    If the job is not started within 10 seconds, then the job is going to be killed.</li> </ol> <p><code>cron-job.yaml</code></p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  schedule: \"* * * * *\"\n  startingDeadlineSeconds: 10\n  concurrentPolicy: Allow\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: busybox:1.28\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - date; echo Hello from the Kubernetes cluster\n          restartPolicy: OnFailure\n</code></pre> <p></p>"},{"location":"k8s/cron-job/#concurrency-policy","title":"Concurrency Policy","text":"Options Description Allow If the previous job is still running, this will allow us to create the new jobs without disturbing the existing job Forbid If the previous job is still running, this will not allow us to create the new jobs until the existing job completes Replace If the previous job is still running, this will delete the existing job and replace them by creating a new job"},{"location":"k8s/deploy-strategy/","title":"Deployment Strategy","text":""},{"location":"k8s/deploy-strategy/#recreate-strategy","title":"Recreate Strategy","text":"<p>All pods are killed at once and recreated with new pods at once. For example, assume there are 4 pods are running the application with v1 features in the production environment. Now, we are ready with v2 features, and it needs to be moved to production. If we are following recreate strategy for deployment, then all existing 4 pods with v1 features will be destroyed together. Then, new pods with v2 features will be created together.</p> <p></p> <p><pre><code># Strategy should be updated as 'Recreate'. By default, it is RollingUpdate \nspec:\n  replicas: 3\n  strategy:\n    type: Recreate\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      name: nginx-pod\n  template:\n    metadata:\n      labels:\n        name: nginx-pod\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.14.2\n          ports:\n            - containerPort: 80\n</code></pre></p> <p></p> <p></p> <p>Now we are updating the image to nginx:latest from nginx:1.14.2</p> <p></p> <p>All pods are terminating at once and recreated at once. Pros</p> <p>When it comes to easy setup, Recreate is a good option.</p> <p>Application state entirely renewed.</p> <p>Cons</p> <p>High impact on the user, expect downtime that depends on both shutdown and boot duration of the application.</p>"},{"location":"k8s/deploy-strategy/#rolling-update-strategy","title":"Rolling Update Strategy","text":"<p>This is the by default strategy in kubernetes. Only one pod is killed and recreated with new pod and continue so until all the required pods are created. For example, assume there are 4 pods are running the application with v1 features in the production environment. Now, we are ready with v2 features, and it needs to be moved to production. If we are following rolling update strategy for deployment, then one pod with v1 features will be destroyed first. Then, one pod with v2 features will be created. Then it will continue until all 4 pods with v2 features are in place.</p> <p></p> <p></p> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxAvailable: 0\n  selector:\n    matchLabels:\n      name: nginx-pod\n  template:\n    metadata:\n      labels:\n        name: nginx-pod\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.14.2\n          ports:\n            - containerPort: 80\n</code></pre> </p> <p>Now we are updating the image to nginx:latest from nginx:1.14.2</p> <p></p> <p><code>Note: In Rolling Update, when maxSurge is mentioned, first new pod will create and once it is moved to running state, then the existing pod will be deleted.</code></p> <p>Pros</p> <p>Slow-release of version across instances.</p> <p>Useful for stateful applications that can handle the data.</p> <p>Cons</p> <p>In this strategy, rollout and rollback take time.</p> <p>There is no control over the traffic.</p>"},{"location":"k8s/deploy-strategy/#rolling-update-and-rolling-back","title":"Rolling Update and Rolling back","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\n\nspec:\n  replicas: 15\n  selector:\n    matchLabels:\n      app: redis-pod\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 0\n  minReadySeconds: 10 #After pod created, it will wait for 10 seconds and route the traffic\n  template:\n    metadata:\n      labels:\n        app: redis-pod\n    spec:\n      containers:\n        - name: redis-container\n          image: redis:6.0 # Upgrade from 6.0 to 5.0 --&gt; 6.0.16 --&gt; 6.2.6\n</code></pre> <p>Now we are updating the image to redis:6.0 from redis:5.0</p> <p></p> <p><code>NOTE: We are not seeing any revision change-cause because we are not recording any revision changes</code></p> <p>Now we are updating the image to redis:5.0 from redis:6.0.16 using CLI. This time we are recording the changes. </p> <p><code>NOTE: Now we are seeing revision change-cause because we are recording the changes</code></p> <p>Now we are updating the image to redis:6.0.16 from redis:6.2.6. Along with we made replica changes to 6 and maxSurge is as 2.</p> <p></p>"},{"location":"k8s/deploy-strategy/#rolling-back-to-particular-revision","title":"Rolling Back to particular revision","text":"<p>Now let us move revision 2 which is image changes to redis:5.0. Remember, in revision 2, the image is redis:5.0, replicas is 15. maxSurge is 4. But when we roll back the current revision 4 to revision 2, this will update only image. It CANNOT update replicas, maxSurge etc.,</p> <p></p> <p>If you look at the revision history, there is no revision 2 because it is updated as revision 5.</p>"},{"location":"k8s/deploy-strategy/#rolling-back-to-previous-revision","title":"Rolling Back to previous revision","text":"<p>Now let us move previous revision which is image changes to redis:6.2.6.  But when we roll back the current revision 5 to previous revision (which is 4), this will update only image. It CANNOT update replicas, maxSurge etc., After rolling back, the current version is 6 and there is no revision 4. Because revision 4 is updated to revision 6.</p> <p></p>"},{"location":"k8s/deploy-strategy/#canary-deployment-strategy","title":"Canary Deployment Strategy","text":""},{"location":"k8s/imperative-commands/","title":"Imperative Commands","text":""},{"location":"k8s/imperative-commands/#create-the-pods-with-below-configuration","title":"Create the pods with below configuration","text":"<p><pre><code>#### without label\napiVersion: v1\nkind: Pod\nmetadata:\n  name: no-label-demo\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n</code></pre> <pre><code>#### with one label\napiVersion: v1\nkind: Pod\nmetadata:\n  name: single-label-demo\n  labels:\n    app: nginx\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n</code></pre> <pre><code>#### with multiple labels\napiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-labels-demo\n  labels:\n    app: nginx\n    env: dev\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n</code></pre> </p> <p></p>"},{"location":"k8s/imperative-commands/#display-the-pod-label","title":"Display the pod label","text":"<p><code>kubectl get pods --show-labels</code></p> <p></p>"},{"location":"k8s/imperative-commands/#filtering-the-pods-based-on-the-label","title":"Filtering the pods based on the label","text":"<p><code>kubectl get pods -l &lt;key=value&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#adding-the-labels-to-the-pods","title":"Adding the labels to the pods","text":"<p><code>kubectl label pods &lt;podname&gt; &lt;key=value&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#overwrite-existing-labels-in-the-pods","title":"Overwrite existing labels in the pods","text":"<p><code>kubectl label pods &lt;podname&gt; &lt;key=value&gt; --overwrite</code></p> <p></p>"},{"location":"k8s/imperative-commands/#remove-the-labels-from-the-pods","title":"Remove the labels from the pods","text":"<p><code>kubectl label pods &lt;podname&gt; key-</code></p> <p></p>"},{"location":"k8s/imperative-commands/#destroy-the-pods-based-on-label","title":"Destroy the pods based on label","text":"<p><code>kubectl delete pods -l &lt;key=value&gt;</code></p> <p> <code>Note: When deleting the pod based on the label, it will delete all the pods if the label exists in the pod configuration.</code></p> <p>For example, Assume the pod contains 3 labels.Out of 3 labels, only 1 label matches while executing delete command,   then this pod will be deleted. </p>"},{"location":"k8s/imperative-commands/#label-selector-highlevel","title":"Label selector - Highlevel","text":""},{"location":"k8s/imperative-commands/#create-deployment","title":"Create Deployment","text":"<p><code>kubectl create deployment &lt;deployment name&gt; --image=&lt;image name&gt;</code></p> <p><code>kubectl create deployment &lt;deployment name&gt; --image=&lt;image name&gt; --replicas=&lt;number of pods&gt;</code></p> <p></p> <p>Below two commands will not create deployment. Instead, it will evaluate if this command can create deployment. If it creates, what would be the yaml file content <code>kubectl create deployment &lt;deployment name&gt; --image=&lt;image name&gt; --replicas=&lt;number of pods&gt; --dry-run=client</code></p> <p><code>kubectl create deployment &lt;deployment name&gt; --image=&lt;image name&gt; --replicas=&lt;number of pods&gt; --dry-run=client -o yaml</code></p> <p></p>"},{"location":"k8s/imperative-commands/#change-the-image-in-the-existing-deployment","title":"Change the image in the existing deployment","text":"<p><code>kubectl set image deployment &lt;deployment name&gt; &lt;container-name&gt;&gt;=&lt;image name&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#rolling-status-for-a-deployment-and-rolling-history-for-a-deployment","title":"Rolling Status for a deployment and rolling history for a deployment","text":"<p><code>kubectl rollout status deployment &lt;deployment name&gt;</code></p> <p><code>kubectl rollout history deployment &lt;deployment name&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#rolling-back-to-previous-deployment","title":"Rolling Back  to previous deployment","text":"<p><code>kubectl rollout undo deployment &lt;deployment name&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#rolling-back-to-particular-revision","title":"Rolling Back to particular revision","text":"<p><code>kubectl rollout undo deployment &lt;deployment name&gt; --to-revision=&lt;revision number&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#scaling-the-deployment","title":"Scaling the deployment","text":"<p><code>kubectl scale deployment &lt;deployment name&gt; --replicas=&lt;number of pods&gt;</code></p> <p></p>"},{"location":"k8s/imperative-commands/#autoscaling-the-deployment-based-on-cpu-usage","title":"Autoscaling the deployment based on CPU usage","text":"<p><code>kubectl autoscale deployment &lt;deployment name&gt; --min=2 --max=6 --cpu-percent=&lt;percentage number&gt;</code></p> <p></p>"},{"location":"k8s/jobs/","title":"Jobs","text":"<p>Jobs runs set number of pods to completion. Jobs are managed by job controller,  which is your kubernetes controller that observes things from control plane and makes sure everything runs as expected.</p> <p><code>Remember you cannot run the pods in parallel. But you can run the pods in parallel using jobs  (you can define how many pods can run parallel).</code></p> <p></p>"},{"location":"k8s/jobs/#jobs-template","title":"Jobs template","text":"<p>Job is responsible to create the pods and monitor them. Pods are responsible for creating the container and ensure that it is running. Container is responsible for running the application.</p> <p>In short, Job creates the pod and watches them to successful completion and cleans them up and then a job completes.</p> <p><code>job.yaml</code></p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-example\nspec:\n  template:\n    spec:\n      containers:\n      - name: job-container\n        image: alpine:latest\n        command: [\"sh\",  \"-c\", 'echo \"I am learning k8s\"']\n      restartPolicy: Never\n  backoffLimit: 4\n</code></pre>"},{"location":"k8s/jobs/#restart-policy","title":"Restart Policy","text":"<p>There are three possible values 1. Always : The pod will be restarted automatically after the container is terminated. This is the default value. 2. Never : The pod will not be restarted automatically after the container is terminated. 3. OnFailure: The pod will not be restarted automatically only if the container is failed (terminated with non-zero exit code)</p> <p><code>Remember that if you want to troubleshoot, you need a logs and if you need a logs, you should not restart the pod even if the container is failed So, keep restart policy is Never in case of troubleshooting</code></p>"},{"location":"k8s/jobs/#number-of-pods-to-be-run","title":"Number of Pods to be run","text":"<p>By default, the job will create only one pod, and the job ends once the pod completes the task it is intended to be.</p> <p></p> <p>In case, if you want to run five pods, then you need to update the job manifest file with <code>completions</code>. However, the pods will be created one after one. It means the first pod will create and execute successfully. Then only, second pod will create and so on.</p> <p><code>job.yaml</code></p> <p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-example\nspec:\n  completions: 5\n  template:\n    spec:\n      containers:\n      - name: job-container\n        image: alpine:latest\n        command: [\"sh\",  \"-c\", 'echo \"I am learning k8s\"']\n      restartPolicy: Never\n</code></pre> </p>"},{"location":"k8s/jobs/#parallel-execution","title":"Parallel Execution","text":"<p>In case, if you want to run five pods and three pods should run in parallel, then you need to update the job manifest file  with <code>completions</code> and <code>parallelism</code>. This will create three pods immediately and execute them successfully. Once any of the pods are completed successfully, the next pod will be created immediately and job controller ensures that three pods are running in parallel.</p> <p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-example\nspec:\n  completions: 5\n  parallelism: 3\n  template:\n    spec:\n      containers:\n      - name: job-container\n        image: alpine:latest\n        command: [\"sh\",  \"-c\", 'echo \"I am learning k8s\"']\n      restartPolicy: Never\n</code></pre> </p> <p><code>Remember that completions and parallelism default value is 1</code></p>"},{"location":"k8s/jobs/#number-of-retries","title":"Number of Retries","text":"<p>Let us assume that your pod is failing for some reason. You want to retry to run the job for a specified number of times. To do this, include <code>backofflimit</code> in the manifest file.</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-example\nspec:\n  backoffLimit: 4\n  template:\n    spec:\n      containers:\n      - name: job-container\n        image: busybox\n        args: [\"/bin/cat\",  \"/etc/os\"]\n      restartPolicy: Never\n</code></pre> <p></p> <p><code>Remember that backoffLimit always expecting sequential failure. For example, in the above case, 4 consecutive failures  make the job as failed</code></p>"},{"location":"k8s/jobs/#terminate-the-pod-after-x-seconds","title":"Terminate the pod after x seconds","text":"<p>Let us assume that your pod is running for a long time. You want to terminate the pod after sometime. To do this, include <code>activeDeadlineSeconds</code> in the manifest file.</p> <p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-example\nspec:\n  activeDeadlineSeconds: 10\n  template:\n    spec:\n      containers:\n      - name: job-container\n        image: alpine:latest\n        command: [\"sh\",  \"-c\", 'echo \"I am learning k8s\" &amp;&amp; sleep 60']\n      restartPolicy: Never\n</code></pre> </p> <p>In the above snapshot, pod is terminated after 10 seconds as <code>activeDeadlineSeconds</code> is defined as 10.</p>"},{"location":"k8s/k8-arch/","title":"Architecture","text":""},{"location":"k8s/k8-arch/#node","title":"Node","text":"<p>It is a physical or virtual machine where kubernetes installed. It is a worker machine and this is where containers are launched by kubernetes. It is also called as minions.</p> <p> Source: Kodekloud</p> <p>In case if you have only one node in your infrastructure and for some reason, this node  gets failed. This will lead to your application goes down. To avoid this, you need more than  one node in your infrastructure.</p>"},{"location":"k8s/k8-arch/#cluster","title":"Cluster","text":"<p>It is a set of nodes grouped together. In case if one node is failed for some reason, you still  have other nodes to serve the request. Hence, your application never goes down. This will also help to share the load between the nodes.</p> <p> Source: Kodekloud</p> <p>Now the challenges are </p> <ol> <li>Who can manage the cluster?</li> <li>Where the members of the cluster information store?</li> <li>How to monitor the nodes?</li> <li>If one node fails, how to change the workload to another node? Who will do this?</li> </ol> <p>To answer all the above questions, Master is the one who will do all these activities.</p>"},{"location":"k8s/k8-arch/#master","title":"Master","text":"<p>Master is also a node where kubernetes installed and configured as Master. This will watch all the nodes in the cluster and responsible for actual orchestration of containers on the worker nodes.</p>"},{"location":"k8s/k8-arch/#k8s-components","title":"K8s Components","text":"<p>When you are installing the kubernetes, it is automatically installed the following components</p> <p> Source: Kodekloud</p>"},{"location":"k8s/k8-arch/#api-server","title":"API Server","text":"<p>It acts as front-end for the kubernetes. The users, management devices, command line interfaces will talk to API servers  to interact with kubernetes.</p>"},{"location":"k8s/k8-arch/#etcd","title":"Etcd","text":"<p>It is distributed and reliable key-value store used by kubernetes to store all the data to manage the cluster. When you have multiple nodes, multiple masters in the cluster, all their information are stored in key-value format in a  distributed manner. It is responsible for implementing the locks within the cluster to ensure that there are no conflicts  between the masters.</p>"},{"location":"k8s/k8-arch/#scheduler","title":"Scheduler","text":"<p>It is responsible to distribute work or containers across the multiple nodes. It searches for the newly created containers and assign them to the nodes.</p>"},{"location":"k8s/k8-arch/#controller","title":"Controller","text":"<p>It is the brain behind the orchestration. They are the one to notice and respond if the containers, nodes goes down. Controller will take the decision to bring up the new containers/nodes/end points in such cases.</p>"},{"location":"k8s/k8-arch/#container-runtime","title":"Container Runtime","text":"<p>This is the underlying software that is used to run the container. It could be docker, rkt, crio.</p>"},{"location":"k8s/k8-arch/#kubelet","title":"Kubelet","text":"<p>It is the agent that runs on each node in the cluster. It is responsible to ensure the containers are running as expected on the nodes.</p>"},{"location":"k8s/k8-arch/#how-to-serve-as-master-or-worker-node","title":"How to serve as master or worker node","text":"<p>Worker node hosts the container. You need container runtime installed to run the containers. Kubelet agents are installed  in worker node and responsible for interacting with master node. Kubelet will share the information with master node about  health of worker node and also executed the instructions provided by master node.</p> <p>Master node has API server. All the information shared by kubelet agent will be stored in key-value format in Etcd. Master also have controller and scheduler.</p> <p> Source: Kodekloud</p>"},{"location":"k8s/k8-arch/#kubectl","title":"kubectl","text":"<p>It is a command line utility and generally called kube command line tool or kube control. It is used to deploy and manage the application in kubernetes cluster.</p> <p><pre><code>kubectl run &lt;application_name&gt;\n</code></pre> The above command is used to deploy an application in k8s cluster. Example: <pre><code>kubectl run hello-minikube\n</code></pre> The below command is used to view the cluster information. <pre><code>kubectl cluster-info\n</code></pre> The below command is used to list all the nodes part of the cluster. <pre><code>kubectl get nodes\n</code></pre></p>"},{"location":"k8s/multi-containers/","title":"Multi Containers","text":"<p>As you know, the simplest pod model is one container inside in the pod. </p> <p></p> <p>However, a pod can host multiple containers inside it as well. This is known as multi container pod.</p> <p></p> <p>In the real world, multi container is used to separate the work between containers. For example, a pod has two containers inside it,  and one pod can perform related to application features,  and another pod can perform connecting the environment based on the connection strings. This is called as separation of concerns which means every container does only one thing.</p> <p></p>"},{"location":"k8s/multi-containers/#generic-patterns","title":"Generic Patterns","text":"<p>Multi containers have three different patterns.</p> <ol> <li>Ambassador</li> <li>Adapter</li> <li>Init Container</li> </ol>"},{"location":"k8s/multi-containers/#ambassador-pattern","title":"Ambassador Pattern","text":"<p>This is the generic multi container pattern. The containers inside in the pod are running together shoulder to shoulder. When the pod dies, both containers also die. They can share the same network, which means they can refer to each other as localhost,  and they have access to the same storage volume.</p> <p></p>"},{"location":"k8s/multi-containers/#adapter-pattern","title":"Adapter Pattern","text":"<p>The containers inside in the pod are running alongside together. App container storing the logs in the storage in a specific form. Adapter container reads the logs and converts them to expected form and ships it to a centralized database.</p> <p></p> <p><code>Remember that both patterns are generic sidecar models and there is no specific change in YAML file.</code></p>"},{"location":"k8s/multi-containers/#init-container","title":"Init Container","text":"<p>Init containers always starts first and completes its task. Until it gets completed, the app container will not start. If there are many init containers in the pod, then they will execute one at a time in sequential order which they defined in the YAML file. Until all the init containers complete successfully, the main container will wait.</p> <p>Init container needs to be specified in the YAML file. </p> <p><code>Remember init container runs only once.</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  containers:\n  - name: main-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 30']\n  initContainers:\n  - name: init-container\n    image: busybox:1.28\n    command: ['sh', '-c', \"sleep 60\"]\n</code></pre> <p></p> <p>In the above snapshot, the main app container starts after 60 seconds. Until then, the main app container is in pending status.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\nspec:\n  containers:\n  \u2014 name: main-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: first-init-container\n    image: busybox:1.28\n    command: ['sh,' '-c,' 'echo First init container is running! &amp;&amp; sleep 10']\n  - name: second-init-container\n    image: busybox:1.28\n    command: ['sh,' '-c,' 'echo Second init container is running! &amp;&amp; sleep 20']\n</code></pre> <p></p> <p>In the above snapshot, init containers are executed sequentially in the order which they defined in the YAML file.</p>"},{"location":"k8s/network-policy/","title":"Network Policy","text":"<p>Network policy is nothing but sets of rules defining which pod can to talk to which pod. To understand this, we need to know what is traffic, ingress and egress.</p> Term Description Traffic Any request coming to the system and going out of the system is called as traffic. Ingress Any request coming to the system is called as ingress. Egress Any request going out of the system is called as egress. <p><code>Remember that ingress and egress are described based on the call originated. It means that if pod A talks to pod B, then ingress and egress should considered for pod A because pod A is the source.</code></p> <p></p> <p>In the above snapshot, if we are defining the network policy for webserver pod 1, the request goes to webserver pod 2 from webserver pod 1 and the request comes to webserver pod 1 from webserver pod 3. While defining the network policy for webserver pod 1, egress to be considered for webserver pod 2 and ingress to be considered for webserver pod 3.</p> <p><code>Remember that there is no deny policy can be set to any pods. We can set only allow policy in k8s.</code></p>"},{"location":"k8s/network-policy/#network-security","title":"Network Security","text":"<p>In kubernetes, by default network policy is <code>all allow</code>. It means any pod can talk to any pod, any service in the cluster. </p> <p></p> <p>Let us assume that there are three pods (Web, API ad DB) running in the node. Web pod needs to talk only to API pod. API pod needs to talk only to DB. There should not be any communication between Web and DB pods. In other words, DB pod should accept traffic only from API.</p> <p></p> <p>In this case, we need to create a network policy and attach it to DB Pod. So, we need to create the network policy in DB pod's perspective. In this network policy, DB can accept ingress traffic but not egress traffic. So, we can add the pod selector which selects only API pod (Based on the label mentioned in API Pod) in the ingress section.</p> <p></p> <p>Only one rule is implemented in the below manifest file. </p> <p><code>netpol.yaml</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-net-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api # This label is mentioned in API pod\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <p><code>Remember that network policy is implemented by network solution (Ex: Calico, Kube-router).  All network solution will not support network policy (Ex: Flannel).</code></p>"},{"location":"k8s/network-policy/#developing-network-policy","title":"Developing Network Policy","text":"<ol> <li>Identify the pod in which we are implementing the network policy</li> <li>Then find out the pod's labels and include them in the pod selector section.</li> <li>Identify the type (ingress/egress) of network policy needs to be implemented in the selected pod.</li> </ol> <p><code>Remember that once you allow ingress traffic to the pod B from pod A,  then it automatically allows egress as well from pod B to pod A</code></p> <ol> <li>Let us assume the type of network policy is ingress, then you need to find the pod where the call starts.</li> <li>In any ingress type, there must be two sections (from and ports).  Under from, we need to include the pod's label from where the call started</li> <li>Under the ports section, we need to include the port in the identified pod for network policy.</li> <li>In any egress type, there must be two sections (to and ports).    Under to, we need to include the pod's label from where the call is going to.</li> <li>Under the ports section, we need to include the port of the pod where the call is going to.</li> </ol> <p>Let us assume that the pod selector label is available in the pods in different namespace like below.</p> <p></p> <p>Then, this network policy is applicable to all the namespaces. It means the pod which has label <code>role:api</code> from any namespace can talk to db pod in the above manifest file. So, if we want to restrict within a specific namespace, then we need to include namespace selector as well.</p> <p><code>netpol.yaml</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-net-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api # This label is mentioned in API pod\n      namespaceSelector:\n        matchLabels:\n          name: prod \n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <p>In the above manifest file, there are two ingress rules (pod selector and namespace selector) and the pod will be selected  if both rules are satisfied. This is similar to <code>AND</code> in if condition.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-net-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api # This label is mentioned in API pod\n    - namespaceSelector:\n        matchLabels:\n          name: prod \n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <p>In the above manifest file, there are two ingress rules (pod selector and namespace selector) and the pod will be selected  if any one of the rules is satisfied because there is <code>-</code> before the selectors (pod &amp; namespace). This is similar to <code>OR</code> in if condition.</p> <p>Let us assume that there are some servers which are not part of the cluster and still want to talk to DB pod for running backup activities. If that is the case, we need to include the <code>ipBlock</code> range in the ingress rule.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-net-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api # This label is mentioned in API pod\n      namespaceSelector:\n        matchLabels:\n          name: prod \n    - ipBlock:\n        cidr: 10.9.198.2/32\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre> <p>In the above manifest file, there are three ingress rules (pod selector and namespace selector and ipBlock). In the first rule, there are two conditions and both conditions must meet to select the pod. In the second rule, if any node from the ip block range, then it can connect to the DB pod. </p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: db-net-policy\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: api # This label is mentioned in API pod\n      namespaceSelector:\n        matchLabels:\n          name: prod\n    ports:\n    - protocol: TCP\n      port: 3306\n\n  - from:\n    - podSelector:\n        matchLabels:\n          role: web # This label is mentioned in API pod\n      namespaceSelector:\n        matchLabels:\n          name: prod\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre> Two different pods can talk to db pod based on the above ingress rule.</p>"},{"location":"k8s/observability/","title":"Observability","text":"<p>~~#Observability~~</p>"},{"location":"k8s/observability/#readiness-probe","title":"Readiness Probe","text":"<p>Pod Status</p> <p>Pending:Pod has a status in its lifecycle. When a pod is created first, it is in pending state. When the pod is in pending state,  scheduler check and find out where to place the pod (Which node). In case, if the scheduler is unable to find the node to place the pod then the pod will be in pending state only until scheduler finds the node. If you want to find out the reason why the pod  is in pending state, run the below command <pre><code>kubectl describe pod &lt;pod name&gt;\n</code></pre> Container Creating:When the scheduler finds the node to place the pod, the pod status moves to container creating  status. This is where the image has been pulled and the container starts. </p> <p>Running:Once the container starts, it goes to running state. In case of multi-container pod, once all the containers start, then only it moves to running state. It will continue to be in running state until the program inside container runs.</p> <p>Terminated:Once the program completes successfully, it moves to terminated state.</p> <p>To check the pod status, run the below command and check the status column <pre><code>kubectl get pods\n</code></pre></p> <p></p> <p><code>Remember the above command gives only high level information</code></p> <p>Pod Conditions</p> <p>There are 4 different conditions. 1. Pod Scheduled</p> <pre><code>`When the pod is scheduled on a node, this condition is set to TRUE`\n</code></pre> <ol> <li> <p>Initialised</p> <p><code>When the pod is initialised, this condition is set to TRUE</code> 3. Containers Ready</p> <p><code>When the container is ready, this condition is set to TRUE. In case of multi-container, once all the containers are ready, then the condition is set to TRUE</code> 4. Ready</p> <p><code>Finally the pod considers that it is ready</code></p> </li> </ol> <p>To check the pod condition, run the below command and check the condition section <pre><code>kubectl describe pod &lt;pod name&gt;\n</code></pre> </p> <p>Once the pod is in ready condition, it is ready to accept the user traffic. However, the application running inside the container may or may not be ready to serve the user request (Sometimes server takes few minutes to warmup and during this time, if we run  kubectl get pods command, the status show the pod is ready which is actually not yet ready). </p> <p><code>By default, k8s assumes that the pod is ready to server user request when it moves to ready state</code></p> <p>A user access the application running inside the container through service. Service will send the user traffic to the pod  once the pod is in ready status. But in the above case, the pod status is ready (But the server is still warming up and not ready) and service start to sending the user request to pod, and it leads to request failure. To avoid this, we need to make sure when the pod is in ready status, it is actually ready.</p> <p>We need to check the application ready status via configuring probe. If it is web server, we can hit the API to make sure we are getting  response. If it is DB server, then we make sure TCP socket is listening the port.</p> <pre><code>apiVersion: v1 --------------------&gt; Based on k8s object, you can update this\nkind: Pod -------------------------&gt; k8s object\nmetadata: -------------------------&gt; Pod properties need to be updated under this\n    name: my-pod ------------------&gt; Pod name (mandatory)\n    labels: -----------------------&gt; used to group pods (optional)\n        apps: sats\nspec:  ----------------------------&gt; container properties need to be updated under these\n    containers: \n    -   image: nginx --------------&gt; container image\n        name: nginx-container -----&gt; container name\n        readinessProbe:------------&gt; k8s check the ready status against it\n            httpGet: --------------&gt; HTTP option to check the probe (Option 1)\n                path: /api/v1/ready -&gt; API to check if the application is ready to accept the traffic\n                port: 8080 --------&gt; to hit API via this port\n            initialDelayResponse: 10 ---&gt; It will wait 10 seconds after the container starts (in ready status)\n            periodSeconds: 20  ----&gt; Every 20 seconds, probe test happens until it gets positive response\n            failureThreshold: 100 -&gt; It will attempt probe test for minimum 100 times until it gets positive response.\n            tcpSocket: ------------&gt; (Option 2) for DB server\n                port: 3303 --------&gt; to hit DB server via this port\n            exec: -----------------&gt; (Option 3) for simple script\n                command: ----------&gt; Command to execute the script\n                - cat\n                - file.txt\n</code></pre> <p><code>Remember that by default, probe test can happen only 3 times. If you want to change that, you need to include failureThreshold: &lt;number&gt;</code></p> <p>Once the pod is in ready state, k8s will not immediately route the traffic. Instead, it performs a check to hit API mentioned, and see if it gets positive response. If it gets positive response, then it will redirect the user traffic. If not, until it  gets positive response, it will not route the traffic.</p>"},{"location":"k8s/observability/#liveness-probe","title":"Liveness Probe","text":""},{"location":"k8s/persistent-volume/","title":"Docker Storage","text":"<p>When you install the docker in the system, it is creating the directory <code>/var/lib/docker</code> and this is where docker stores all the data (containers, images information) by default.</p> <pre><code>/var/lib/docker\n\u2502\u2500\u2500 aufs\n\u2502\u2500\u2500 images\n\u2502\u2500\u2500 containers\n\u2502\u2500\u2500 volumes\n</code></pre> <p>Docker builds the image in a layered architecture using storage drivers (Aufs, device mapper, overlay, etc.). It means each line in Docker file creates the layer.  Let's assume your docker file contains five lines. Then, your image should have five layers and each layer will have changes from the previous layer. Finally, the image is built and it is a read-only mode. You cannot change any data within the image. If you want to change any data in the image, you need to make the change in the docker file and rebuild the image.</p> <p>When you create a container using the image built, container layer will create on top of all the layers. This container layer is read-write mode.</p> <p></p> <p>However, when the container dies, whatever data it creates will also die. To avoid this, we create a volume and attach it to the container. </p> <p></p> <p>In case if the volume is not created, but attached to the container. Then, volume will be automatically created.</p> <p><code>Remember volumes are not handled by storage drives. It is handled by volume driver plugins.</code></p>"},{"location":"k8s/persistent-volume/#volumes-in-k8s","title":"Volumes in K8s","text":"<p>Volumes in k8s will be used to store the data permanently. In the below example, a random number is generated by the application running in a container and wrote the generated number in the file inside the container (/opt/number.out). This container is attached to the volume (data-volume) and mounted the path /opt to the volume. It means whatever data is stored in /opt path will be written in the data-volume as well. This volume (data-volume) is storing all the data in the /data directory in the host machine. Once this pod dies, data in the path /opt also dies. However, the data is still available in the path /data in data-volume.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-example-linux\nspec:\n  containers:\n  - name: example-container\n    image: alpine\n    command: \n    - sh\n    - -c\n    args: [\"shuf -i 0-100 -n 1 &gt;&gt; /opt/number.out;\"]\n    volumeMounts:\n    - mountPath: /opt # directory location on container\n      name: data-volume\n  volumes:\n  - name: data-volume\n    hostPath:\n      path: /data # directory location on host\n      type: Directory # this field is optional\n</code></pre> <p></p>"},{"location":"k8s/persistent-volume/#persistent-volume","title":"Persistent Volume","text":"<p>As volumes are available in k8s, why do we need persistent volume?</p> <p>To answer this question, we need to think about the real world production environment where too many pods are running. If we start configuring the volume for each pod, then it is a tedious process and maintaining them is challenging. To make it simple, persistent volume comes to the picture. It is a cluster wide pool of storage volumes configured by admin. Now user can deploy the application in the cluster and use the volumes in the pool using claims.</p> <p></p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0003\nspec:\n  volumeMode: Filesystem\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/opt\"\n</code></pre> <p><code>Remember that if your cluster is on prem, then storage also should be in on prem. If cluster is on cloud, then storage also in cloud</code></p> <p></p> <p>Storage drivers and plugin are the one that connects the cluster and storage shown in the above snapshot.</p> <p>Overall end to end flow is below</p> <p></p>"},{"location":"k8s/persistent-volume/#persistent-volume-claims","title":"Persistent Volume Claims","text":"<p>In kubernetes, PV and PVC are different objects. PV is created by admin and PVC is created by user to use the storage.</p> <p>After PVC is created, PV is automatically bound with PVC based on the properties set on the volume. It is a one-to-one relationship. It means one PV is bound with one PVC. Kubernetes will find the right PV that has enough capacity requested by PVC. Once it is found, PV and PVC will bind each other,  and after that no other PVC can claim the same PV even if there is enough space left over.</p> <p></p> <p>Let us assume it, there are 3 PV available in the cluster. You are creating a PVC now. So K8s will try to find the best fit for the PVC based on its properties. In case of multiple matches, then K8s will find PV based on the labels mentioned in PVC manifest file. If labels are not mentioned in the PVC manifest file, then K8s will find any one of those 3 PV.</p> <p></p> <p>If there is no PV created or not available, then PVC will remain in the pending status until newer PV is available. </p> <p>Let us see an example.</p> <p><code>pv.yaml</code></p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  persistentVolumeReclaimPolicy: Retain\n  accessModes:\n    - ReadWriteMany\n  capacity:\n    storage: 100Mi\n  hostPath:\n    path: /pv/log\n</code></pre> <p><pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: claim-log-1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Mi\n</code></pre> When we create both resources in k8s cluster, pvc is not bound with pv. The reason for not bounding is the access mode difference between pv and pvc.</p> <p></p> <p>Now, if we update access mode to <code>ReadWriteMany</code>  in pvc, then it will bound with pv.</p> <p></p> <p>Let us create a pod to use the pvc.</p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: webapp\nspec:\n  containers:\n  - name: event-simulator\n    image: kodekloud/event-simulator\n    env:\n    - name: LOG_HANDLERS\n      value: file\n    volumeMounts:\n    - mountPath: /log\n      name: my-pv\n\n  volumes:\n  - name: my-pv\n    persistentVolumeClaim:\n        claimName: my-pvc\n</code></pre> The Overall relations between pv, pvc and pod are shown in below</p> <p></p> <p></p> <p>If we delete pvc, it will not delete it as it was used by pod.</p> <p></p> <p>If you delete the pod, then pvc will be deleted.</p> <p></p> <p>However, pv is still available as the claim policy is mentioned as <code>Retain</code>. </p> <p></p>"},{"location":"k8s/persistent-volume/#volume-claim-policy","title":"Volume Claim Policy","text":"Claim Policy Description Retain PV is not getting deleted. But PV cannot be used for other claims Delete PV will be deleted once PVC is deleted Recycle PV is not getting deleted. But PV can be used for other claims"},{"location":"k8s/persistent-volume/#storage-class","title":"Storage Class","text":"<p>So far, whenever we need storage, we are creating the persistent volume manually and then use it in persistent volume claim. This is known as static provisioning. Before creating pv, we need to create a disk in the host machine or in the cloud.</p> <p>However, it can be done during runtime as well. To do so, we need storage class. Storage class will create pv automatically when we are running the application which requires storage. After creating pv, it will attach it with pvc when the claim is made. This is known as dynamic provisioning.</p> <p></p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: my-sc\n    provisioner: kubernetes.io/no-provisioner\nreclaimPolicy: Retain \nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p><code>volumeBindingMode</code> has two options. 1. WaitForFirstConsumer\u2014This will wait for pod being created and consumed the pv via claims 2. Immediate\u2014This will immediately create pv and will not wait for pod to be created.</p>"},{"location":"k8s/requests-limits/","title":"Request Requirements","text":"<p>When we create the pod in the cluster, kube-schedule decides where (in the node) the pod should be placed based on the resource availability in the node and how much resources need for pod. If the node doesn't have enough resources available, then the pod will not be placed in the node and the pod will be in pending status.</p>"},{"location":"k8s/requests-limits/#requests-estimated-resources-needed","title":"Requests (Estimated Resources Needed)","text":"<p>This is nothing but how much resources (CPU and memory) needed for a pod to run well. This is a guaranteed amount of resources for a pod. This is just an educated guess, and we cannot exactly tell how much resources it required. But node should have more resources than what we are requesting for pod.</p>"},{"location":"k8s/requests-limits/#limits-maximum-resources-needed","title":"Limits (Maximum Resources Needed)","text":"<p>This is nothing but how much resources (CPU and memory) go in case of more resources required for a pod to run well. This is a maximum amount of resources allocated for a pod.</p> <p><code>request.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: frontend\nspec:\n  containers:\n  - name: app\n    image: images.my-company.example/app:v4\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre> <p>1 cpu means 1 virtual cpu core. The value is from 0.1 to go any number. <code>0.1</code> means 100m (100 milli).</p> <p></p> <p>Similarly, memory values are below</p> <p></p> <p>If the pod goes beyond the limit mentioned, then the pods will be crashed.</p> <p></p> <p><code>Remember that limits and requests are tied with pods (container).</code></p>"},{"location":"k8s/requests-limits/#limit-range","title":"Limit Range","text":"<p>This kubernetes object will ensure that the pod will get minimum resources by default in the specific namespace.</p> <p><code>limit-range.yaml</code></p> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: cpu-resource-constraint\nspec:\n  limits:\n  - default: # this section defines default limits\n      cpu: 500m\n    defaultRequest: # this section defines default requests\n      cpu: 500m\n    max: # max define the limit range\n      cpu: \"1\"\n    min: # max define the request \n      cpu: 100m\n    type: Container\n</code></pre>"},{"location":"k8s/requests-limits/#resource-quota","title":"Resource Quota","text":"<p>Resource quota sets the limits and requests for namespace level. You can decide how much resources required for a namespace and how many pods can run in the namespace, etc.</p> <p></p> <p><code>resource-quota.yaml</code></p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: mem-cpu-demo\nspec:\n  hard:\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n    pods: \"10\"\n</code></pre>"},{"location":"k8s/security-context/","title":"Security Context","text":"<p>Security contexts provide additional privileges and access control settings. You can configure the security context settings either the container level or the pod level. If you configure it in the pod level, then the security context setting is applicable to all the containers inside the pod. If you configure it in the container level, then the security context setting is applicable only to the container. In case if you configure it in both the pod and container level, then the security context setting is set in the container level has the high priority than the pod level.</p>"},{"location":"k8s/security-context/#security-context-options","title":"Security Context Options","text":"<ol> <li>UID/GUID setting - We can run the container or pod with a specific user id or group id</li> <li>Privilege escalation - We can ensure that the container or pod is working as a root user or non-root user.</li> <li>Filesystem setting - We can ensure that the container or pod has access to a certain file system.</li> <li>Linux Capabilities - We can either add or drop linux capabilities to the pod or container.</li> </ol> <p><code>security-context.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-pod\nspec:\n  securityContext:\n    runAsUser: 1001\n    runAsGroup: 1005\n    fsGroup: 2000\n  containers:\n  -  image: ubuntu\n     name: web\n     command: [\"sleep\", \"5000\"]\n     securityContext:\n      runAsUser: 1002\n\n  -  image: ubuntu\n     name: sidecar\n     command: [\"sleep\", \"5000\"]\n</code></pre> <p>In the above manifest file, container <code>web</code> will run as user <code>1002</code> whereas the container <code>sidecar</code> will run as user <code>1001</code>. If we want to add or drop linux capabilities, then we can add them in the manifest file either in the container level.</p> <p><code>Remember that capabilities supports only the container level and not the pod level</code></p> <p><code>security-context.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-pod\nspec:\n  securityContext:\n    runAsUser: 1001\n    runAsGroup: 1005\n    fsGroup: 2000\n    fsGroupChangePolicy: OnRootMismatch\n  containers:\n  -  image: ubuntu\n     name: web\n     command: [\"sleep\", \"5000\"]\n     securityContext:\n      runAsUser: 1002\n      capabilities:\n        add: [\"NET_ADMIN\"]\n        drop: [\"SYS_TIME\", \"MAC_ADMIN\"]\n\n  -  image: ubuntu\n     name: sidecar\n     command: [\"sleep\", \"5000\"]\n     securityContext:\n      runAsNonRoot: true\n</code></pre> <p></p>"},{"location":"k8s/services/","title":"Services","text":"<p>Remember below points - In k8s, every pod has its own IP address - Containers within the pod communicates over the localhost</p> <p></p> <p>As both pods are in same node, Front end pod will access the backend pod using backend pod's IP. What happens if the backend  pod was terminated due to some reasons? As you know k8s will recreate the backend pod in case of termination. However, when it is recreated, the IP of the backend pod will change.</p> <p></p> <p>So, you need to update backend IP details in the frontend pod configuration everytime whenever backend pod restarts. Else,  you will get 404 error.</p> <p>In case of multiple backend pods need to be connected from front end pod, how do we configure front end pod? Also, if we enable autoscaling based on load, backend pods will be created and destroyed based on load. In that case, how do we  configure front end pod to access backend pods?</p> <p>To answer above questions, the solution is service.</p> <p></p> <p>So - Service provides single IP or DNS through which pods can be accessible. - Services allow performing operations like load balancing and scaling.</p> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-1.0\n  labels:\n    app: nginx-1.0\n    env: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-1.0\n  template:\n    metadata:\n      labels:\n        app: nginx-1.0\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.19\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-1.1\n  labels:\n    app: nginx-1.1\n    env: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-1.1\n  template:\n    metadata:\n      labels:\n        app: nginx-1.1\n    spec:\n      containers:\n        - name: nginx\n          image: anshuldevops/magicalnginx:latest\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: access-pod\nspec:\n  containers:\n    - name: access-pod-container\n      image: ubuntu:latest\n      command: [\"sh\", \"-c\", \"sleep 3600\"]\n</code></pre></p> <p></p> <p>Remember - We CANNOT access any pod outside the cluster   - To access any pod, we need to get into another pod in the cluster and run the curl command like</p> <pre><code> ```curl POD IP ADDRESS```\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080       -----------&gt; Listening Port (service is getting request port 8080 from frontend)\n      targetPort: 80   -----------&gt; Forwarding Port (service is connecting/redirecting pod's port 80 here and pod process the request)\n</code></pre> <p>We created a service now. However, our service doesn't know which pod it needs to connect. To find out which pod it needs to connect, we can create end point.</p> <p></p> <p><pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  name: my-service -----&gt; This is how endpoint find which service it needs to connect\nsubsets:\n  - addresses:\n      - ip: 10.244.0.119 -----&gt; POD 1 IP\n      - ip: 10.244.0.180 -----&gt; POD 2 IP\n      - ip: 10.244.0.21  -----&gt; POD 3 IP\n    ports:\n      - port: 80\n</code></pre> </p> <p></p>"},{"location":"k8s/services/#service-types","title":"Service Types","text":""},{"location":"k8s/services/#cluster-ip-default-type","title":"Cluster IP (Default type)","text":"<ul> <li>When you create a cluster Ip type service, it can be access ONLY within the cluster. You CANNOT access outside cluster.</li> <li>In cluster IP service, internal ip address automatically assigned to the service. As it is an internal IP, outside traffic will be unable to access this service.</li> </ul> <p>Remember, if you DON'T want to expose your pods to outside world, then use CLUSTER IP service.</p>"},{"location":"k8s/services/#nodeport","title":"NodePort","text":"<ul> <li>When you create a node port type service, you have to specify the type property as \"NodePort\".</li> <li>When you create a node port type service, it can be accessed  outside cluster as well.</li> <li>Node port type service will create cluster IP type service automatically to access the pods within the cluster.</li> <li>Node port type service will open the port on the worker node. It exposes the service on each node's ip at a static port.</li> <li>User will be connected the service using : from outside cluster. <li>By default, k8s control plane allocate the port ranges from 30000 to 32767.</li>"},{"location":"k8s/services/#load-balancer","title":"Load balancer","text":"<ul> <li>Whenever you create a load balancer type service, k8s automatically create load balancer and attach it to the ports.</li> <li>It is quite expensive. To avoid more expenses, create single load balancer service and create multiple ingress and attach the ingress to the load balancer .</li> </ul>"},{"location":"k8s/services/#dynamic-endpoints-using-selectors","title":"Dynamic endpoints using selectors","text":"<p>Service can connect the pods using selectors. In this case, we don't need to create endpoints manually.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-3\n  labels:\n    app: nginx-3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      color: blue\n  template:\n    metadata:\n      labels:\n        color: blue\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:1.19\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-4\n  labels:\n    app: nginx-4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      color: blue\n  template:\n    metadata:\n      labels:\n        color: blue\n    spec:\n      containers:\n        - name: nginx\n          image: anshuldevops/magicalnginx:latest\n          ports:\n            - containerPort: 80\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-blue-service\nspec:\n  selector:\n    color: blue\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 80\n</code></pre> <p></p> <p>In the above case, we didn't create any end points for the services. However, it is picked the pod's ip address based on the selector. If we are increasing the replicas of the pods, still service picks pod's ip address automatically.</p> <p></p>"},{"location":"kafka/building-blocks/","title":"Building Blocks of Kafka","text":""},{"location":"kafka/building-blocks/#broker","title":"Broker","text":"<p>Broker stores and manage the messages in kafka cluster. Kafka broker is distributed in nature. Below are the main benefits from kafka broker</p> <p>1. Message management: Kafka broker is basically a storage, and it stores events/messages and retrieved by consumer whenever they need.</p> <p>2. Cluster node: Kafka broker is a node in a cluster. It can be one node or many nodes in the cluster.  Hence, it can handle multiple messages without any performance degradation.</p> <p>3. Scalability: Kafka broker is scalable easily depends on the workload.  When it scales, there is no downtime that ensures kafka's stability and reliability.</p> <p>4. Fault tolerance: Kafka broker is distributed in nature.  Hence, if there is any failure in the broker, the messages can be processed by another kafka broker without data loss.</p> <p>5. Dynamic membership: kafka broker can scale dynamically based on the load without disturbing any running kafka.  It can join or leave clusters without downtime.</p>"},{"location":"kafka/building-blocks/#topics","title":"Topics","text":"<p>Topics are nothing but a place where producers send the message and consumers consume the message. Topics are not physical entities like brokers. It is the mechanism or the way to categorize and organize the messages within the broker. </p> <p>Below are the characteristics for topics</p> <p>1. Message categorization: Topics are grouping the related messages and organize it logically.</p> <p>2. Immutable log: Topics ensure that messages are stored sequentially and not modified in between.</p> <p>3. Multi consumer access: Topics ensure that multiple consumers can read the messages from the same topic. </p> <p>4. Decoupled communication: Topic helps to decouple the communication between producers and consumers. </p> <p>5. Replication: It ensures that replication is available across broker.  This will ensure that there is no data loss even if something goes wrong.</p>"},{"location":"kafka/building-blocks/#partitions","title":"Partitions","text":"<p>Partitions and replication help us to avoid the downtime if broker goes down. </p> <ol> <li> <p>[ ] Partitions distribute the topic across all brokers. This will help if one of the brokers goes down, other brokers can still serve the data from the topic.</p> </li> <li> <p>[ ] Parallelism ensures processing of messages by consumers at the same time. This increases throughput</p> </li> <li> <p>[ ] Partition helps a topic to scale easily when the load goes high and a single broker cannot handle the load.</p> </li> <li> <p>[ ] Partition ensures the order of the message in the topic. The order should be sequential. </p> </li> </ol> <p></p> <p>Key points to note</p> <ol> <li> <p>Partition key used to send the message to the same partition.</p> </li> <li> <p>Over partitioning and under partitioning are creating a problem. So, proper partitioning helps to achieve the throughput</p> </li> <li> <p>Consumer group should have the number of consumers which match the number of partitions.</p> </li> </ol>"},{"location":"kafka/intro/","title":"Event Streaming","text":"<p>Event streaming is the continuous flow of data and processing in real time.</p>"},{"location":"kafka/intro/#event-streaming-architecture-drawbacks","title":"Event Streaming Architecture - Drawbacks","text":""},{"location":"kafka/intro/#processed-events","title":"Processed events","text":"<p>When service A produces an event and sends it to kafka broker for service B to consume it. Once service B consumes the event and sends back the response to service A. This event is called as processed event</p> <ol> <li> <p>Tight coupling: Let us assume one service (service A) depends on another service (service B).If you are making  a change in service A, this will impact service B as well.    If service A goes down, service B cannot work.</p> </li> <li> <p>Reduced scalability: You cannot scale the services if the load goes high.</p> </li> <li> <p>Single point of failure: If service A goes down, service B cannot work. The whole system is unable to perform as expected.</p> </li> <li> <p>No message persistence: If your data triggered an event and in queue.  In case your data lost due to single point of failure, you cannot recover it again.    There is no persistent storage.</p> </li> </ol> <p>To avoid these drawbacks, Apache kafka comes into the picture. Kafka solves these problems and gives the below benefits</p> <ul> <li>High throughput: It can process millions of data without losing them in a short span.</li> <li>Fault tolerance: It is distributed, and hence there is no single point of failure. Hence, reliable and highly available.</li> <li>Scalability: Kafka is highly scalable without any service disruption.</li> </ul>"},{"location":"kafka/intro/#introduction-apache-kafka","title":"Introduction - Apache Kafka","text":"<p>Apache kafka is nothing but a distributed event streaming platform and highly scalable for below</p> <ol> <li>Creating a real-time data streams</li> <li>Processing a real-time data streams</li> </ol> <p>For example, let's assume it; you are sending a message in WhatsApp in every minute. This message goes to kafka server. Similarly, the same kafka server will receive the message from many people at every minute as well resulting a continuous  data streams. So, bringing your message to kafka server is called as \"Creating a real-time data streams\".</p> <p>Now, your message needs to send it to the recipient within a second or at least within a minute. So, you have to create an application which listens to kafka server continuously and process the real-time data streams as soon as it arrives in kafka  server.So, bringing your message from kafka server to the recipient is called as \"Processing a real-time data streams.\" </p> <p>In the above example, you are the publisher, your recipient is the consumer, and kafka server is a broker.</p> <p></p>"},{"location":"kafka/intro/#kafka-architecture","title":"Kafka Architecture","text":""},{"location":"kafka/intro/#kafka-components","title":"Kafka Components","text":"<ol> <li>Kafka Broker - This is a kafka server</li> <li>Kafka Client - This is a publisher and consumer</li> <li>Kafka Connect - This helps the data integration </li> <li>Kafka Streams - This helps to create real-time stream processing</li> <li>KSQL - This helps kafka become to database.</li> </ol>"},{"location":"kafka/intro/#kafka-core-concepts","title":"Kafka Core Concepts","text":""},{"location":"kafka/intro/#1-producer","title":"1. Producer:","text":"<p>It is an application to produce the data (message) and send it to kafka server. The data may have different structure, schema. However, kafka server considers the data as an array of bytes. Assume if you have a database having 10 rows, and your application reads those 10 rows and sends each row as a message to kafka server, so totally 10 messages to the server.</p>"},{"location":"kafka/intro/#2-consumer","title":"2. Consumer:","text":"<p>It is an application to receive the data (message) from kafka server. Consumers can request the data to kafka server from  any producer as long as they have required permission to the producer. Consumer requests the data continuously as long as kafka server provides the data. Consumer application uses the message; however, it wants.</p> <p></p>"},{"location":"kafka/intro/#3-broker","title":"3. Broker:","text":"<p>This is nothing but a kafka server. This is called broker because it is a place in between producer and consumer and acts  as mediator. Producers and Consumers DO NOT interact directly. They use kafka server as a broker to exchange the message. </p>"},{"location":"kafka/intro/#role-of-broker","title":"Role of broker","text":"<p>A System produces an event and sends it to broker. System B and system X consume it from the broker. In this architecture, the System doesn't be aware who is going to consume the events it is generated. At the same time, System B and System X also don't know where the events are coming from.</p> <p></p>"},{"location":"kafka/intro/#4-cluster","title":"4. Cluster:","text":"<p>It is a group of computers acting for common goal. Each computer instance runs one instance of kafka broker.</p>"},{"location":"kafka/intro/#5-topic","title":"5. Topic:","text":"<p>It is part of kafka server (broker). One Kafka broker may have many topics, and each topic has unique data. It is a unique  name of the data streams. It is similar to a table in a database. Producer sends the data to the specific topic, and consumers  read the data from the topic and process it as long as it has a read permission to that topic. Creating 'n' of topics is  depending on architectural design.</p>"},{"location":"kafka/intro/#6-partitions","title":"6. Partitions:","text":"<p>It is a small portion of the topic. It can be any size (maybe large storage or small storage).  The Number of partitions in a topic is also decided by architectural design. </p>"},{"location":"kafka/intro/#7-partitions-offset","title":"7. Partitions Offset:","text":"<p>It is a unique id of a message in the partition. Unique ID is automatically assigned by kafka broker to every message as it arrives in the partition. This unique ID will NOT change once it is assigned. Unique ID starts from 0 for every message, and it is local to that partitions.</p>"},{"location":"kafka/intro/#8-consumer-group","title":"8. Consumer Group:","text":"<p>It is nothing but a group of consumers to share the load.</p> <p>NOTE</p> <p>Kafka does NOT allow more than one consumer to read and process the data from the same partition simultaneously. </p>"},{"location":"kafka/intro/#kafka-backbone-of-event-driven-architecture","title":"Kafka - Backbone of event driven architecture","text":"<p>Kafka solves the below problems in the event driven architecture.</p> <ol> <li> <p>High Throughput: Kafka can process millions of events without any message to be left out.</p> </li> <li> <p>Fault Tolerance: Kafka's distributed architecture ensures that high availability and reliable.  Even one of the systems goes down, kafka ensures that there is no data loss.</p> </li> <li> <p>Scalability: Kafka can scale horizontally when the load goes high. It is dynamic in nature.</p> </li> </ol>"},{"location":"kafka/setup/","title":"Setup Kafka Cluster","text":"<p>We are setting up the kafka cluster in local machine using docker. Along with it, we are going to set up kafka UI as well.</p> <p>Step # 1: Create docker network</p> <pre><code>docker network create kafka-net\n</code></pre> <p></p> <p>Step # 2: Create kafka cluster container</p> <pre><code>docker run --rm -d --network kafka-net --name kafka-cluster -e ADV_HOST=kafka-cluster -e USER=admin -e PASSWORD=secret -p 9092:9092  -p 8081:8081 -p 8083:8083 -p 3030:3030 lensesio/fast-data-dev\n</code></pre> <p></p> <p>Step # 3: Create kafka UI container</p> <pre><code>docker run --rm -d --network kafka-net -p 7001:8080 -e DYNAMIC_CONFIG_ENABLED=true --name kafka-ui provectuslabs/kafka-ui\n</code></pre> <p></p>"},{"location":"nginx/intro/","title":"NGINX - A Web Server","text":""},{"location":"nginx/intro/#what-is-webserver","title":"What is Webserver?","text":"<p>Web server is nothing but a server which contains the collection of web pages, and it serves the web page based on the incoming request. When the user makes a request (website) from browser, it goes to DNS, and fetches the IP address of the website's web server. Then, the request is redirected to the web server, and the web server responds with the web page to the request.</p> <p>Examples of Web servers : Apache, IIS, Litespeed, Caddy, NGINX, and Openresty</p>"},{"location":"nginx/intro/#introduction-to-nginx","title":"Introduction to NGINX","text":"<p>NGINX is a web server. 1. It is much better than Apache and IIS in terms of performance. 2. For example, NGINX can support 4x connections with half of the resources which Apache/IIS consumes. 3. It has the best feature to serve the static content efficiently. 4. It has features like caching, rate limiting, load balancing, data compression, connection handling, etc. 5. NGINX serves as        1. Load balancer       2. Reverse proxy       3. Forward proxy       4. Cache</p>"},{"location":"nginx/intro/#architecture-of-nginx","title":"Architecture of NGINX","text":"<p>NGINX follows event-driven architecture. The multiple incoming requests are handled parallel, and it is called as asynchronous processing. So, it is non-blocking and fast.</p> <p></p> <p>When the customer makes incoming request (HTTP/HTTPS), it goes to NGINX, and NGINX will log the request and put them into loop. Then NGINX looks for a new event to serve without blocking new requests. Then the incoming request will be processed. In case, if the incoming request required some data from DB or third party services, it will go to waiting state. While it is in waiting state, NGINX will pick another request from loop (instead of pausing) and start processing it. Once the data is received, it switches from the current loop to old one and finishes the processing and sends the response back to the customer.  </p> <p></p> <p>Here event loop is crucial as it accepts the incoming requests, log them and pass them to process the event. Then it goes back to look for another incoming request.</p>"},{"location":"nginx/intro/#request-handling-in-nginx","title":"Request Handling in NGINX","text":"<p>NGINX follows Master - Worker architecture to process the request. Hence, it serves multiple requests simultaneously</p> <ul> <li>Master manages the worked processes and ensures that they are up, working/serving multiple incoming requests. In case, if the worker process is down, it will handle the restart the worker process or update the configuration etc.</li> <li>Master never handles any incoming request. Instead, it assigns them to a worker process and manages them.</li> <li>Worker processes are running independent using event-driven approach. </li> <li>Worker processes are serving the incoming requests and ensure that event loop is logging the request  and then immediately checking for the new event.</li> </ul> <p></p>"},{"location":"nginx/intro/#structure-of-nginx-configuration-file","title":"Structure of NGINX Configuration File","text":"<p>NGINX configuration file has four different sections.</p>"},{"location":"nginx/intro/#global-settings","title":"Global settings","text":"<p>This setting affects the entire NGINX server. This is the place where you can configure the privileged user details, number of workers, caching, rate limiting, etc.</p> <ul> <li>User: NGINX runs as a service with the user mentioned here.</li> <li>Worker_processes: Worker processes are decided based on the CPU core of the server.  If the server has four CPU core, then there are four worker processes run. However, setting this option is \"auto\"   would be perfect and NGINX will decide the number of worker processes.</li> <li>pid: This is the path where the \u201cmaster\u201d process identifier stores.   Whenever we give some signals (restart, reload, shutdown),  this will go to this pid and master will decide the action based on the signal.</li> </ul>"},{"location":"nginx/intro/#events-block","title":"Events block","text":"<p>This is managing the connections like how many worker connections for each worker process, how the worker process picks the incoming request to process, etc.</p> <ul> <li>worker_connections: The number of maximum simultaneous connections can be performed by one worker process. For example, if the server has four CPU core, then four worker processes can run by master.   Each worker process can support 1024   (mentioned in the snapshot. It can change as per our need) simultaneous connections.   As we have four worker processes, it can support 4 x 1024 connections in parallel.   </li> </ul>"},{"location":"nginx/intro/#http-block","title":"HTTP block","text":"<p>This is the setting where we can handle the web traffic. This includes server block as well. We can configure security setting, HTTP optimization techniques such as caching, rate limiting, data compression, etc. You can create one server block for one website. If you have multiple websites, you can create multiple server blocks and configure NGINX setting for each website.</p> <p><code>NOTE: You can have one server block and can have multiple websites as well.  But that is not preferable as one changes impacts multiple websites.</code></p> <p>Here we can configure all HTTP related configurations, log format, rate limiting, cache details, keep-alive settings, and other global configurations.</p>"},{"location":"nginx/intro/#server-block","title":"Server block","text":"<p>This is the key configuration. This is where we can inform NGINX how it should behave for each domain and subdomain requests. </p> <ul> <li>listen: This is the port NGINX should listen for incoming traffic. <code>Port 80</code> for HTTP traffic and <code>Port 443</code> for HTTPS traffic.</li> <li>server_name: This is the domain name or subdomain name which a user enters in their browser</li> <li>root: This is the directory where all the website files are available. Your <code>index.html</code> file should be here in the path.</li> <li>location /: This is the landing page. </li> <li>try_files /: This looks for the file under the root location. If it finds, then it serves the webpage.  If it didn't find, then it will return as 404 error page.</li> </ul> <p></p> <p></p>"},{"location":"nginx/intro/#important-nginx-configuration-files","title":"Important NGINX Configuration Files","text":""},{"location":"nginx/intro/#nginx-commands","title":"NGINX Commands","text":"S.No Command Description 1 nginx -v Gives NGINX version 2 nginx -h Describes the command and its details 3 nginx -t Validate the configuration file (nginx.conf) 4 nginx -T Useful for sharing config when seeking support 5 nginx -V Gives NGINX build version, config info and module details 6 nginx -s Gives signal to master to perform an action"},{"location":"observability/authentication/","title":"Authentication &amp; Encryption","text":"<p>I am running three different EC2 instances in my AWS account. Out of these three machines, two machines are having prometheus (one is an actual prometheus server and another one is normal ubuntu) and one machine has a node exporter. The goal is prometheus server should scrape the node server metrics,  and ubuntu server should not scrape the node server metrics.</p> <p></p> <p>As of now, both ubuntu server and prometheus server are scraping the metrics.</p> <p> </p> <p></p> <p>This goal can be achieved using authentication and encryption.</p>"},{"location":"observability/authentication/#authentication","title":"Authentication","text":"<p>Step 1 - Create directories and assign permissions in the node server</p> <pre><code>sudo mkdir /etc/node_exporter\nsudo touch /etc/node_exporter/config.yml\nsudo chmod 700 /etc/node_exporter\nsudo chmod 700 /etc/node_exporter/config.yml\nsudo chown -R node_exporter:node_exporter /etc/node_exporter \nsudo systemctl daemon-reload\nsudo systemctl restart node_exporter.service \n</code></pre> <p></p> <p></p> <p>Step 2 - Create password for node server for authentication</p> <pre><code>sudo apt update\nsudo apt install apache2-utils -y\nhtpasswd -nBC 10 \"\" | tr -d ':\\n'; echo\n</code></pre> <p></p> <p></p> <p>Step 3 - Update password in the node server configuration</p> <p>Update the node server configuration file located at <code>/etc/node_exporter/config.yml</code></p> <pre><code>vi /etc/node_exporter/config.yml\n\nbasic_auth_users:\n  &lt;username&gt;: &lt;password in hash format&gt;\n</code></pre> <p></p> <p>Step 4 - Update the node service and restart the node server</p> <p><pre><code>[Service]\nUser=node_exporter\nGroup=node_exporter\nType=simple\nExecStart=/usr/local/bin/node_exporter --web.config=/etc/node_exporter/config.yml\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> </p> <p>Restart the node server</p> <pre><code>sudo systemctl restart node_exporter.service\ncurl -u &lt;username&gt;:&lt;password&gt; http://&lt;hostname&gt;:&lt;port&gt;/metrics\n</code></pre> <p></p> <p></p> <p>Step 5 - Update the authentication in prometheus server</p> <p>Update the authentication in prometheus configuration located at <code>/etc/prometheus/prometheus.yml</code></p> <pre><code>basic_auth:\n  username: &lt;username&gt;\n  password: &lt;password in plain text&gt;\n</code></pre> <p></p> <p>Step 5 - Restart prometheus server</p> <pre><code>sudo systemctl restart prometheus\n</code></pre> <p></p>"},{"location":"observability/authentication/#encryption","title":"Encryption","text":"<p>Step 1 - Create certificates for node server</p> <p><pre><code>openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout node_exporter.key -out node_exporter.crt -subj \n\"/C=US/ST=California/L=Oakland/O=MyOrg/CN=localhost\" -addext \"subjectAltName = DNS:localhost\"\n</code></pre> </p> <p>Step 2 - Copy certificates to node server location</p> <p><pre><code>sudo cp node_exporter.* /etc/node_exporter/\nsudo chown -R node_exporter:node_exporter /etc/node_exporter\n</code></pre> </p> <p>update the node server configuration file</p> <pre><code>tls_server_config:\n  cert_file: &lt;path of crt file&gt;\n  key_file: &lt;path of key file&gt;\n</code></pre> <p></p> <p>Restart node server</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart node_exporter.service\n</code></pre> <p>Step 3 - Copy node server certificates to prometheus server</p> <p><pre><code>scp &lt;username&gt;@&lt;node server hostname&gt;:/etc/node_exporter/node_exporter.crt /opt/prometheus/node_exporter.crt\n</code></pre> </p> <p>Step 4 - Update prometheus configuration</p> <p>Update the prometheus configuration located at <code>/opt/prometheus/prometheus.yml</code> for a node job</p> <p><pre><code>scheme: https\ntls_config:\n  ca_file: /etc/prometheus/node_exporter.crt\n  insecure_skip_verify: true\n</code></pre> </p> <p>Step 5 - Restart the prometheus server</p> <p>Restart the prometheus server</p> <pre><code>sudo systemctl restart prometheus\n</code></pre> <p>Now prometheus server is able to connect and scrape the metrics for node server.</p> <p></p> <p>However, ubuntu server is unable to connect the node server because it does not be authorized to connect the node server.</p> <p></p>"},{"location":"observability/docker-monitoring/","title":"Docker Engine &amp; Container Monitoring","text":"<p>We can monitor the docker engine and docker containers in prometheus. </p> <p>I am using EC2 instance (my ubuntu server) where docker engine is running and another EC2 instance where prometheus server is running that will scrape docker engine and containers metrics from my ubuntu server.</p>"},{"location":"observability/docker-monitoring/#docker-engine-monitoring","title":"Docker Engine Monitoring","text":"<p>Step 1 - Update docker daemon configuration</p> <p>Create/update the docker daemon file located at <code>/etc/docker/daemon.json</code></p> <pre><code>{\n  \"metrics-addr\": \"0.0.0.0:9323\",\n  \"experimental\": true\n}\n</code></pre> <p></p> <p>Step 2 - Restart docker engine</p> <p>Run the following command to restart the docker</p> <pre><code>systemctl restart docker\n</code></pre> <p></p> <p>Step 3 - Check locally if the docker metrics are visible</p> <p>Run the following command to see if the docker metrics are visible</p> <pre><code>curl http://localhost:9323/metrics\n</code></pre> <p></p> <p>Step 4 - Configure prometheus to scrape the docker machine</p> <p>Update the prometheus configuration file located at <code>/opt/prometheus/prometheus.yml</code></p> <p></p> <p>Step 5 - Restart prometheus</p> <p>Run the following command to restart the prometheus server</p> <pre><code>systemctl restart prometheus\n</code></pre> <p></p> <p>Step 6 - Check the target status in prometheus UI</p> <p></p> <p></p>"},{"location":"observability/docker-monitoring/#docker-containers-monitoring","title":"Docker Containers Monitoring","text":"<p>To monitor docker containers, we need cAdvisor to run on the machine.</p> <p>cAdvisor (Container Advisor) is an open-source tool by Google that provides container users with an understanding of  the resource usage and performance characteristics of their running containers, offering detailed real-time metrics  about CPU, memory, file and network usage.</p> <p>I am using EC2 instance (my ubuntu server) where cAdvisor is running and another EC2 instance where prometheus server is running that will scrape docker containers metrics from my ubuntu server.</p> <p>Step 1 - Run cAdvisor in ubuntu server</p> <p>Create the docker compose file (anywhere in the machine) and run it.</p> <pre><code>version: '3.4'\nservices:\n  cadvisor:\n    image: gcr.io/cadvisor/cadvisor\n    container_name: cadvisor\n    privileged: true\n    devices:\n      - \"/dev/kmsg:/dev/kmsg\"\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:ro\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /dev/disk/:/dev/disk:ro\n    ports:\n      - 8070:8080\n</code></pre> <p></p> <p>To run the docker compose file, go to the file location and run the below command</p> <pre><code>docker compose up -d\n</code></pre> <p></p> <p>Step 2 - Check docker container metrics scrape or not in ubuntu server</p> <p>Run the command</p> <pre><code>curl localhost:8070/metrics\n</code></pre> <p></p> <p>As per the snapshot, container metrics are scraped locally in the ubuntu server machine.</p> <p>Step 3 - Integrate cAdvisor with Prometheus</p> <p>Update the prometheus configuration file located at <code>/opt/prometheus/prometheus.yml</code></p> <pre><code>  - job_name: 'cadvisor'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['&lt;ubuntu server ip&gt;:8070']\n</code></pre> <p></p> <p>Step 4 - Restart Prometheus</p> <p>Run the following command to restart the prometheus server</p> <pre><code>systemctl restart prometheus\n</code></pre> <p></p> <p>Step 5 - Check the target status in prometheus UI</p> <p></p> <p></p>"},{"location":"observability/intro/","title":"Observability","text":"<p>Observability is nothing but the ability to understand and measure the state of the system based on the data generated by the system.</p> <p>Observability helps:</p> <ol> <li>Give insights of the internal working of a system/application</li> <li>Monitoring the application</li> <li>Speed up the troubleshooting process</li> <li>Detect the problem that is hard to catch</li> </ol> <p>Without observability in place, your application is just blind because the data goes to your application and the data comes out of the application. You never know what is going on inside your application.</p> <p></p> <p>Microservice based applications and distributed system must need observability in place. There are so many  internal components are talking to each other, and when a problem occurs, we need to figure out which component  is causing the problem, and what is the reason for the problem. This will help us to avoid the problem in the future.</p> <p>Examples: 1. Why is the error rate suddenly increasing? 2. Why is the latency increase? 3. Why is the service getting timeout? 4. Why is the database query taking so long?</p> <p>There are three pillars of observability</p> <ol> <li>Logging</li> <li>Traces</li> <li>Metrics</li> </ol>"},{"location":"observability/intro/#logging","title":"Logging","text":"<p>Logs are record of events that occurred and provide information about that event. The Log contains two important fields.</p> <ol> <li>Timestamp</li> <li>Message</li> </ol>"},{"location":"observability/intro/#traces","title":"Traces","text":"<p>Traces are allowing us to follow as it traverses an end to end flow involving various systems and services. It helps us to view individual request flow through different system hop by hop. </p> <p></p> <p>When you send a request to your application, you can watch it go through each layer (system by system) and you can see  how the request and response work in each system.</p> <ol> <li>Each request has trace id (unique one) that helps to identify the request in each layer.</li> <li>Individual events occur in each layer forms a trace, and it is called as spans.</li> <li>Each span has </li> <li>Duration</li> <li>Parent ID</li> <li>Start time</li> </ol> <p></p>"},{"location":"observability/intro/#metrics","title":"Metrics","text":"<p>It provides the information about the state of the system by using numerical values. The data can be collected over a period of time and aggregated to help identify the trend over time.</p> <p>Examples:</p> <ul> <li>Number of errors</li> <li>Response time</li> <li>CPU utilization</li> <li>Disk space</li> <li>Memory utilization</li> </ul> <p>Metrics contain the following components</p> <ol> <li>Metric name</li> <li>Metric value (current or recent one)</li> <li>Timestamp</li> <li>Dimensions (Additional information about the metrics)</li> </ol> <p>When you design a system, it is important to set the target/measurable goals to help the organization to make it the  right balance between a development and operational team. This helps to ensure the reliability of the product and  customer's confidence about the product.</p> <p>To do so, there are few parameters that need to be defined. They are below</p>"},{"location":"observability/intro/#service-level-indicator-sli","title":"Service Level Indicator (SLI)","text":"<p>It is nothing but a quantitative measure of the service that we provide.</p> <p>Example: </p> <ol> <li>Availability - Uptime should be 99%</li> <li>Latency - Request latency should be less than 100ms.</li> </ol> <p>When you set SLI, always consider customer/user perspective. Not all metrics are a good SLI.</p>"},{"location":"observability/intro/#service-level-objective-slo","title":"Service Level Objective (SLO)","text":"<p>It is a target value or range of SLI.</p> <p>Example: </p> <ol> <li>If we are considering SLI for latency/response time to quantify your product reliability, then we should align it with  SLO. It means we are setting the target value for latency. Latency should not exceed 100ms. This is nothing but a SLO.</li> <li>Availability is SLI and 99% uptime is SLO.</li> </ol>"},{"location":"observability/intro/#service-level-agreement-sla","title":"Service Level Agreement (SLA)","text":"<p>It is just a contract between vendor and user that guarantees SLO. If not meeting SLO, there could be a penalty.</p>"},{"location":"observability/node-exporter/","title":"Node Exporter Installation","text":"<p>I am using linux ubuntu machine for this installation. I spin up linux ubuntu machine in AWS cloud, and it is up and running. I have also connected EC2 instance using iterm terminal in my Mac machine.</p> <p>Step 1</p> <p>Go to the website \"https://prometheus.io/download/#node_exporter\" and find the latest version of node exporter tar file. Copy the link for that tar file.</p> <p></p> <p>Step 2</p> <p>Run the command \"wget \" in the terminal to download the tar file. <p><code>wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz</code></p> <p></p> <p>Step 3</p> <p>Untar the downloaded file using the command \"tar -xvzf . <p><code>tar -xvzf node_exporter-1.8.2.linux-amd64.tar.gz</code></p> <p></p> <p>Step 4</p> <p>Navigate to node exporter directory (unzipped one)</p> <p></p> <p>Step 5</p> <p>Start the node exporter executable file</p> <p></p> <p>Step 6</p> <p>Open the browser and enter node exporter server IP followed by port number 9100.</p> <p></p> <p>Node exporter is up and running now. However, if I close the terminal where node exporter executable is running or my EC2 instance is down, then the node exporter also stops. Hence, this is not the ideal way to run the node exporter.  The ideal way of running node exporter is to install and run as a service (systemd).</p> <p>The ideal way of running node exporter as a service. </p> <p>Step 1 - Create a system user for node exporter</p> <p>System username is <code>node_exporter</code>. Before doing it, we need to ensure <code>node_exporter</code> user is not created yet. </p> <p>To do so, run the command <code>cat /etc/passwd | grep -i node_exporter</code> If it returns value, it means <code>node_exporter</code> user is already available in the system.</p> <p>We can create system user now. To create the user, run the below commands</p> <pre><code>sudo useradd --no-create-home --shell /bin/false node_exporter\n</code></pre> <p></p> <p>To check if the user is created successfully, run the command </p> <pre><code>cat /etc/passwd | grep -i node_exporter\n</code></pre> <p></p> <p>Step 2 - Copy the executable file to bin location</p> <p>To copy the node exporter to bin location, run the below commands</p> <pre><code>sudo cp node_exporter /usr/local/bin/\n</code></pre> <p></p> <p>Step 3 - Assign permission and ownership to the system user</p> <p>To assign the ownership to <code>node exporter</code> user, run the below commands</p> <pre><code>sudo chown node_exporter:node_exporter /usr/local/bin/node_exporter\n</code></pre> <p></p> <p>Step 4 - Create a service file for node exporter</p> <p>Update the below content in the node exporter service file located at <code>/etc/systemd/system/node_exporter.service</code></p> <pre><code>[Unit]\nDescription=Node Exporter\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=node_exporter\nGroup=node_exporter\nType=simple\nExecStart=/usr/local/bin/node_exporter\nRestart=always\nRestartSec=3\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p></p> <p>Step 5 - Reload the daemon service</p> <p>To do so, run the below command </p> <pre><code>systemctl daemon-reload\n</code></pre> <p></p> <p>Step 6 - Start the node exporter service</p> <p>To do so, run the below command </p> <p><pre><code>systemctl start node_exporter.service\n</code></pre> </p> <p>Step 7 - Check the status of the node exporter service</p> <p>To do so, run the below command </p> <p><pre><code>systemctl status node_exporter.service\n</code></pre> </p> <p>Step 8 - Enable the node exporter service</p> <p>To do so, run the below command. This will ensure that the node exporter service automatically starts at system booting. </p> <p><pre><code>systemctl enable node_exporter.service\n</code></pre> </p> <p>Step 9 - Check the metrics are populated</p> <p>To do so, run the below command </p> <p><pre><code>curl localhost:9100/metrics\n</code></pre> </p>"},{"location":"observability/node-exporter/#integrate-node-exporter-with-prometheus","title":"Integrate Node Exporter with Prometheus","text":"<p>Prometheus scrapes the target and pulls the metrics. So, it should know what the target is prior to pull the metrics. Prometheus target configuration is configured in the <code>prometheus.yml</code> file by default. Hence, we need to update the configuration file with the target details of node exporter server details. After this change, restart the prometheus. Then, Prometheus will start to scrape the node metrics after restarting the prometheus.</p> <p>Step 1 - Check Prometheus server status and Node exporter status</p> <p>Connect the prometheus server in the terminal and run the following command.</p> <p><pre><code>systemctl status prometheus.service\n</code></pre> </p> <p>Connect the node exporter machine in the terminal and run the following command.</p> <pre><code>systemctl status node_exporter.service\n</code></pre> <p></p> <p>Step 2 - Update prometheus configuration file</p> <p>Add the node server ip address in the target section to ensure prometheus knows what machine it needs to scrape.</p> <pre><code>  - job_name: \"Node server\"\n    scrape_interval: 10s\n    scrape_timeout: 5s\n    metrics_path: /metrics\n    static_configs:\n      - targets: [\"172.31.84.187:9100\"]\n</code></pre> <p></p> <p>Step 3 - Restart Prometheus Server</p> <p>Connect the prometheus server in the terminal and run the following command.</p> <p><pre><code>systemctl restart prometheus.service\n</code></pre> </p> <p>Step 4 - Check Prometheus server status</p> <p>Connect the prometheus server in the terminal and run the following command.</p> <p><pre><code>systemctl status prometheus.service\n</code></pre> </p> <p>Step 5 - Check Prometheus UI for target status</p> <p>Open the browser and enter the .  <p></p> <p>Click on the menu \"Status -&gt; Targets\"</p> <p></p> <p>As we see, prometheus server is able to understand the node service and where it is running, what is the current state, etc. </p> <p>If we go to the \"graph\" menu, and type \"node\" you will be able to see all the metrics related to node service.</p> <p></p> <p>That's all. Simple, isn't it? </p>"},{"location":"observability/prom-install/","title":"Prometheus Installation","text":"<p>I am using linux ubuntu machine for this installation. I spin up linux ubuntu machine in AWS cloud, and it is up and running. I have also connected EC2 instance using iterm terminal in my Mac machine.</p> <p>Step 1</p> <p>Go to the website \"https://prometheus.io/download/\" and find the latest version of prometheus tar file. Copy the link for that tar file.</p> <p></p> <p>Step 2</p> <p>Run the command \"wget \" in the terminal to download the tar file. <p><code>wget https://github.com/prometheus/prometheus/releases/download/v2.54.1/prometheus-2.54.1.linux-amd64.tar.gz</code></p> <p></p> <p>Step 3</p> <p>Untar the downloaded file using the command \"tar -xvzf . <p><code>tar -xvzf prometheus-2.54.1.linux-amd64.tar.gz</code></p> <p></p> <p>Step 4</p> <p>Navigate to prometheus directory (unzipped one)</p> <p></p> <p>Step 5</p> <p>Start the prometheus executable file (This is nothing but prometheus server)</p> <p></p> <p>Step 6</p> <p>Open the browser and enter prometheus server IP followed by port number 9090.</p> <p></p> <p>Stopping prometheus server</p> <p></p> <p>Now prometheus UI is not working.</p> <p></p> <p>Prometheus is up and running now. However, if I close the terminal where prometheus executable is running or my EC2 instance is down, then the prometheus also stops. Hence, this is not the ideal way to run the prometheus.</p> <p>The ideal way of running prometheus is to install and run as a service. </p> <p>To do so, we need to follow below steps</p> <p>Step 1 - Update the system packages</p> <p>Run the command  <pre><code>sudo apt update\n</code></pre></p> <p></p> <p>Step 2 - Create a system user for prometheus</p> <p>System username is <code>prometheus</code>. Before doing it, we need to ensure <code>prometheus</code> user is not created yet. </p> <p>To do so, run the command <code>cat /etc/passwd | grep -i prometheus</code> If it returns value, it means <code>prometheus</code> user is already available in the system.</p> <p></p> <p>As the above command returns nothing, we can create system user now. To create the user, run the below commands</p> <p><pre><code>sudo groupadd --system prometheus\nsudo useradd -s /sbin/nologin --system -g prometheus prometheus\n</code></pre> </p> <p>Let us ensure the user <code>prometheus</code> is created.</p> <pre><code>cat /etc/passwd | grep -i prometheus\n</code></pre> <p></p> <p>Step 3 - Create Directories for Prometheus</p> <p>To store configuration files and libraries for Prometheus, you need to create a few directories.  The directories will be located in the <code>/etc</code> and the <code>/var/lib</code> directory respectively.  Use the commands below to create the directories:</p> <pre><code>sudo mkdir /etc/prometheus\nsudo mkdir /var/lib/prometheus\n</code></pre> <p></p> <p>Step 4 - Move the Binary Files &amp; Set Owner</p> <p>Navigate to the prometheus directory. You need to move some binary files (prometheus and promtool) and change the  ownership of the files to the \"prometheus\" user and group. You can do this with the following commands:</p> <pre><code>sudo mv prometheus /usr/local/bin\nsudo mv promtool /usr/local/bin\n\n# set the owner \nsudo chown prometheus:prometheus /usr/local/bin/prometheus\nsudo chown prometheus:prometheus /usr/local/bin/promtool\n</code></pre> <p></p> <p>Step 5 - Move the Configuration Files &amp; Set Owner</p> <p>Next, move the configuration files and set their ownership so that Prometheus can access them.  To do this, run the following commands:</p> <pre><code>sudo mv consoles /etc/prometheus\nsudo mv console_libraries /etc/prometheus\nsudo mv prometheus.yml /etc/prometheus\n\n# set the owner \nsudo chown prometheus:prometheus /etc/prometheus\nsudo chown -R prometheus:prometheus /etc/prometheus/consoles\nsudo chown -R prometheus:prometheus /etc/prometheus/console_libraries\nsudo chown -R prometheus:prometheus /var/lib/prometheus\n</code></pre> <p></p> <p>The <code>prometheus.yml</code> file (/etc/prometheus/prometheus.yml) is the main Prometheus configuration file.  It includes settings for targets to be monitored, data scraping frequency, data processing, and storage.</p> <p></p> <p>Step 6 - Create Prometheus Systemd Service</p> <p>Now, you need to create a system service file for Prometheus.  Create and open a prometheus.service file with the vi text editor using:</p> <p><pre><code>vi /etc/systemd/system/prometheus.service\n</code></pre> Content of the file is</p> <pre><code>[Unit]\nDescription=Prometheus\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nUser=prometheus\nGroup=prometheus\nType=simple\nExecStart=/usr/local/bin/prometheus \\\n    --config.file /etc/prometheus/prometheus.yml \\\n    --storage.tsdb.path /var/lib/prometheus/ \\\n    --web.console.templates=/etc/prometheus/consoles \\\n    --web.console.libraries=/etc/prometheus/console_libraries\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p></p> <p>Step 7 - Reload Systemd</p> <p>You need to reload the system configuration files after saving the prometheus.service file so that changes made  are recognized by the system. Reload the system configuration files using the following:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p></p> <p>Step 8 - Start Prometheus Service</p> <p>Next, you want to enable and start your Prometheus service. Do this using the following commands:</p> <pre><code>sudo systemctl enable prometheus\nsudo systemctl start prometheus\n</code></pre> <p></p> <p>Step 9 - Check Prometheus Status</p> <p>After starting the Prometheus service, you may confirm that it is running or if you have encountered errors using:</p> <pre><code>sudo systemctl status prometheus\n</code></pre> <p></p> <p>Step 10 - Update firewall rules Prometheus runs on port 9090 by default, so you need to allow port 9090 on your firewall, Do that using the command:</p> <pre><code>sudo ufw allow 9090/tcp\n</code></pre> <p></p> <p>With Prometheus running successfully, you can access it via your web browser using :9090 <p></p> <p>Run the query in prometheus UI.</p> <p></p>"},{"location":"observability/prometheus-intro/","title":"Introduction to Prometheus","text":"<p>Prometheus is an open-source monitoring tool that helps to collect the metrics data and visualize them.  It allows us to trigger an alert when the metric exceeded the threshold value. </p> <p>Prometheus collects the metrics from scraping the target machines (which need to be monitored). Target machines are exposing their metrics to an HTTP endpoint,  and Prometheus scrapes them by sending GET request to that endpoint. </p> <p>All scraped metrics are stored in a time-series database and can be queried by prometheus using promql. </p> <p>Prometheus is written in <code>Go</code> language. It is primarily designed to monitor for numerical data and NOT designed for system logs, events, etc.</p>"},{"location":"observability/prometheus-intro/#architecture","title":"Architecture","text":"<p>In prometheus server, there are three main parts involved.</p> <ol> <li>Retrieval</li> <li>Time-series database</li> <li>HTTP server</li> </ol> <p>Retrieval is usually collecting the metrics from the target machines by initiating GET request to the endpoint where the target machine exposes the metrics.</p> <p>Once the metrics are gathered, it will be stored in a time-series database.</p> <p>HTTP server allows us to read the metrics stored in a time-series database. User will make an HTTP request to HTTP server  using PromQL. Then HTTP server retrieves the data from time-series for the user request. </p> <p>Exporters</p> <p>In prometheus architecture, exporters are crucial. Exporters are nothing but a process running in a target machine. It collects the metrics data from the target machine and converts them to the format that prometheus can understand. Then exporters expose the metrics to an HTTP endpoint. Retrieval components will make a call to this endpoint and scrape  the metrics (pull the metrics) for this target machine.</p> <p>Short-lived Jobs &amp; Push Gateway</p> <p>Assume that we need to monitor some job that lives short (let's say 5 seconds), and our scraping interval in the prometheus server configured as 15 seconds. By the time the scraping starts, the job must have terminated, and we lose the metrics for that job. </p> <p>To avoid losing the jobs metric, there is another component named <code>push gateway</code> involved. Short-lived jobs push the metric to push gateway, and prometheus server will scrape the metrics from push gateway.</p> <p>Service Discovery</p> <p>In general, prometheus expects all the target details to be configured in the configuration file. However, these are static details such as IP address, and port. In the case of dynamic environment like kubernetes, AWS EC2 instances, the target details will be dynamic. To handle this situation, Service discovery is in place. It provides the list of targets to be scraped to prometheus server. </p> <p>Alert Manager</p> <p>Prometheus never sends an email or sms to the users. It is a responsibility of alert manager. Prometheus trigger the alerts and push them to alert manager when the metric is exceeded the threshold value. These threshold values are configured in alerting rules. Once alerts are reached to alert manager, alert manager starts sending an alert to the respective person via email or sms based on the configuration created for alerting.</p> <p>Visualise</p> <p>Using PromQL, grafana/prometheus UI will query the data and visualize them in the graphical format. They will make a query to HTTP server and read the metric.</p> <p>Remember</p> <ul> <li>Prometheus will scrape the metrics from <code>/metric</code> endpoint by default.   In case your application exposes the metric in a different endpoint,   you have to configure it in the prometheus configuration file.</li> <li>By default, most of the servers will not collect the metrics and expose them to an endpoint.   That's why we install exporters on those servers and gather the metrics.</li> </ul> <p>Client Libraries</p> <p>If we want to gather application metrics, prometheus allows us to install the client libraries and expose application metrics that can be scraped by prometheus server. Client libraries support the following languages - Go - Python - Java - Ruby - Rust</p>"},{"location":"observability/prometheus-intro/#pull-based-model","title":"Pull-Based Model","text":"<p>Prometheus follows pull-based model. It means that it has the list of the servers (targets) to be scraped.  Prometheus pulls the metrics from these target machines and target machines never send any data to the prometheus server.</p> <p>Example:</p> <ol> <li>Prometheus</li> <li>Nagios</li> </ol>"},{"location":"observability/prometheus-intro/#push-based-model","title":"Push-Based Model","text":"<p>The target machines send the data to the server. The Server will collect them and aggregate them and do what it wants to. </p> <p>Example:</p> <ol> <li>Logstash</li> <li>Open TSDB</li> </ol> <p>Remember</p> <ul> <li>In pull-based model, it is easy to tell if the server is up or down as the server maintains the list of the targets.</li> <li>In push-based model, it is not easy to tell if the server is down or decommissioned as servers don't have any list of the targets</li> </ul>"},{"location":"observability/promql/","title":"Introduction to PromQL (Prometheus Query Language)","text":"<p>PromQL is used to get the metrics in Prometheus. When you run the promql expression, we can get any of the below four types of metric.</p> <ol> <li>String (currently unused. <code>Example: \"some string\"</code>)</li> <li>Scalar (floating point numeric value. <code>Example: 124.28</code>)</li> <li>Instant vector - It is a set of time-series contains a single sample for each time-series. All sharing the same timestamp.</li> </ol> <p><code>Example:</code></p> <p> </p> <ol> <li>Range vector - It is a set of time-series contains a range of data points over time for each time-series.</li> </ol> <p><code>Example:</code></p> <p> </p>"},{"location":"observability/promql/#selectors","title":"Selectors","text":"<p>Selectors are nothing but a filter. It helps to filter the metric for specific parameter.</p> <p> </p> <p>In the above snapshot, we want to capture the metric for <code>steal</code> mode, then we need to use <code>mode=steal</code> selector.</p> <p> </p>"},{"location":"observability/promql/#multiple-selectors","title":"Multiple Selectors","text":"<p>Multiple selectors are nothing but a filter with multiple labels. It helps to filter the metric for specific parameter.</p> <p> </p>"},{"location":"observability/promql/#range-vector-selectors","title":"Range Vector Selectors","text":"<p>Range vector selectors are nothing but a filter over the period of time.</p> <p> </p>"},{"location":"observability/promql/#matchers","title":"Matchers","text":"<p>There are four types of matcher.</p> <ol> <li>Equality</li> <li>Negative Equality</li> <li>Regular Expression Matcher</li> <li>Negative Regular Expression Matcher</li> </ol> <p>Equality Matcher</p> <p>It checks the label/selector should be equal to the exact value. In the below case, mode is the selector, and it should exactly match <code>steal</code>. Then the metric can be fetched. </p> <p> </p> <p>Negative Equality Matcher</p> <p>It checks the label/selector should not be equal to the exact value. In the below case, mode is the selector, and except <code>steal</code> value, it can bring all other mode metrics. Then the metric can be fetched. </p> <p> </p> <p>Regular Expression Matcher</p> <p>It checks the label/selector should equal to the approximate value with the pattern mentioned in the label.</p> <p> </p> <p>In the above case, mountpoint has multiple values. Now if we want to filter all the metrics for the mountpoint starts with <code>/bo</code> word.</p> <p> </p> <p>Negative Regular Expression Matcher</p> <p>It checks the label/selector should not be equal to the approximate value with the pattern mentioned in the label.</p> <p> </p> <p>In the above case, mountpoint has multiple values. Now if we do not want to filter all the metrics for the mountpoint starts with <code>/bo</code> word.</p> <p> </p> <p>If we want to filter two values, we can use <code>|</code> in the regular expression selector.</p> <p> </p>"},{"location":"observability/promql/#offset","title":"Offset","text":"<p>If you want to check the historic data, you can use offset.</p> <p> </p> <p>In the above snapshot, the value is displayed that occurred exactly 30 minutes ago from now.</p>"},{"location":"observability/promql/#modifier","title":"Modifier","text":"<p>If you want to check the historic data for a specific time, you can use modifier. For example, I want to see the metric on Thursday, 19 September 2024 06:58:24. I need to convert the date to epoch time and use it in modifier like below</p> <p>Thursday, 19 September 2024 06:58:24 = Epoch time is 1726709304</p> <p> </p> <p><code>We can use both offset and modifier in the same promql expression.</code></p> <p> </p> <p>Remember that </p> <p><pre><code>node_cpu_seconds_total{mode=\"user\"} @1726709304 offset 5m = node_cpu_seconds_total{mode=\"user\"} offset 5m @1726709304 \n</code></pre> <code>node_cpu_seconds_total{mode=\"user\"}[1m] @1726709304 offset 5m</code> we can use offset and modifier in range vector as well.</p> <p> </p>"},{"location":"postgres/acid/","title":"ACID (Atomicity, Consistency, Isolation and Durability)","text":"<p>A transaction is a single unit of work that interacts with the database and modifying the database through read or write operations. To maintain the database consistency before and after the transaction, it must be followed by ACID properties.</p> <p></p>"},{"location":"postgres/acid/#atomicity-in-acid","title":"Atomicity in ACID","text":"<p>Atomicity treats the transaction is a single unit and all operations defined in the transaction executed together as a single unit. It ensures that either all the operations are successfully executed or none of them executed successfully. It is even called as \"All or Nothing\" rule.</p> <p></p> <p>Atomicity means if any part of the transaction is failed due to some reason, it ensures that entire transactions are rolled back to the previous state before transactions start.</p> <p>In Postgres, atomicity can be done in below ways.</p> <ol> <li>WAL (Write Ahead Logging) - It maintains the transaction log and ensures the changes can be done or undone. </li> <li>Transactional Control Commands - Commands can be set to the transactions as boundaries and control the outcome.</li> </ol> <p>Let's do the example.</p> <ol> <li>Create the database and switch to it.</li> </ol> <p></p> <ol> <li>Create the required tables (accounts and transactions).</li> </ol> <pre><code>CREATE TABLE accounts (\naccount_id SERIAL PRIMARY KEY, \naccount_holder_name VARCHAR(255) NOT NULL, \nbalances NUMERIC(15,2) NOT NULL CHECK (balances &gt;= 0)\n);\n\nCREATE TABLE transactions (\ntransaction_id SERIAL PRIMARY KEY, \namount NUMERIC(15,2) NOT NULL, \naccount_id INT NOT NULL REFERENCES accounts (account_id), \ntransaction_type VARCHAR(10) NOT NULL CHECK (transaction_type IN ('Debit','Credit')),\ndate TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre> <p></p> <p></p> <ol> <li>Let's do the successful transaction using a TCC option.</li> </ol> <p><pre><code>BEGIN;\n\nINSERT INTO accounts (account_id, account_holder_name, balances) VALUES (1,'Satheesh',1000);\nUPDATE accounts SET balances = balances - 100 WHERE account_id = 1;\nINSERT INTO transactions (transaction_id, amount, account_id, transaction_type) VALUES (101,800,1,'Debit');\n\nCOMMIT;\n</code></pre> </p> <p>In the above example, all the operations (insert, update and insert) are successfully executed. </p> <ol> <li>Let's do the failure transaction using a TCC option.</li> </ol> <pre><code>BEGIN;\n\nINSERT INTO accounts (account_id, account_holder_name, balances) VALUES (2,'Pandian',2000);\nUPDATE accounts SET balances = balances - 100 WHERE account_id = 2;\nINSERT INTO transactions (transaction_id, amount, account_id, transaction_type) VALUES (101,1800,1,'Debit');\n\nCOMMIT;\n</code></pre> <p></p> <p>In the above example, the first two operations (insert and update) are successfully executed, but the third operation (insert) is failed due to duplicate key value. Hence, all the operations are not successful and restored the database to previous stage.</p>"},{"location":"postgres/acid/#atomicity-with-savepoint","title":"Atomicity with savepoint","text":"<p>Savepoint is similar to snapshot and the operations stored in it. Based on the failure, we can make partial operations to be successful using this savepoint.</p> <pre><code>BEGIN;\n\nINSERT INTO accounts (account_id, account_holder_name, balances) VALUES (3,'Praba',3000);\nSAVEPOINT savepoint1;\n\nUPDATE accounts SET balances = balances - 500 WHERE account_id = 3;\nSAVEPOINT savepoint2;\n\nINSERT INTO transactions (transaction_id, amount, account_id, transaction_type) VALUES (101,2800,1,'Debit');\nROLLBACK TO savepoint1;\n</code></pre> <p></p> <p>In the above example, savepoint 1 is included the insert command and savepoint 2 is included both insert and update commands. When the failure occurs,  we are instructed to roll back up to savepoint 1. This means we are good to go for inserting the record in the accounts table, but not ready to update the balance value. </p>"},{"location":"postgres/acid/#consistency-in-acid","title":"Consistency in ACID","text":""},{"location":"postgres/exercise/","title":"Exercises","text":""},{"location":"postgres/exercise/#db-task-001","title":"DB-TASK-001","text":"<p>1. Retrieve film titles and their rental rates. Use column aliases to rename title as \"Movie Title\" and rental_rate as \"Rate\".</p> <p>SELECT title AS \"Movie Title\", rental_rate AS \"Rate\" FROM film;</p> <p>2. List customer names and their email addresses. Alias first_name and last_name as \"First Name\" and \"Last Name\".</p> <p>SELECT first_name AS \"First name\", last_name AS \"Last Name\" FROM customer;</p> <p>3. Get a list of films sorted by rental rate in descending order. If two films have the same rental rate, sort them alphabetically by title.</p> <p>SELECT title AS \"Movie Title\", rental_rate AS \"Rate\" FROM film ORDER BY \"Rate\" DESC, \"Movie Title\" ASC;</p> <p>4. Retrieve actor names sorted by last name, then first name.</p> <p>SELECT first_name AS \"FIRST NAME\", last_name AS \"LAST NAME\" FROM actor ORDER BY \"LAST NAME\", \"FIRST NAME\";</p> <p>5. List all unique replacement costs from the film table.</p> <p>SELECT DISTINCT replacement_cost FROM film;</p> <p>6. List all films' title and length in minutes. Alias length as \"Duration (min)\".</p> <p>SELECT title AS \"Movie Title\", length AS \"Duration (Mins)\" FROM film;</p> <p>7. Retrieve customer first and last names along with their active status. Alias active as \"Is Active\".</p> <p>SELECT first_name AS \"First name\", last_name AS \"Last name\", activebool AS \"Is Active\" from customer;</p> <p>8. Retrieve the list of film categories sorted alphabetically.</p> <p>SELECT name AS \"FILM CATEGORY\" from category ORDER BY \"FILM CATEGORY\";</p> <p>9. List films by length, sorted in descending order. Include only the title and length.</p> <p>SELECT title AS \"MOVIE TITLE\", length AS \"DURATION (MINS)\" FROM film ORDER BY \"DURATION (MINS)\" DESC;</p> <p>10. Retrieve all actor names, sorted by their first name in descending order.</p> <p>SELECT first_name AS \"FIRST NAME\" FROM actor ORDER BY \"FIRST NAME\" DESC;</p> <p>11. List all unique ratings available in the film table.</p> <p>SELECT DISTINCT rating AS \"RATING\" FROM film;</p> <p>12. Find all unique rental durations from the film table.</p> <p>SELECT DISTINCT rental_rate AS \"RENTAL RATE\" FROM film;</p> <p>13. Retrieve the first unique customer ID based on active status. Include the customer_id and active columns, and order by customer_id.</p> <p>SELECT DISTINCT customer_id AS \"CUSTOMER ID\", active AS \"ACTIVE\" from customer ORDER BY \"CUSTOMER ID\" limit 1;</p> <p>14. List the earliest rental date for each customer. Include customer_id and rental_date, and order by customer_id.</p> <p>SELECT DISTINCT customer_id AS \"CUSTOMER ID\", rental_date AS \"RENTAL DATE\" FROM rental ORDER BY \"RENTAL DATE\" ASC;</p> <p>15. List the 10 shortest films by length. Include the title and length.</p> <p>SELECT title \"MOVIE TITLE\", length \"DURATION\" from film ORDER BY \"DURATION\" limit 10;</p> <p>16. Get the top 5 customers with the highest customer_id. Include the first and last name.</p> <p>SELECT customer_id AS \"CUSTOMER ID\", first_name AS \"FIRST NAME\", last_name AS \"LAST NAME\" from customer ORDER BY \"CUSTOMER ID\" DESC limit 5;</p> <p>17. Retrieve all unique values of store_id from the inventory table.</p> <p>SELECT DISTINCT store_id AS \"STORE ID\" from inventory;</p> <p>18. Find all unique replacement_cost values in the film table. Sort the results in ascending order.</p> <p>SELECT DISTINCT replacement_cost \"REPLACEMENT COST\" FROM film ORDER BY \"REPLACEMENT COST\" ASC;</p> <p>19. List the first rental date for each store. Include store_id and rental_date, and sort by store_id.</p> <p>SELECT rental_date AS \"RENTAL DATE\", store_id AS \"STORE ID\" FROM  ORDER BY \"STORE ID\"; <p>20. Retrieve a list of film ratings sorted alphabetically and include only unique values.</p> <p>SELECT DISTINCT rating AS \"RATING\" FROM film ORDER BY \"RATING\";</p> <p>21. List films by rating in ascending order and length in descending order.</p> <p>SELECT rating AS \"RATING\", length AS \"DURATION\" FROM film ORDER BY \"RATING\" ASC, \"DURATION\" DESC;</p> <p>22. Retrieve actor names sorted by last_name in ascending order and first_name in descending order.</p> <p>SELECT first_name AS \"FIRST NAME\", last_name AS \"LAST NAME\" FROM actor ORDER BY \"LAST NAME\" ASC, \"FIRST NAME\" DESC;</p> <p>23. List films ordered by replacement_cost in ascending order and rental_rate in descending order.</p> <p>SELECT replacement_cost \"REPLACEMENT COST\", rental_rate \"RATE\" FROM film ORDER BY \"REPLACEMENT COST\" ASC, \"RATE\" DESC;</p> <p>24. Retrieve customer names sorted by last_name ascending and first_name descending.</p> <p>SELECT first_name \"FIRST NAME\", last_name \"LAST NAME\" FROM customer ORDER BY \"LAST NAME\" ASC, \"FIRST NAME\" DESC;</p> <p>25. List all rentals sorted by customer_id ascending and rental_date descending.</p> <p>SELECT * FROM rental ORDER BY customer_id ASC, rental_date DESC;</p> <p>26. Retrieve a list of films ordered by rental_duration ascending and title descending.</p> <p>SELECT title \"MOVIE NAME\", rental_duration \"RENTAL DURATION\" FROM film ORDER BY rental_duration ASC, \"MOVIE NAME\" DESC;</p>"},{"location":"postgres/exercise/#db-task-002","title":"DB-TASK-002","text":"<p>1. Get all movies (films) that have a rental rate greater than $3.</p> <p>SELECT title, rental_rate FROM film WHERE rental_rate &gt; 3;</p> <p>2. Get all movies that have a rental rate greater than $3 and a replacement cost less than $20.</p> <p>SELECT title, rental_rate, replacement_cost FROM film WHERE rental_rate &gt; 3 AND replacement_cost &lt; 20;</p> <p>3.Get all movies that are either rated as 'PG' or have a rental rate of $0.99.</p> <p>SELECT title, rating, rental_rate FROM film WHERE rating='PG' OR rental_rate=0.99; </p> <p>4. Show the first 10 movies sorted by rental rate (highest first).</p> <p>SELECT title, rental_rate FROM film ORDER BY rental_rate DESC LIMIT 10;</p> <p>5. Skip the first 5 movies and fetch the next 3 sorted by rental rate in ascending order.</p> <p>SELECT title, rental_rate FROM film ORDER BY rental_rate ASC OFFSET 5 LIMIT 3;</p> <p>6. Skip the first 5 movies and fetch the next 3 sorted by rental rate in ascending order.</p> <p>SELECT title, rental_rate FROM film ORDER BY rental_rate ASC OFFSET 5 FETCH NEXT 3 ROWS ONLY;</p> <p>7. Get all movies with a rental duration between 3 and 7 days.</p> <p>SELECT title, rental_duration FROM film WHERE rental_duration BETWEEN 3 AND 7;</p> <p>8. Get all movies where the title starts with 'A' and ends with 'e'</p> <p>SELECT title FROM film WHERE title LIKE 'A%e';</p> <p>9. Find all customers who do not have an email address listed.</p> <p>SELECT first_name, last_name FROM customer WHERE email is NULL;</p> <p>10. Find all movies released in 2006 with a rental rate of $2.99 or $3.99, and their title starts with 'S'.  Display the top 5 results. SELECT title, rental_rate, release_year</p> <p>SELECT title,rental_rate, release_year FROM film WHERE release_year=2006 AND rental_rate IN (2.99, 3.99) AND title LIKE 'S%' LIMIT 5;</p> <p>11. Display 10 customers after skipping the first 20, sorted alphabetically by last name.</p> <p>SELECT first_name, last_name FROM customer ORDER BY last_name ASC OFFSET 20 LIMIT 10;</p> <p>12. Get the top 5 movies with the highest replacement cost, skipping the most expensive one.</p> <p>SELECT title, replacement_cost FROM film ORDER BY replacement_cost DESC OFFSET 1 FETCH NEXT 5 ROWS ONLY;</p> <p>13. Find all rentals that occurred between '2005-05-01' and '2005-06-01'.</p> <p>SELECT rental_date FROM rental WHERE rental_date BETWEEN '2005-05-01' AND '2005-06-01';</p> <p>14. Get all actors whose last names contain the letters \"man\".</p> <p>SELECT first_name, last_name FROM actor WHERE last_name LIKE '%man%';</p> <p>15. Find all movies where the special features are not listed (i.e., special_features is NULL).</p> <p>SELECT title FROM film WHERE special_features is NULL;</p> <p>16. Find all movies where the rental duration is more than 7 days.</p> <p>SELECT title, rental_duration FROM film WHERE rental_duration &gt; 7;</p> <p>17. Find the first 10 movies with a rental rate of $2.99 or $4.99, a rating of 'R', and a title containing the word \"L\".</p> <p>SELECT title, rental_rate, rating FROM film WHERE rating='R' AND title LIKE '%L%' AND rental_rate IN (2.99, 4.99) LIMIT 10;</p> <p>18. Find all movies where the title starts with \"A\" or \"B\" and ends with \"s\".</p> <p>SELECT title FROM film WHERE title SIMILAR TO '[AB]%s';</p> <p>19.Find all movies where the title contains \"Man\", \"Men\", or \"Woman\".</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%(Man|Men|Woman)%';</p>"},{"location":"postgres/exercise/#bonus-qa","title":"Bonus Q/A","text":"<p>1. Find all movies where the special features are not listed (i.e., special_features is NULL).</p> <p>SELECT title FROM film WHERE special_features is NULL;</p> <p>2. Find all movies where the rental duration is more than 7 days.</p> <p>SELECT title, rental_duration FROM film WHERE rental_duration &gt; 7;</p> <p>3. Find all movies that have a rental rate of $4.99 and a replacement cost of more than $20.</p> <p>SELECT title, rental_rate, replacement_cost FROM film WHERE rental_rate = 4.99 AND replacement_cost &gt; 20;</p> <p>4. Find all movies that have a rental rate of $0.99 or a rating of 'PG-13'.</p> <p>SELECT title, rental_rate, rating FROM film WHERE rental_rate=0.99 AND rating = 'PG-13';</p> <p>5. Retrieve the first 5 rows of movies sorted alphabetically by title.</p> <p>SELECT title FROM film ORDER BY title ASC LIMIT 5;</p> <p>6. Skip the first 10 rows and fetch the next 3 movies with the highest replacement cost.</p> <p>SELECT title, replacement_cost FROM film ORDER BY replacement_cost DESC OFFSET 10 FETCH NEXT 3 ROWS ONLY;</p> <p>7. Find all movies where the rating is either 'G', 'PG', or 'PG-13'.</p> <p>SELECT title, rating FROM film WHERE rating IN ('G', 'PG', 'PG-13');</p> <p>8. Find all movies with a rental rate between $2 and $4.</p> <p>SELECT title, rental_rate FROM film WHERE rental_rate BETWEEN 2 AND 4;</p> <p>9. Find all movies with titles that start with 'The'.</p> <p>SELECT title FROM film WHERE title LIKE 'The%';</p> <p>10. Find the first 10 movies with a rental rate of $2.99 or $4.99, a rating of 'R', and a title containing the word \"Love\".</p> <p>SELECT title, rental_rate, rating FROM film WHERE title LIKE '%Love%' AND rental_rate IN (2.99, 4.99) AND rating='R' LIMIT 10;</p> <p>11. Find all movies where the title contains the % symbol.</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%$%%' ESCAPE '$';</p> <p>12. Find all movies where the title contains an underscore (_).</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%!_%' ESCAPE '!';</p> <p>13. Find all movies where the title starts with \"A\" or \"B\" and ends with \"s\".</p> <p>SELECT title FROM film WHERE title SIMILAR TO '[AB]%s';</p> <p>14. Find all movies where the title contains \"Man\", \"Men\", or \"Woman\".</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%(Man|Men|Woman)%';</p> <p>15. Find all movies with titles that contain digits (e.g., \"007\", \"2\", \"300\").</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%[0-9]%';</p> <p>16. Find all movies with titles containing a backslash ().</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%\\%';</p> <p>17. Find all movies where the title does contain the words \"Love\" or \"Hate\".</p> <p>SELECT title FROM film WHERE title !~~ '%Love%' OR title !~~ '%Hate%';</p> <p>18. Find the first 5 movies with titles that end with \"er\", \"or\", or \"ar\".</p> <p>SELECT title FROM film WHERE title SIMILAR TO '%(er|or|ar)';</p>"},{"location":"postgres/filter/","title":"Filtering Data","text":"<p>To filter the data from the table, we will use <code>WHERE</code> condition. This condition will evaluate and return true or false. The query returns only rows that satisfy the condition in the WHERE clause.</p> <pre><code>SELECT &lt;column name&gt; FROM &lt;table name&gt; WHERE &lt;condition&gt;\n</code></pre> <pre><code>SELECT first_name FROM actor WHERE first_name='Tim';\n</code></pre> <p></p> <p>If you are using the column aliases, you can't use them in the query.</p> <pre><code>SELECT first_name AS \"First name\" FROM actor WHERE 'First name'='Tim'; -&gt; This will throw an error.\n</code></pre> <p>To form the condition in the WHERE clause, you use comparison and logical operators</p> Operator Description = Equal &gt; Greater than &lt; Less than &gt;= Greater than or equal &lt;= Less than or equal &lt;&gt; or != Not equal AND Logical operator AND OR Logical operator OR IN Return true if a value matches any value in a list BETWEEN Return true if a value is between a range of values (range inclusive) LIKE Return true if a value matches a pattern IS NULL Return true if a value is NULL NOT Negate the result of other operators"},{"location":"postgres/filter/#where-clause-with-the-equal-operator","title":"WHERE clause with the equal (=) operator","text":"<pre><code>SELECT first_name FROM actor WHERE first_name='Tim';\n</code></pre>"},{"location":"postgres/filter/#where-clause-with-the-and-operator","title":"WHERE clause with the AND operator","text":"<pre><code>SELECT first_name FROM actor WHERE first_name='Tim' AND last_name='Hackman';\n</code></pre>"},{"location":"postgres/filter/#where-clause-with-the-or-operator","title":"WHERE clause with the OR operator","text":"<pre><code>SELECT first_name, last_name FROM actor WHERE first_name='Tim' OR last_name='Hackman';\n</code></pre>"},{"location":"postgres/filter/#where-clause-with-the-in-operator","title":"WHERE clause with the IN operator","text":"<p><code>Remember that when the column involves date with timestamp, you have to give column::date in the filter condition</code></p> <pre><code>SELECT first_name, last_name FROM actor WHERE first_name in ('Dan','Bob','Sandra','Elvis');\n\nSame query can be written like below \n\nSELECT first_name, last_name FROM actor WHERE first_name='Dan' OR first_name='Bob' OR first_name='Sandra' OR first_name='Elvis';\n</code></pre> <p></p>"},{"location":"postgres/filter/#where-clause-with-the-like-operator","title":"WHERE clause with the LIKE operator","text":"<pre><code>SELECT first_name, last_name FROM actor WHERE first_name LIKE 'An%';\n</code></pre> <p><code>%</code> will match zero or more characters.</p> <p></p> <p><pre><code>SELECT first_name, last_name FROM actor WHERE first_name LIKE 'An_e';\n</code></pre> <code>_</code> will match only one character.</p> <p></p>"},{"location":"postgres/filter/#where-clause-with-the-between-operator","title":"WHERE clause with the BETWEEN operator","text":"<p>BETWEEN includes the range boundary as well. It means that between 3 and 5 includes 3 and 5 value too.</p> <pre><code>SELECT\n  first_name,\n  LENGTH(first_name) \"NAME LENGTH\"\nFROM\n  actor\nWHERE\n  first_name LIKE 'A%'\n  AND LENGTH(first_name) BETWEEN 4\n  AND 6\nORDER BY\n  \"NAME LENGTH\";\n</code></pre> <p></p>"},{"location":"postgres/filter/#where-clause-with-the-not-equal-operator","title":"WHERE clause with the not equal operator","text":"<pre><code>SELECT\n  first_name,\n  last_name\nFROM\n  actor\nWHERE\n  first_name LIKE 'Ang%'\n  AND last_name &lt;&gt; 'Hudson';\n</code></pre>"},{"location":"postgres/filter/#postgressql-and-operator","title":"PostgresSQL AND operator","text":"First Condition Second Condition Result true true true true false false true null null false false false false true false false null false"},{"location":"postgres/filter/#postgres-or-operator","title":"Postgres OR Operator","text":"First Condition Second Condition Result true true true true false true true null true false false false false true true false null null"},{"location":"postgres/filter/#limit","title":"LIMIT","text":"<p><code>LIMIT</code> will filter the number of rows to be displayed from the results returned by query. It is an optional clause.</p> <pre><code>SELECT &lt;column name&gt;\nFROM &lt;table name&gt;\nORDER BY &lt;sort expression&gt;\nLIMIT &lt;row count&gt;;\n</code></pre> <pre><code>SELECT first_name, last_name FROM actor LIMIT 5;\n</code></pre> <p></p>"},{"location":"postgres/filter/#offset","title":"OFFSET","text":"<p><code>OFFSET</code> will skip the number of rows returned from the results returned by query. It is an optional clause.</p> <pre><code>SELECT &lt;column name&gt;\nFROM &lt;table name&gt;\nORDER BY &lt;sort expression&gt;\nLIMIT &lt;row count&gt;\nOFFSET &lt;row count to be skipped&gt;;\n</code></pre> <pre><code>SELECT actor_id, first_name, last_name FROM actor LIMIT 5 OFFSET 5;\n</code></pre> <p></p> <p>Order of execution in the above query is below</p> <ol> <li>Query execution</li> <li>Offset implementation</li> <li>Limit the result after offset implementation</li> </ol>"},{"location":"postgres/filter/#limit-offset-to-get-topbottom-n-rows","title":"LIMIT OFFSET to get top/bottom N rows","text":"<p>TOP rows</p> <p><pre><code>SELECT actor_id, first_name, last_name FROM actor ORDER BY actor_id LIMIT 7 OFFSET 3;\n</code></pre> </p> <p>BOTTOM rows</p> <pre><code>SELECT actor_id, first_name, last_name FROM actor ORDER BY actor_id DESC LIMIT 7 OFFSET 3;\n</code></pre> <p></p>"},{"location":"postgres/filter/#fetch","title":"FETCH","text":"<p>This is similar to <code>LIMIT</code>.</p> <pre><code>SELECT &lt;column name&gt;\nFROM &lt;table name&gt;\nORDER BY &lt;sort expression&gt;\nOFFSET &lt;row to skip&gt; { ROW | ROWS }\nFETCH { FIRST | NEXT } [ row_count ] { ROW | ROWS } ONLY\n</code></pre> <pre><code>SELECT actor_id, first_name, last_name FROM actor OFFSET 5 FETCH NEXT 3 ROWS ONLY;\n\nor \n\nSELECT actor_id, first_name, last_name FROM actor OFFSET 5 FETCH FIRST 3 ROWS ONLY;\n</code></pre> <p></p>"},{"location":"postgres/filter/#postgressql-in-operator-with-a-list-of-dates","title":"PostgresSQL IN operator with a list of dates","text":"<pre><code>SELECT first_name, last_name FROM actor WHERE first_name IN ('Tim', 'Anne');\n</code></pre>"},{"location":"postgres/filter/#between","title":"BETWEEN","text":"<pre><code>SELECT rental_date FROM rental WHERE rental_date::date BETWEEN '2005-05-25' AND '2005-05-26' LIMIT 5;\n</code></pre>"},{"location":"postgres/filter/#postgressql-like-operator","title":"PostgresSQL LIKE operator","text":"<p>Suppose that you want to find customers, but you don\u2019t remember their names exactly.  However, you can recall that their names begin with something like <code>Jen</code>.</p> <p>You can use the PostgresSQL LIKE operator to match the first names of customers with a string using the following query.</p> <pre><code>SELECT\n  first_name,\n  last_name\nFROM\n  customer\nWHERE\n  first_name LIKE 'Jen%';\n</code></pre> <p></p>"},{"location":"postgres/filter/#postgressql-ilike-operator","title":"PostgresSQL ILIKE operator","text":"<p><code>ILIKE</code> is similar to <code>LIKE</code>, but it is case-insensitive. </p> <pre><code>SELECT\n  first_name,\n  last_name\nFROM\n  customer\nWHERE\n  first_name ILIKE 'jEn%';\n</code></pre> <p></p> Operator Equivalent ~~ LIKE ~~* ILIKE !~~ NOT LIKE !~~* NOT ILIKE <p></p> <p></p>"},{"location":"postgres/filter/#similar-to","title":"SIMILAR TO","text":"<p><code>SIMILAR TO</code> operator in PostgresSQL offers a way to search for patterns that follow regular expression rules. The <code>SIMILAR TO</code> operator allows you to match a pattern to a string using regular expression syntax</p> <p><pre><code>value SIMILAR TO pattern    \n</code></pre> Where:</p> <p><code>value</code> is the string or column to be matched.</p> <p><code>pattern</code> is the regular expression pattern that you want to compare against.</p>"},{"location":"postgres/filter/#basic-similar-to","title":"Basic SIMILAR TO","text":"<p>Below query fetches the first name starts with 'J' and followed by any one of the characters mentioned within the bracket and followed by 'n' and any characters.</p> <pre><code>SELECT first_name, last_name\nFROM customer\nWHERE first_name SIMILAR TO 'J(e|o|a|u|i)n%';\n</code></pre> <p></p>"},{"location":"postgres/filter/#similar-to-with-multiple-patterns","title":"SIMILAR TO with Multiple Patterns","text":"<p>Below query fetches the first name starts with either \"J\" or \"M\" and followed by any 3 characters.</p> <pre><code>select first_name from actor where first_name SIMILAR TO '[JM]___' LIMIT 5\n</code></pre> <p></p>"},{"location":"postgres/filter/#similar-to-with-wildcards","title":"SIMILAR TO with Wildcards","text":"<p>Below, a query fetches the first name contains 'l'. Prefix and suffix of 'l' can be anything.</p> <pre><code>select first_name from actor where first_name SIMILAR TO '%l%' LIMIT 7;\n</code></pre> <p></p>"},{"location":"postgres/filter/#escape","title":"ESCAPE","text":"<p><code>ESCAPE</code> clause in SQL is used to define an escape character in a query.  This allows you to escape special characters (like %, _, ', and others) that are typically used for pattern matching or  other special functions.</p>"},{"location":"postgres/filter/#single-quote","title":"Single Quote (')","text":"<pre><code>select title from test where title SIMILAR TO '%''%';\n</code></pre>"},{"location":"postgres/filter/#percent-sign","title":"Percent Sign (%)","text":"<p>If you want to match a literal %, you need to escape it.</p> <pre><code>select * from test where title like '%$%%' escape '$';\n</code></pre> <p></p>"},{"location":"postgres/filter/#underscore_","title":"Underscore(_)","text":"<p>To use an underscore literally (i.e., if you\u2019re searching for an actual underscore in your data), you need to escape it.</p> <pre><code>select first_name from actor where first_name LIKE '!T_%' escape '!';\n</code></pre> <p></p> Character Escape Required Example Single Quote (') Yes 'It''s a movie' Percent Sign (%) Yes (with ESCAPE) LIKE '%!%%' ESCAPE '!' Underscore (_) Yes (with ESCAPE) LIKE '%!_%' ESCAPE '!' Backslash () Yes 'This is a backslash: \\' Square Brackets ([]) Yes (with ESCAPE) LIKE '%[[]%' ESCAPE '!'"},{"location":"postgres/intro/","title":"Introduction to Postgres","text":"<p>Postgres is one of the databases widely used in the market. It is a type of relational database. Basically it is a collection of records in an organized manner. There are different types of database models</p> <ol> <li>Hierarchy structure database model - Parent child like structure</li> <li>Network database model - It is a web of complexity </li> <li>Relational database model - Organized as rows and columns in a table</li> </ol>"},{"location":"postgres/intro/#data-integrity","title":"Data Integrity","text":"<p>There are four different integrities available in the database.</p> <ol> <li>Entity integrity - Uniqueness - Primary key ensures it.</li> <li>Referential integrity - Connection between tables/databases - Foreign key ensures it</li> <li>Domain integrity - Each field should follow some rules. For example, age should be a positive number</li> <li>User defined integrity - It depends on the application. For example, voting age should be 18. If not, it cannot update it in the table.</li> </ol>"},{"location":"postgres/intro/#installation","title":"Installation","text":"<p>As I am using Mac, the installation steps are below</p> <ol> <li>Install homebrew</li> </ol> <p><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"</code></p> <ol> <li>Install Postgres</li> </ol> <p><code>brew install postgresql</code></p> <ol> <li>Start the postgres services</li> </ol> <p><code>brew service start postgresql</code></p> <ol> <li>Verify Postgres installation</li> </ol> <p><code>brew service list</code></p> <ol> <li>Access Postgres</li> </ol> <p><code>psql postgres</code></p>"},{"location":"postgres/intro/#components-installed-during-installation","title":"Components installed during installation","text":"<p>When you install postgres, there are several processes occur to set up the database in the cluster.</p>"},{"location":"postgres/intro/#1-installation-phase","title":"1. Installation Phase","text":"<p>Binary installations such as postgres, psql are installed using the packages like homebrew, yum, apt etc. Configuration files such as postgresql.conf, pg_hba.conf, pg_ident.conf are configured by default.</p>"},{"location":"postgres/intro/#2-initiation-phase","title":"2. Initiation Phase","text":"<p>The <code>initdb</code> command runs in the initiation phase and prepares the environment for Postgres DB cluster. This will create the following components.</p>"},{"location":"postgres/intro/#1-directory-creation","title":"1. Directory creation","text":"<p><code>/var/lib/postgresql/data</code> directory will create automatically if it does not exist. Subdirectories like <code>base, global, pg_wal, pg_stat</code> also create to organize the datafiles and metadata.</p>"},{"location":"postgres/intro/#2-configuration-file-creation","title":"2. Configuration file creation","text":"<p><code>postgresql.conf</code> : This is the main configuration file</p> <p><code>ps_hba.conf</code> : This is for client authentication</p> <p><code>ps_ident.conf</code>: This helps to map OS users to DB users</p>"},{"location":"postgres/intro/#3-creation-of-system-catalog","title":"3. Creation of system catalog","text":"<p>Core system catalog such as <code>pg_class, pg_attribute, pg_database</code> are created to manage the database objects. Essential metadata is initialized.</p>"},{"location":"postgres/intro/#4-default-template-setup","title":"4. Default template setup","text":"<p>There are two default templates (template 0 and template 1). </p> template 0 template 1 This is pristine template used to create other databases This is used to create databases by default. If it is broken, template 0 will be used to restore it You cannot modify template 0 (read only) You can modify as per your need (custom extension, schemas and objects)"},{"location":"postgres/intro/#5-default-databases","title":"5. Default databases","text":"<p><code>postgres</code> : A default database created for DBA <code>postgres</code> : A default user (super user) is created <code>template 0 &amp; template 1</code> : Refer point # 4</p>"},{"location":"postgres/intro/#6-wal-initialization","title":"6. WAL initialization","text":"<p>This ensures that the data recovery and durability. Writing Ahead Logging (WAL) is set up under <code>pg_wal</code>.</p> <p></p>"},{"location":"postgres/intro/#background-processes","title":"Background processes","text":"<p>Once initialized, postgresql runs many different background processes for DBMS.</p> <ol> <li> <p><code>postmaster</code> process: This is the main server process that manages all the sub processes.</p> </li> <li> <p><code>WAL writer</code> process: It handles WAL updates</p> </li> <li> <p><code>Checkpoint</code> process: It helps to write the dirty pages to disk to maintain the data integrity</p> </li> <li> <p><code>Autovacuum</code> process: Automatically manages vacuum and analyse tasks</p> </li> <li> <p><code>Stats collector</code> process: Collects the stats for query optimization</p> </li> <li> <p><code>Archiver</code> process: Archives completed WAL segments when configured</p> </li> </ol> <p></p>"},{"location":"postgres/intro/#role-management","title":"Role Management","text":"<p>Roles are important in postgres as everything is handled using roles. Roles own the database objects and grant permission to perform operation like SELECT, UPDATE, DELETE, etc.</p> <p>Roles can be two types</p> <ol> <li>Login role</li> <li>Non login role</li> </ol> <p>If you need to perform any operation in postgres, you need to assign a role. </p> <p><code>A role with login privilege is called user</code>. </p> <pre><code>CREATE ROLE satheesh WITH LOGIN PASSWORD 'password';\nGRANT developers TO satheesh;\n</code></pre> <p>Now <code>satheesh</code> becomes user.</p> <pre><code>CREATE ROLE pandian ;\nGRANT SELECT, INSERT on MY_TABLE TO pandian;\n</code></pre> <p><code>pandian</code> is a role because it doesn't have a login privilege. This role has a select, insert permission only to my_table. It cannot access any other table, and also it doesn't do anything other than insert, select on my_table.</p>"},{"location":"postgres/intro/#key-points-about-roles","title":"Key Points About Roles","text":"<ol> <li>No Login by Default: Roles can only log in if the LOGIN attribute is set.</li> <li>Inheritance: Roles can inherit permissions from other roles using GRANT. </li> <li>Combining Roles: Users can belong to multiple roles, and their permissions will combine.</li> </ol> <p>Examples for Roles</p> <p>1. Basic Role Without Login</p> <pre><code>CREATE ROLE satheesh;\nGRANT SELECT, INSERT, UPDATE on EMPLOYEE TO satheesh;\n</code></pre> <p>2. Role with Login (User Role)</p> <pre><code>CREATE ROLE pandian WITH LOGIN PASSWORD 'password';\nGRANT SELECT, INSERT, UPDATE on EMPLOYEE TO pandian;\n</code></pre> <p>3. Role with Inheritance</p> <pre><code>CREATE ROLE employee;\nGRANT SELECT on CUSTOMER TO employee;\n\nCREATE ROLE manager;\nGRANT INSERT, UPDATE ON CUSTOMER TO manager;\nGRANT employee TO manager;\n\nCREATE ROLE adhira WITH LOGIN PASSWORD 'password';\nGRANT manager TO adhira;\n</code></pre> <p>Now <code>adhira</code> can perform SELECT, INSERT, and UPDATE operations on the CUSTOMER table because she inherits the permission from employee through a manager role.</p> <p>4. Temporary Role</p> <pre><code>CREATE ROLE consultant WITH LOGIN PASSWORD 'consult123' VALID UNTIL '2025-12-31';\nGRANT SELECT ON CUSTOMER TO consultant;\n</code></pre>"},{"location":"postgres/select/","title":"Querying Data","text":"<p>Before querying the data from the database, we need to create the database in the postgres.</p> <p>Source</p> <p>Sample data is stored at <code>https://github.com/syedjaferk/postgres_sample_database/blob/main/dvd_rental/dvdrental.tar</code> location.</p>"},{"location":"postgres/select/#steps-to-import-the-database","title":"Steps to import the database","text":"<ol> <li>Download the tar file from the given link.</li> <li>Login to the postgres db using <code>psql postgres</code> and create the database <code>dvdrental</code></li> </ol> <ol> <li>Verify if the database is created using <code>\\l</code></li> </ol> <ol> <li>Disconnect the postgres using <code>exit</code></li> </ol> <ol> <li>Load the dataset into the database using <code>pg_restore</code></li> </ol> <ol> <li>Login to the postgres with the database as dvdrental and verify the database is loaded.</li> </ol>"},{"location":"postgres/select/#select-query","title":"SELECT Query","text":"<p>The SELECT statement is an SQL command used to retrieve data from the database.</p> <p>1. Basic SELECT Statement</p> <pre><code>SELECT &lt;column name&gt; FROM &lt;table name&gt;;\n</code></pre> <p><pre><code>SELECT \n*\nFROM\nstore;\n</code></pre> </p> <p>2. SELECT statement with multiple columns</p> <pre><code>SELECT &lt;column name 1, column name 2, ..&gt; FROM &lt;table name&gt;;\n</code></pre> <p><pre><code>SELECT \nstore_id,\nmanager_staff_id,\nlast_update\nFROM\nstore;\n</code></pre> </p> <p>Remember that we should not use <code>*</code> in the select query because it leads to the following issues</p> <p>1. Performance issues : Retrieving all the columns will make database operations slow if the retrieving data is huge.</p> <p>2. Application overhead : If we are retrieving the data which is unnecessary for application operation will lead to more traffic/load to the application.</p> <p>3. Application break : If you are using <code>*</code> in the query, application may break in case of database structure changed after the application deployed in the production.</p> <p>So, if we don't know the table structure, we can use <code>*</code> to find out the table columns. Also, if we are troubleshooting, then we can use <code>*</code> in the select statement. Other than this, we cannot use <code>*</code> in any other situation. It is not a good practice.</p> <p>Example: Instead of this</p> <p><pre><code>SELECT * FROM employees;\n</code></pre> Do this, <pre><code>SELECT id, name, department FROM employees;\n</code></pre></p>"},{"location":"postgres/select/#alias","title":"ALIAS","text":"<p>The main purpose of column aliases is to make the headings of the output more meaningful. This is temporary during the execution of the query.</p> <pre><code>SELECT column_name AS alias_name\nFROM table_name;\n\nSELECT column_name alias_name\nFROM table_name;\n</code></pre> <p>1. Assigning alias to a column</p> <pre><code>SELECT first_name, last_name FROM customer LIMIT 5;\n</code></pre> <pre><code>SELECT first_name AS \"First Name\", last_name Surname FROM customer LIMIT 5;\n</code></pre> <p></p> <p>2. Assigning alias to an expression</p> <p><pre><code>SELECT\n   first_name || ' ' || last_name\nFROM\n   customer LIMIT 5;\n</code></pre> <pre><code>SELECT\n    first_name || ' ' || last_name AS full_name\nFROM\n    customer LIMIT 5;\n</code></pre> </p>"},{"location":"postgres/select/#order-by","title":"Order By","text":"<p>The ORDER BY clause allows you to sort rows returned by a SELECT clause in ascending or descending order based on a sort expression. The ORDER BY uses ASC (Ascending) by default.</p> <pre><code>SELECT\n  column name\nFROM\n  table_name\nORDER BY\n  sort_expression1 [ASC | DESC],\n  sort_expression2 [ASC | DESC];\n</code></pre> <p>1. ORDER BY clause to sort rows by one column</p> <pre><code>SELECT\n  first_name,\n  last_name\nFROM\n  customer\nORDER BY\n  first_name ASC LIMIT 5;\n</code></pre> <p></p> <p>2. ORDER BY clause to sort rows by multiple columns</p> <pre><code>SELECT\n  first_name,\n  last_name\nFROM\n  customer\nORDER BY\n  first_name ASC,\n  last_name DESC;\n</code></pre> <p></p> <p>3. ORDER BY clause to sort rows by expressions</p> <pre><code>SELECT\n  first_name,\n  LENGTH(first_name) len\nFROM\n  customer\nORDER BY\n  len DESC LIMIT 5;\n</code></pre> <p></p>"},{"location":"postgres/select/#select-distinct","title":"SELECT DISTINCT","text":"<p>The SELECT DISTINCT removes duplicate rows from a result set. The SELECT DISTINCT clause can be applied to one or more columns in the select list of the SELECT statement.</p> <pre><code>SELECT\n  DISTINCT column1\nFROM\n  table_name;\n</code></pre> <p>1. SELECT DISTINCT with one column</p> <pre><code>SELECT DISTINCT rental_rate FROM film;\n</code></pre> <p></p> <p>2. SELECT DISTINCT on multiple columns</p> <pre><code>SELECT DISTINCT rental_rate, amount FROM film;\n</code></pre>"},{"location":"terraform/basics/","title":"Terraform Basics","text":"<p>When we run the <code>terraform init</code> command in a directory contains configuration file,  terraform downloads the plugin and install them for the providers mentioned (AWS/Azure/GCP/Ali) in the configuration file.</p> <p>Terraform plugins are available in Hashicorp registry  https://registry.terraform.io  for all the providers. There are three different providers.</p> <ol> <li>**Official Providers**: Owned and maintained by Hashicorp</li>  **Example**: AWS, Azure, GCP and Local providers <li>**Partner Providers**: Owned and maintained by third party company  that has partner provider access with Hashicorp</li> **Example**: Heroku, F5, Digital Ocean <li>**Community Providers**: Published and maintained by individual contributors of Hashicorp community</li> **Example**: NetappGCP, Active Directory </ol> <p>When we run the command <code>terraform init</code>, it will show the version of the plugin is installed.</p> <p></p> <p>In the above snapshot, <code>hashicorp/local</code> is known as source address. This is used to identify the provider. After identifying the provider, it will download the plugins from the provider and install it. </p> <p>terraform init command is a safe command, and it can be run as many times needed without impacting the actual infrastructure.</p> <p>All plugins are downloaded into <code>&lt;working directory&gt;/.terraform</code> directory. When we download the plugins  (during <code>terraform init</code> command), this will tell you where the plugins are from.</p> <p><code>&lt;Hostname&gt;/&lt;Organisation namespace&gt;/&lt;Provider name&gt;</code></p> <p>Whereas,</p> <p>Hostname is the name of the registry where the plugin is located, and it is optional.  If it is not mentioned, then it is default value (registry.terraform.io)</p> <p>Organisation namespace is Hashicorp/Partner Provider/Community Provider</p> <p>Provider name is AWS/Azure/GCP/Local etc.,</p> <p>Configuration Directory</p> <p>Terraform considers the file with <code>.tf</code> extension is a configuration file.  A single configuration file can have any number of configuration details (Like main.tf has multiple resource configurations).</p> <p>Also, we can have any number of configuration files as well in a directory (Like cat.tf, dog.tf and main.tf).  There are few other common configuration files created in a directory.</p> Filename Purpose main.tf main configuration file variables.tf Contains variable declaration outputs.tf Contains output from resources providers.tf Contains provider definitions <p>Multiple Providers</p> <p>You can create a configuration file which contains multiple providers like below.</p> <p></p> <p></p> <pre><code>NOTE: Let us assume that there are two configurations defined in the configuration file. \nBoth have different plugins and one of the plugin is already installed and other one is not yet installed \n(Meaning terraform init command has executed for only one plugin)\nIn that case, if you run terraform plan / terraform apply command will yield an error. \nThis is because whenever we add a resource for a provider that has not been used so far in the configuration directory, \nwe have to initialize the directory by running terraform init command.\n</code></pre> <p>Input Variables</p> <p>For re-usability, we can use variables. To do so, we can create a variable.tf file and update all variables in it. In general, whatever arguments we are passing in block, we can use variable for those arguments. Remember, both variable configuration file and your main configuration file should be in the same directory.</p> <p>Syntax <pre><code>variable &lt;VARIABLE_NAME&gt; {\n    default = \"&lt;value&gt;\"\n}\n</code></pre></p> <p>Example</p> <p><pre><code>resource \"local_sensitive_file\" \"games\" {\n    filename = \"/Users/satheeshpandian.j/Documents/Sats/Learning/Terraform/names.txt\"\n    content = \"I am Satheesh and this is terraform update example\"\n}\n</code></pre> In the above code snippet, we can use variable for filename and content.</p> <pre><code>variable \"filename\" {\n    default = \"/Users/satheeshpandian.j/Documents/Sats/Learning/Terraform/names.txt\"\n}\n\nvariable \"content\" {\n    default = \"I am Satheesh and this is terraform update example\"\n}\n</code></pre> <p>Now, our main configuration file \"main.tf\" needs to be updated like below</p> <pre><code>resource \"local_sensitive_file\" \"games\" {\n    filename = var.filename\n    content = var.content\n}\n</code></pre> <p>In case you want to change the content or filename, you just need to update it in variable.tf configuration file. You DO NOT touch main configuration file at all.</p> <p>Variable Types</p> <p>In variable block, there are different parameters used.</p> <pre><code>variable &lt;VARIABLE_NAME&gt; {\n    default = &lt;value&gt;\n    type = &lt;ANY/STRING/NUMBER/BOOL/LIST/MAP/OBJECT/TUPLE&gt;\n    description = &lt;description of the value&gt;\n}\n</code></pre> <p>Whereas,</p> <p>type and description are optional. If you don't use <code>type</code> parameter, it will consider as <code>any</code> by default. If you use <code>type</code> parameter, then it will make sure that default value should have the same type.</p> <p>List</p> <p>List will be used with its index and index starts from <code>0</code> <pre><code>variable \"names\" {\n    default = [\"Satheesh\", \"Pandian\", \"Jeganathan\", \"Bangalore\", \"Karnataka\"]\n    type = list\n    description = \"List of names\"\n}\n</code></pre></p> <pre><code>resource \"random_pet\" \"players\" {\n    firstname = var.names[1]\n}\n</code></pre> <p>We can enforce the list values using STRING/NUMBER/BOOL.</p> <p><pre><code>variable \"names\" {\n    default = [\"Satheesh\", \"Pandian\", \"Jeganathan\", \"Bangalore\", \"Karnataka\"]\n    type = list(string)\n    description = \"List of names\"\n}\n</code></pre> This will ensure that all the default values should be <code>string</code>. It will throw an error if you pass the number or boolean value.</p> <pre><code>variable \"names\" {\n    default = [5, 7, 8, 10, 23]\n    type = list(number)\n    description = \"List of numbers\"\n}\n</code></pre> <p></p> <p></p> <p></p> <p>This will ensure that all the default values should be <code>number</code>. It will throw an error if you pass the string or boolean value.</p> <p></p> <p></p> <p>Map</p> <p>Map will be in key/value format. </p> <pre><code>variable \"names\" {\n    default = {\n        \"firstname\" = \"Satheesh\"\n        \"lastname\" = \"Pandian\"\n    }\n    type = map\n    description = \"map of names\"\n}\n</code></pre> <pre><code>resource \"random_pet\" \"players\" {\n    lastname = var.names[\"lastname\"]\n}\n</code></pre> <p>We can enforce the map values using STRING/NUMBER/BOOL.</p> <p><pre><code>variable \"names\" {\n        default = {\n        \"firstname\" = \"Satheesh\"\n        \"lastname\" = \"Pandian\"\n    }\n    type = map(string)\n    description = \"Map of names\"\n}\n</code></pre> This will ensure that all the default values should be <code>string</code>. It will throw an error if you pass the number or boolean value.</p> <pre><code>variable \"names\" {\n        default = {\n        \"favourite\" = 5\n        \"birthdate\" = 10\n        \"anniversary\" = 23\n    }\n    type = map(number)\n    description = \"Map of numbers\"\n}\n</code></pre> <p>Sets</p> <p>This is similar to list except that list can have duplicate elements whereas sets won't allow duplicate elements.</p> <p></p> <p>Duplication example</p> <p></p> <p></p> <p>Object</p> <p>Object is a custom variable type. You can have combination all other variable type using object.</p> <p><pre><code>variable \"player\" {\n     default = {\n        \"name\" = \"Satheesh\"\n        \"age\" = 20\n        \"favourite_place\" = [\"Bangalore\", \"Chennai\", \"Mumbai\"]\n        \"working_location\" = {\n            \"state\" = \"Karnataka\"\n            \"city\" = \"Bangalore\"\n        }\n     }\n     type = object({\n        \"name\" = string\n        \"age\" = number\n        \"favourite_place\" = list(string)\n        \"working_location\" = map(string)    \n     })\n    description = \"Map of numbers\"\n}\n</code></pre> </p> <p></p> <p></p> <p>Tuples</p> <p>This is similar to list except that tuples can have all variable type.</p> <pre><code>variable \"names\" {\n    default = [5, 7, \"Satheesh\", true, 4]\n    type = tuple([number, number, string, bool, number])\n    description = \"Tuple example\"\n}\n</code></pre> <p>Ways to use variable in terraform</p> <ol> <li> <p>We can create <code>variable.tf</code> file and put all the variables and then use it in <code>main.tf</code>.  You can refer all the above cases for this type of usage.</p> </li> <li> <p>During runtime via terminal (Interactive Mode):</p> <p>To do this, we need to create a <code>variable.tf</code> file. But we don't need to update any variable, just keep at empty. <pre><code>variable \"filename\" {\n}\nvariable \"content\" {\n}\n</code></pre> </p> </li> </ol> <p></p> <ol> <li> <p>Via command line arguments</p> <p>To do this, we need to create a <code>variable.tf</code> file. But we don't need to update any variable, just keep at empty. <pre><code>variable \"filename\" {\n}\nvariable \"content\" {\n}\n</code></pre> </p> </li> <li> <p>Via environmental variables</p> <p>To do this, we need to create a <code>variable.tf</code> file. But we don't need to update any variable, just keep at empty. <pre><code>variable \"filename\" {\n}\nvariable \"content\" {\n}\n</code></pre></p> </li> </ol> <p></p> <ol> <li> <p>Via variable definition files</p> <p>Variables are loaded from <code>terraform.tfvars</code> file. This filename can be anything, but the extension should be <code>terrform.tfvars</code> or <code>terraform.tfvars.json</code> or <code>.auto.tfvars</code> or <code>.auto.tfvars.json</code>. Then it will be autoloaded.  In case, if you have variable definition file named as <code>satheesh.tfvars</code>, then you need to pass this via command line arguments.</p> </li> </ol> <pre><code>terraform plan -var-file satheesh.tfvars\n</code></pre> <p>To do this, we need to create a <code>variable.tf</code> file. But we don't need to update any variable, just keep at empty.</p> <pre><code>variable \"filename\" {\n}\nvariable \"content\" {\n}\n</code></pre> <p><pre><code>filename=\"./test.txt\"\ncontent=\"This is Satheesh\"\n</code></pre> </p> <p></p> <p>Variable Definition Precedence</p> <p>The highest priority to lowest is as follows:</p> Highest Precedence Order Way of passing variable to terraform script 1 command line arguments (Ex : <code>-var \"filename=./test.txt\"</code>) 2 .auto.tfvars or .auto.tfvars.json (* \u2192 can be anything.  If there is more than one file, then it will be applicable by alphabetical order) 3 terraform.tfvars or terraform.tfvars.json 4 environmental variables <p>NOTE: Environmental variable has the lowest precedence compared to all other options. </p> <p>Resource Attributes</p> <p>We want to get the value from one configuration (output of it) and pass it to another configuration, this can be achieved.</p> <p></p> <p></p> <p>Here, <code>local_file</code> resource depends on <code>random_pet</code> resource. So, terraform will create the resource for random_pet first and then local_file. While destroying, <code>local_file</code> will be deleted first and then <code>random_pet</code>. This will be taken care by terraform itself. This is known as implicit dependency.</p> <p>We can specifically mention the dependency in the configuration file like below. This is called as explicit dependency. <pre><code> resource \"local_file\" \"my-file\" {\n       filename = \"./test.txt\"\n       content = \"This is Satheesh\"\n       depends_on = [random_pet.my-pet-name.id]\n}\n\nresource \"random_pet\" \"my-pet-name\" {\n       prefix = \"Dr\"\n       separator = \".\"\n       length = 2\n} \n</code></pre></p> <p>Output Variable</p> <p>Output variables can be used to store the value of an expression. This can be passed into another configuration file as an input.</p> <p>Syntax</p> <pre><code>output &lt;OUTPUT_VARIABLE&gt; {\n   value = &lt;output value from the configuration file&gt;\n}\n</code></pre> <p>Example</p> <pre><code> resource \"local_file\" \"my-file\" {\n       filename = \"./test.txt\"\n       content = \"This is ${random_pet.my-pet-name.id}\"\n}\n\nresource \"random_pet\" \"my-pet-name\" {\n       prefix = \"Dr\"\n       separator = \".\"\n       length = 2\n} \n\noutput \"pet-name\" {\n   value = random_pet.my-pet-name.id\n   description = \"This is output value of random_pet and passed as an input to local_file\"\n}\n</code></pre> <p></p> <p></p> <p><code>terraform output</code> command will print all the output variable in the configuration file.</p> <p>Assume that in the configuration file, there are many output variables. You would like to see only one output variable among 5.  In that case, run the below command</p> <p><code>terraform output &lt;variable_name&gt;</code></p> <p>Example: <code>terraform output pet-name</code></p> <p></p>"},{"location":"terraform/commands/","title":"Terraform Basic Commands","text":"<p>Terraform Validate</p> <p>This command helps you to validate the configuration file. It will validate all the configuration files available in a  directory and tells you if there is any issue. If all the files are properly updated, then it will tell you th success message. Remember, this will validate only the syntax</p> <p>Syntax</p> <pre><code>terraform validate\n</code></pre> <p></p> <p></p> <p>Terraform Format</p> <p>This command helps you to format the configuration file in canonical format. This is mainly useful for readability.</p> <p>Syntax</p> <pre><code>terraform fmt\n</code></pre> <p></p> <p></p> <p>Terraform Show</p> <p>This command helps you to show the current state of the infrastructure resource including all attributes created by Terraform.</p> <p>Syntax</p> <pre><code>terraform show\n</code></pre> <p></p> <p><code>terraform show -json</code> command will show the current state of the infrastructure resource in JSON format</p> <pre><code>terraform show -json\n</code></pre> <p></p> <p>Terraform Provides</p> <p>This command helps you to list out all the providers used in the configuration directory.</p> <p>Syntax</p> <p><pre><code>terraform providers\n</code></pre> </p> <p>Terraform Provides Mirror</p> <p>This command helps you to copy provider plugins required for the current configuration to another directory.</p> <p>Syntax</p> <p><pre><code>terraform providers mirror &lt;NEW DIR PATH&gt;\n</code></pre> </p> <p></p> <p>In the above example, all the provider plugins are copied from <code>test</code> directory to <code>new_dir</code> directory.</p> <p>Terraform Refresh</p> <p>This command helps you to sync terraform with real world infrastructure.</p> <p>Syntax</p> <pre><code>terraform providers mirror &lt;NEW DIR PATH&gt;\n</code></pre> <p>Terraform Refresh</p> <p>This command helps you to refresh the configuration file if any changes made.</p> <p>Syntax</p> <p><pre><code>terraform refresh\n</code></pre> </p> <p>Terraform Graph</p> <p>This command helps you to create a visual representation of the dependencies and configuration. Output is graph generated in a DOT format.</p> <p>Syntax</p> <pre><code>terraform graph\n</code></pre> <p>If we have installed graphviz, then we can generate a graph which tells the dependencies and configuration details in diagram.</p> <p></p> <pre><code>terraform graph | dot -Tsvg &gt; graph.svg\n</code></pre> <p><code>graph.svg</code> can be viewd in any browser.</p> <p></p> <p>Mutable Vs Immutable</p> <p>If a software wants to upgrade from one version to another version, it can be done via manual approach, ansible, or shell script. But this can be done during maintenance window (if there is only server). In case, if we are using multiple server, we can  upgrade the server one by one for HA. This is called as in place update. This is mutable infrastructure. </p> <p>If there are multiple servers in the infrastructure, and each server has different version of same software. Then this is called  as Configuration drift.</p> <p>If there are multiple servers in the infrastructure and while upgrading the software, we spin up new server with new version and  once that is in place and working, we can destroy one of the old server which running with old version. Similar approach can be taken for other servers too. This is called as Immutable infrastructure. Remember, while upgrading  if any failures occur, then old server will still be in place. Terraform follows immutable approach. However, there is only one difference. By default, Terraform first destroy the resources first and then will create a new resource . However, it can be changed via lifecycle rules.</p>"},{"location":"terraform/iac/","title":"Infrastructure As Code (IAC)","text":"<p>In modern world technologies, provisioning infrastructure is the biggest challenge. There are lots of cloud providers who can help to provide resources such as virtual machines, networking, databases and storage devices etc., You can use any one of them and build/create your infrastructure based on your business need.</p> <p>There are two ways you can obtain their resource provisioning.</p> <ol> <li> Management Console</li> <li> Through coding</li> </ol> <p>Out of these two, the better way is to code the entire provisioning process. The reason for this is that you can write the code to define provision, configure, update and destroy all resources whenever you don't need them, and you can redo this process anytime without manual intervention. Whereas, in the management console, you need to do them manually every single time which is not the correct way. This is called Infrastructure As Code (IAC).</p> <p>With IAC, we can define infrastructure resources using simple, human-readable  high level language. </p> <p>Types of IAC tools</p> <ol> <li> Configuration Management</li> <ul> <li>Ansible</li> <li>Puppet</li> <li>Saltstack</li> <li>Chef</li> </ul> <li> Server Templating</li> <ul> <li>Docker</li> <li>Vagrant</li> <li>Packer</li> </ul> <li> Provisioning Tools</li> <ul> <li>Terraform</li> <li>Cloud Formation</li> </ul> </ol> <p>1.Configuration Management</p> <ol> <li>These are used to install and manage the software on existing infrastructure resources. </li> <li>It maintains the standard structure of the code (Not like we write the code for our need) and it can be reused. </li> <li>It designed to run in multiple resources at once (You can create and execute resources in multiple machine using single command).</li> <li>Version control options (You can update/upload your ansible playbook/role into version control repo [ansible galaxy]).</li> <li>They are idempotent. It means if the resources are already exist, we don't need to create them again. Those creating code can be skipped easily (For this, we don't need to write any logic, it can be handled by default) and move to next steps/process.</li> </ol> <p>2.Server Templating</p> <ol> <li>It is used to create a custom images of VMs/containers. These VMs/containers are already preinstalled all required software and dependencies.</li> <li>Immutable infrastructure </li> <li>Once you deployed a VM/container, you CANNOT change their config. It remains unchanged. (This can be possible in configuration management tools such as Ansible)</li> <li>If you need to change, you can update the image and redeploy.</li> </ol> <p>3.Provisioning Tools</p> <ol> <li>These tools are used to provisioning the infrastructure components (can be servers/DBs/NW devices/storages/security groups/VPCs/services) using simple code.</li> </ol>"},{"location":"terraform/intro/","title":"Terraform","text":"<p>Terraform is a IAC tool, specifically used for provisioning.  It is an open source tool developed by Hashicorp. It is allowing us to build image, manage and destroy infrastructure in few minutes.  The biggest advantages of terraform is that it can support multiple platforms including private and public cloud providers such as Azure/AWZ/GCP/VMware/Physical machines.</p> <p>Providers (AWS/Azure/GCP etc.,) helps terraform to manage the third party platforms through their APIs.  Terraform uses HCL(Hashicorp Configuration Language) which is very simple language to define the infrastructure resources to be provisioned as a block of code. All infrastructure resources are defined in configuration files with <code>tf</code> extension. The source code can be maintained in source control system. So, it can be distributed to other teams as well.</p> <p><code>NOTE: Remember, every objects that terraform manages known as resources.  Terraform manages the object's entire life cycle starting from provisioning to destroying</code></p> <p>Terraform has three phases</p> <ul> <li>Init</li> <li>Plan</li> <li>Apply</li> </ul> <p>In <code>init</code> phase, terraform initiates the project and identify the providers. This provider will be used in the targeted environment.</p> <p>In <code>plan</code> phase, terraform creates the plan to get the resources to target state.</p> <p>In <code>apply</code> phase, terraform makes necessary changes to the plan to get the resources to desired state in the target environment.</p> <p>Terraform maintains the record that state of the infrastructure. Based on this record, it can decide what action can be taken when updating the resources in particular platform such as AWS and Azure.</p> <p>It is Terraform's responsibility to maintain the defined state for all the resources in the code at all time.</p> <p>Terraform can read the attribute of the existing resources by configuring datasource. This can be used to configure other datasource within Terraform. It can also import other resources outside of Terraform and manages them.</p>"},{"location":"terraform/intro/#hashicorp-configuration-language-hcl","title":"HashiCorp Configuration Language (HCL)","text":"<p>HCL consists of blocks and arguments. <pre><code>&lt;BLOCK&gt; &lt;PARAMETERS&gt; {\n    &lt;ARGUMENTS&gt;\n}\n</code></pre> Arguments will be in key value format and represents configuration data. <pre><code>&lt;BLOCK&gt; &lt;PARAMETERS&gt;{\n    &lt;KEY1&gt;=&lt;VALUE1&gt;\n    &lt;KEY2&gt;=&lt;VALUE2&gt;\n}\n</code></pre> <code>Block</code> contains the information of infrastructure platform and set of resources to be created within the platform.</p> <p><code>Block</code> contains <code>resource name</code> and <code>resource type</code>.</p> <p>Example</p> <p>Let us see an example to create a file in local laptop/desktop using terraform.</p> <pre><code>resource \"local_file\" \"names\" {\n    filename = \"/Users/satheeshpandian.j/Documents/Sats/Learning/Terraform/names.txt\n    content = \"Satheesh \\nPandian \\nJeganathan\"\n}\n</code></pre> <p>Explanation</p> <p><code>resource \"local_file\" \"names\"</code> =&gt; Block</p> <p>Below is the argument <code>filename = \"/Users/satheeshpandian.j/Documents/Sats/Learning/Terraform/names.txt\" content = \"Satheesh \\nPandian \\nJeganathan\"</code></p> <p>Type of the block is <code>resource</code> which is identified by keyword <code>resource</code> in the beginning of the block.</p> <p><code>local_file</code> =&gt; <code>local</code> is provider + <code>file</code> is type of resource</p> <p><code>local_file</code> =&gt; Resource type we want to create. This is a fixed value and will be changed based on the provider.</p> <p><code>names</code> =&gt; Resource name. It can be anything.</p> <p>As it is file type of resource and needs to be a local type provider, <code>filename</code> is the mandatory argument.  Refer this link</p> <p></p> <p>Now, we need to run the file (local-file-creation.tf) we created.  To do so, we need to initialize the project using <code>terraform init</code> command</p> <p></p> <p>Next, we need to ensure if whatever resource needs to be created, it will be there. To check this, you need to run <code>terraform plan</code> command</p> <p></p> <p>If you observe the above output, you can see the filename, along with content (NOT VISIBLE as it is not yet applied), file permission etc.  Terraform will create additional attributes as an arguments alone with user provided attributes.</p> <p></p> <p></p> <p></p> <p>Now, let us update the resource we created (local file).</p> <pre><code>resource \"local_file\" \"names\" {\n    filename = \"/Users/satheeshpandian.j/Documents/Sats/Learning/Terraform/names.txt\"\n    content = \"I am Satheesh and this is terrform update example\"\n}\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Now, let us destroy the resource we created (local file).</p> <p></p> <p></p> <p>To hide the file content during <code>terraform plan / terraform apply</code>, use the below code <pre><code>resource \"local_sensitive_file\" \"games\" {\n    filename = \"/Users/satheeshpandian.j/Documents/Sats/Learning/Terraform/names.txt\"\n    content = \"I am Satheesh and this is terraform update example\"\n}\n</code></pre></p> <p>For AWS Provider <pre><code>resource \"aws_instance\" \"app_server\" {\n  ami           = \"ami-830c94e3\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"ExampleAppServerInstance\"\n  }\n}\n</code></pre></p> Provider Link AWS https://registry.terraform.io/providers/hashicorp/aws/latest Azure https://registry.terraform.io/providers/hashicorp/azurerm/latest Google https://registry.terraform.io/providers/hashicorp/google/latest"},{"location":"terraform/state/","title":"Terraform State","text":"<p>When you run <code>terraform apply</code> command, the state file is created. If this command is NOT executed once, then the state file WILL NOT create at all. When we are running the command <code>terraform apply</code>, terraform refresh the state in memory and see if there is any resource created with same configuration. If it was already created, then the resources will not be created. Else, it will create with new <code>id</code>. The state file is JSON data structure, and it has complete information about the infrastructure created by terraform.</p> <p>This state file will be used by <code>terraform plan</code> and <code>terraform apply</code> commands to compare the existing infrastructure information and take the necessary action accordingly. When terraform creates the resource, it records all the information in the state. All resources created by terraform would have a unique id which is used to identify the resource in the real world. Also, state file tracks the resource dependency as well.</p> <p>If any resource is depending upon another resource, then it will NOT create until dependant resource created. This will improve performance as it refers local state file and skip if the resources are already available (Terraform store caches of attribute values).  We can inform terraform that it can only refer the local state file using the below command <pre><code>terraform plan --refresh = false\n\nOR\n\nterraform apply --refresh = false\n</code></pre></p> <p>Note that state file contains all the information including sensitive information. Hence, you need to make sure that it will not be kept for public or source control system (GitHub/bitbucket). Instead, it can be stored at AWS S3, Google cloud storage etc.,  Remember that we should NOT edit the state file. If we need to edit (if it is mandatory), we need to use terraform command.</p>"},{"location":"terraform/terraform_in_aws/","title":"Terraform in AWS","text":""},{"location":"terraform/terraform_in_aws/#introduction-to-iam-identity-and-access-management","title":"Introduction to IAM (Identity and Access Management)","text":"<p>When we create users in AWS, there are two types of access that we can configure to the users.</p> <p>1. Management Console Access : User should have valid username and password to access 2. Programmatic Access: User should have access key id and secret access key and using via CLI. But they CANNOT access management console.</p> <p>When the user created,we need to assign the permission via IAM policy in AWS. This policy tells you that what user can do or cannot do.</p>"},{"location":"terraform/traditional-it/","title":"Challenges with Traditional IT Infrastructure","text":"<p>Imagine you are building an application for your business. This application has to be hosted for customers to use. Your business team gathers the business requirements and passes it to business analyst. Then, business analyst analyzes the requirements and transforms them to technical specification. Your solution architect reads the technical specification and creates the infrastructure, designs the deployment architecture for your application to be hosted.  This design includes the number of servers required, type of servers, configuration, quantity, etc.</p> <p>Once the request for server procurement to be created, it takes a few weeks to a few months to be arrived. After arriving the servers, multiple teams need to be involved to set up the configuration, networking and, storages, etc. Manually will do this.</p>"},{"location":"terraform/traditional-it/#key-challenges-of-the-traditional-deployment-model","title":"Key challenges of the traditional deployment model","text":"<p>1. Turn around time - Takes a few weeks to a few months to be set up the complete infrastructure  2. Scalability - Scaling the infrastructure whenever required is really difficult  3. High cost - All the server procurement cost, maintenance cost are high 4. Under utilization - Some servers are not fully used. Remaining resources are waste. 5. Human error - As a multiple team involved to set up the environment, high chances of human error is possible</p> <p></p>"},{"location":"terraform/work_terraform/","title":"Lifecycle Rules","text":"<p>By default, Terraform first destroys the resources first, and then will create a new resource.</p> <p>Example</p> <p>You have local file creating configuration file your directory, and the resource was provided already. Now, you want to make changes in the content of the file like below <pre><code>resource \"local_file\" \"my-file\" {\n    filename=\"./test.txt\"\n    content=\"Satheesh Pandian\" // earlier it is \"Satheesh\"\n}\n</code></pre></p> <p>When you run <code>terraform apply</code> command. It destroys the existing file first, and then it will create a new one.</p> <p></p> <p>Create first and Destroy then</p> <p>If you want to change this behavior, you can use lifecycle rules.</p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename=\"./test.txt\"\n    content=\"My SRE Journey\" // earlier it is \"Satheesh Pandian Jeganthan\"\n\n    lifecycle {\n        create_before_destroy = true\n    }\n}\n</code></pre> <p></p> <p>Do not destroy</p> <p>If you DO NOT want to destroy any resource, then you need to use the below lifecycle rules. This will be used when you deal with a database. Most of the time, we DO NOT want to delete a database while changing something.  So, you need to update lifecycle rule with <code>prevent_destroy = true</code>.</p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename=\"./test.txt\"\n    content=\"My SRE Journey\" // earlier it is \"Satheesh Pandian Jeganthan\"\n\n    lifecycle {\n        prevent_destroy = true\n    }\n}\n</code></pre> <p></p> <p><code>terraform destroy</code> command still destroys the resource even if you mentioned lifecycle rule with <code>prevent_destroy = true</code>  in the configuration file.</p> <p>Ignore changes</p> <p>If you DO NOT want to create any resource in case of any particular attribute change. In the below example, I am making changes in <code>content</code></p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename=\"./test.txt\"\n    content=\"Satheesh Pandian Jeganthan\" // earlier it is \"I am SRE in Bangalore\"\n\n    lifecycle {\n        ignore_changes = [content] // you can add any number of attribute in the list\n    }\n}\n</code></pre> <p></p> <p><code>ignore_changes = all</code> means that it won't create any resource even if there is any change in any attribute.</p> <p>QUICK SUMMARY</p> <p></p> <p>It is better NOT to use <code>create_before_destroy = true</code> lifecycle rule for a local file  because the rule will create the local file first and the same file to be destroyed during the recreate operation.</p> <p>Data Sources</p> <p>If you want to read the data from any file that is in your current configuration directory, but not created/maintained by terraform. In that case, you can use that file as a data resource.</p> <pre><code>&gt;cat data.txt\nI am Satheesh from Bangalore and I am working as Sr.SRE in a bank.\n</code></pre> <p>The above file is created manually without using terraform commands. Also, terraform does not have any information  about this file so far.</p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename = \"./test.txt\"\n    content = data.local_file.my-data.content\n    lifecycle {\n        create_before_destroy = true\n    }\n}\n\ndata \"local_file\" \"my-data\" {\n    filename=\"./data.txt\"\n}\n</code></pre> <p></p> <p>Remember, you CANNOT do create/update/destroy for data sources. You can ONLY read the data source</p> <p>Meta Argument - Count</p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename = \"./test.txt\"\n    content = \"Satheesh\"\n    count = 3\n}\n</code></pre> <p></p> <p>However, the problem is here that there is ONLY one file created instead of 3. This is because the filename  is not unique. Hence, terraform is recreated the same file again and again.</p> <p></p> <p>Hence, we need to create three different files. Here, I used a variable configuration file for creating three files and modified configuration file to read the filename from variable configuration. <pre><code>variable filename {\n    default = [\"./test.txt\",\"./sample.txt\",\"./exam.txt\"]\n}\n</code></pre></p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename = var.filename\n    content = \"Satheesh\"\n    count = 3\n</code></pre> <p></p> <p></p> <p>In the above example, even if you have 100 arguments in a variable configuration file, this will ALWAYS create three files as we hardcoded <code>count = 3</code>. To create the number of files defined in a variable configuration file, we need to update the main configuration file to pick the length of the list variable.</p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename = var.filename\n    content = \"Satheesh\"\n    count = length(var.filename)\n</code></pre> <p>For - Each</p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename = each.value\n    for_each = var.filename\n    content=\"\"\n</code></pre> <pre><code>variable \"filename\" {\n    default = [\"./test.txt\",\"sample.txt\",\"goal.txt\"]\n}\n</code></pre> <p>We can create three files using <code>for_each</code> meta argument as well. However,  <code>for_each</code> is expecting a set or map type in the variable configuration file. It will not work with list type variable.  Hence, a variable file needs to be updated like below</p> <pre><code>variable \"filename\" {\n    type=set(string) # should be added to convert the list type to set type\n    default = [\"./test.txt\",\"sample.txt\",\"goal.txt\"]\n}\n</code></pre> <p>or </p> <pre><code>resource \"local_file\" \"my-file\" {\n    filename = each.value\n    for_each = toset(var.filename)\n    content=\"\"\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>As seen in the snapshot, there are three files have been created.</p> <p>We can do the same as below as well.</p> <p></p> <p></p> <p></p> <p>As seen in the snapshot, there are three files have been created.</p> <p>Version Constraints</p> <p>This will be helpful to use the specific version of the specific provider.</p> <p>When you run <code>terraform init</code> command, this will automatically download the latest version of the provider plugin that are needed for the configuration file. Sometimes, the functionality of the provider plugin will vary between the versions. In that case, we need to download a specific provider plugin to create the resources using a terraform configuration file.</p> <p>For example, the local_file resource provider latest version (at the time of writing) is 2.5.3.</p> <p></p> <p>In case, if we want to use the local_file resource provider version 2.5.0, we need to use the below code snippet</p> <pre><code>terraform {\n  required_providers {\n    local = {\n      source = \"hashicorp/local\"\n      version = \"2.5.0\" ## Specific version to be mentioned here\n    }\n  }\n}\n</code></pre> <p>Example</p> <p></p> <p></p> <p></p> <p>We can use the version value in different ways.</p> Value Description version=\"!=2.5.0\" Any version other than 2.5.0 would be okay to download version=\"&lt;2.5.0\" Any version greater than 2.5.0 would be okay to download version=\"&gt;2.5.0\" Any version less than 2.5.0 would be okay to download version=\"!=2.5.0, &gt;2.4.1, &lt;2.5.2\" Any version other than 2.5.0 and greater than 2.4.1 and less than 2.5.2 would be okay to download version=\"~&gt;2.4\" Any version from 2.4, 2.5, 2.6 etc would be okay to download version=\"~&gt;2.4.0\" Any version from 2.4.0, to 2.4.9 etc would be okay to download"},{"location":"tips/shell-script/","title":"Shell","text":""},{"location":"tips/shell-script/#tips-and-simple-code-snippets","title":"Tips and Simple Code Snippets","text":""},{"location":"tips/shell-script/#use-of-curly-braces","title":"Use of curly braces {}","text":"<ul> <li>If you want to concatenate the variable value with other strings/numbers, then curly braces for variable is mandatory.</li> </ul> <p><pre><code>weight=71\n# Without curly braces for variable weight\necho \"Without curly braces = My weight is $weightkgs\"\n\n# With curly braces for variable weight\necho \"With curly braces = My weight is ${weight}kgs\"\n</code></pre> </p>"},{"location":"tips/shell-script/#use-of-double-quotes","title":"Use of double quotes \"\"","text":"<ul> <li>If you have a variable contains spaces, then double quotes for variable will process the variable as a whole. If you dont put double quotes for variable, then it will split the variable using the space. <pre><code>#!/bin/bash\nsentence=\"Satheesh from India\"\n# Without quotes for variable sentence\nfor word in ${sentence}\ndo\n  echo \"Without quotes =&gt; ${word}\"\ndone\n\n# With quotes for variable sentence\nfor word in \"${sentence}\"\ndo\n  echo \"With quotes =&gt; ${word}\"\ndone\n</code></pre> </li> </ul>"},{"location":"tips/shell-script/#use-of-basename-command","title":"Use of basename command","text":"<p>It is a shell command used to extract the filename portion of a file path. It takes a path as input and returns only the final component of that path, removing any preceding directory path information. It is useful when you only need to work with the file name and don\u2019t require the full path.</p> <p>For example, if the path is /home/user/documents/report.docx, basename will return report.docx.  <pre><code>#!/bin/bash\n# Below is the full path for a file\necho \"ABSOLUTE PATH for a FILE =&gt; /User/satheeshpandian.j/Documents/Sats/Learning/Shell/Practice/if_exit.sh\"\n# Below command is just fetch the filename\nfile_name=$(basename /User/satheeshpandian.j/Documents/Sats/Learning/Shell/Practice/if_exit.sh)\necho \"FILE NAME =&gt; ${file_name}\"\n</code></pre> </p> <p>If you need the filename excluding the extension, then the command is <pre><code>basename /User/satheeshpandian.j/Documents/Sats/Learning/Shell/Practice/if_exit.sh .sh\n</code></pre> </p>"},{"location":"tips/shell-script/#print-the-file-names-in-current-directory","title":"Print the file names in current directory","text":"<pre><code>#!/bin/bash\n# print all the files in the current directory\nfor f in * \ndo \n  echo \"$f\"\ndone\n</code></pre>"}]}